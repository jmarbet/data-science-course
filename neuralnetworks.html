<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Science - 4&nbsp; Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./additional.html" rel="next">
<link href="./decisiontrees.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html">Overview and Methods</a></li><li class="breadcrumb-item"><a href="./neuralnetworks.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="./Data-Science.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this Course</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Overview and Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Basic Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decisiontrees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neuralnetworks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./additional.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Additional Methods</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./applicationI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Loan Default Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./applicationII.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">House Price Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-a-neural-network" id="toc-what-is-a-neural-network" class="nav-link active" data-scroll-target="#what-is-a-neural-network"><span class="header-section-number">4.1</span> What is a Neural Network?</a>
  <ul class="collapse">
  <li><a href="#origins-of-the-term-neural-network" id="toc-origins-of-the-term-neural-network" class="nav-link" data-scroll-target="#origins-of-the-term-neural-network"><span class="header-section-number">4.1.1</span> Origins of the Term “Neural Network”</a></li>
  </ul></li>
  <li><a href="#an-artificial-neuron" id="toc-an-artificial-neuron" class="nav-link" data-scroll-target="#an-artificial-neuron"><span class="header-section-number">4.2</span> An Artificial Neuron</a>
  <ul class="collapse">
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">4.2.1</span> Activation Functions</a></li>
  <li><a href="#a-special-case-perceptron" id="toc-a-special-case-perceptron" class="nav-link" data-scroll-target="#a-special-case-perceptron"><span class="header-section-number">4.2.2</span> A Special Case: Perceptron</a></li>
  </ul></li>
  <li><a href="#building-a-neural-network-from-artificial-neurons" id="toc-building-a-neural-network-from-artificial-neurons" class="nav-link" data-scroll-target="#building-a-neural-network-from-artificial-neurons"><span class="header-section-number">4.3</span> Building a Neural Network from Artificial Neurons</a></li>
  <li><a href="#relation-to-linear-regression" id="toc-relation-to-linear-regression" class="nav-link" data-scroll-target="#relation-to-linear-regression"><span class="header-section-number">4.4</span> Relation to Linear Regression</a></li>
  <li><a href="#a-simple-example" id="toc-a-simple-example" class="nav-link" data-scroll-target="#a-simple-example"><span class="header-section-number">4.5</span> A Simple Example</a></li>
  <li><a href="#deep-neural-networks" id="toc-deep-neural-networks" class="nav-link" data-scroll-target="#deep-neural-networks"><span class="header-section-number">4.6</span> Deep Neural Networks</a></li>
  <li><a href="#universal-approximation-and-the-curse-of-dimensionality" id="toc-universal-approximation-and-the-curse-of-dimensionality" class="nav-link" data-scroll-target="#universal-approximation-and-the-curse-of-dimensionality"><span class="header-section-number">4.7</span> Universal Approximation and the Curse of Dimensionality</a></li>
  <li><a href="#training-a-neural-network-determining-weights-and-biases" id="toc-training-a-neural-network-determining-weights-and-biases" class="nav-link" data-scroll-target="#training-a-neural-network-determining-weights-and-biases"><span class="header-section-number">4.8</span> Training a Neural Network: Determining Weights and Biases</a>
  <ul class="collapse">
  <li><a href="#choice-of-loss-function" id="toc-choice-of-loss-function" class="nav-link" data-scroll-target="#choice-of-loss-function"><span class="header-section-number">4.8.1</span> Choice of Loss Function</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="header-section-number">4.8.2</span> Gradient Descent</a></li>
  <li><a href="#backpropagation-algorithm" id="toc-backpropagation-algorithm" class="nav-link" data-scroll-target="#backpropagation-algorithm"><span class="header-section-number">4.8.3</span> Backpropagation Algorithm</a></li>
  </ul></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations"><span class="header-section-number">4.9</span> Practical Considerations</a></li>
  <li><a href="#python-implementation" id="toc-python-implementation" class="nav-link" data-scroll-target="#python-implementation"><span class="header-section-number">4.10</span> Python Implementation</a></li>
  </ul>
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"><i class="bi bi-link-45deg"></i>Scikit-Learn: MLP Classifier</a></li><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"><i class="bi bi-link-45deg"></i>Scikit-Learn: MLP Regressor</a></li><li><a href="https://pytorch.org/"><i class="bi bi-link-45deg"></i>PyTorch</a></li><li><a href="https://playground.tensorflow.org"><i class="bi bi-link-45deg"></i>TensorFlow Playground</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="notebooks/neural_networks.ipynb"><i class="bi bi-file-code"></i>Download Jupyter Notebook</a></li><li><a href="https://colab.research.google.com/github/jmarbet/data-science-course/blob/main/notebooks/neural_networks.ipynb"><i class="bi bi-file-code"></i>Open Jupyter Notebook in Google Colab</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html">Overview and Methods</a></li><li class="breadcrumb-item"><a href="./neuralnetworks.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this chapter, we have a look at neural networks which are a popular machine learning method. We will cover the basics of neural networks and how they can be trained.</p>
<section id="what-is-a-neural-network" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="what-is-a-neural-network"><span class="header-section-number">4.1</span> What is a Neural Network?</h2>
<p>Neural networks are at the core of many cutting-edge machine learning models. They can be used as both a <strong>supervised and unsupervised learning method</strong>. In this course, we will focus on their application in supervised learning where they are used for both <strong>regression and classification</strong> tasks. While they are conceptually not much more difficult to understand than decision trees, a neural network is <strong>not as easy to interpret as a decision tree</strong>. For this reason, they are often called black boxes, meaning that it is not so clear what is happening inside. Furthermore, neural networks tend to be more difficult to train and for tabular data, which is the type of structured data that you will typically encounter, gradient-boosted decision trees tend to perform better. Nevertheless, since neural networks are what enabled many of the recent advances in AI, they are an important topic to cover, even if it is only to better understand what has been driving recent innovations.</p>
<div id="fig-basic-neural-network" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-basic-neural-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/basic-neural-network-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-basic-neural-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: A Single-Layer Feedforward Neural Network
</figcaption>
</figure>
</div>
<p>It is common to represent neural networks as directed graphs. <a href="#fig-basic-neural-network" class="quarto-xref">Figure&nbsp;<span>4.1</span></a> shows a single-layer feedforward neural network with <span class="math inline">\(N=2\)</span> inputs, <span class="math inline">\(M=3\)</span> neurons in the hidden layer, and a single output. The input layer is connected to the hidden layer, which is connected to the output layer. For simplicity, we will only consider neural networks that are feedforward (i.e.&nbsp;their graphs are acyclical), with dense layers (i.e.&nbsp;each layer is fully connected to the previous), and without connections that skip layers.</p>
<p>As we will see later on, under certain (relatively weak) conditions</p>
<ul>
<li>Neural networks are <strong>universal approximators</strong> (can approximate any (Borel measurable) function)</li>
<li>Neural networks <strong>break the curse of dimensionality</strong> (can handle very high dimensional functions)</li>
</ul>
<p>This makes them interesting for a wide range of fields in economics, e.g., quantitative macroeconomics or econometrics. However, neural networks are not a magic bullet, and there are some downsides in terms of the large data requirements, interpretability and training difficulty.</p>
<section id="origins-of-the-term-neural-network" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="origins-of-the-term-neural-network"><span class="header-section-number">4.1.1</span> Origins of the Term “Neural Network”</h3>
<div id="fig-biological-neuron" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-biological-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Neuron3.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-biological-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: A biological neuron (Source: Wikipedia)
</figcaption>
</figure>
</div>
<p>The term “neural network” originates in attempts to find mathematical representations of information processing in biological systems <span class="citation" data-cites="Bishop2006">(<a href="references.html#ref-Bishop2006" role="doc-biblioref">Bishop 2006</a>)</span>. The biological interpretation <strong>not very important for research</strong> anymore and one should not get too hung up on it. However, the interpretation can be useful when starting to learn about neural networks. <a href="#fig-biological-neuron" class="quarto-xref">Figure&nbsp;<span>4.2</span></a> shows a biological neuron.</p>
</section>
</section>
<section id="an-artificial-neuron" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="an-artificial-neuron"><span class="header-section-number">4.2</span> An Artificial Neuron</h2>
<div id="fig-artificial-neuron" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-artificial-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/artifical-neuron-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-artificial-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Artificial Neuron
</figcaption>
</figure>
</div>
<p>Artificial neurons are the <strong>basic building blocks</strong> of neural networks. <a href="#fig-artificial-neuron" class="quarto-xref">Figure&nbsp;<span>4.3</span></a> shows a single artificial neuron. The <span class="math inline">\(N\)</span> inputs denoted <span class="math inline">\(x=(x_1,x_2,\ldots,x_N)'\)</span> are linearly combined into <span class="math inline">\(z\)</span> using weights <span class="math inline">\(w\)</span> and bias <span class="math inline">\(b\)</span></p>
<p><span class="math display">\[z = b + \sum_{i=1}^N w_i x_i = \sum_{i=0}^N w_i x_i\]</span></p>
<p>where we defined an additional input <span class="math inline">\(x_0=1\)</span> and <span class="math inline">\(w_0=b\)</span>.</p>
<p>The linear combination <span class="math inline">\(z\)</span> is transformed using an <strong>activation function</strong> <span class="math inline">\(\phi(z)\)</span>.</p>
<p><span class="math display">\[a = \phi(z) = \phi\left( \sum_{i=0}^N w_i x_i \right)\]</span></p>
<p>The activation function <strong>introduces non-linearity</strong> into the neural network and allows it to learn highly non-linear functions. The particular choice of activation function depends on the application.</p>
<p>This should look familiar to you already. If we set <span class="math inline">\(\phi(z)=z\)</span>, we get a <em>linear regression</em> model and if we set <span class="math inline">\(\phi(z)=\frac{1}{1+e^{-z}}\)</span>, we get a <em>logistic regression</em> model. This is because the basic building block, the artificial neuron, is a generalized linear model.</p>
<section id="activation-functions" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">4.2.1</span> Activation Functions</h3>
<div id="fig-activation-functions" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-activation-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/activation-functions-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-activation-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: Activation Functions
</figcaption>
</figure>
</div>
<p>Common activation functions include</p>
<ul>
<li>Sigmoid: <span class="math inline">\(\phi(z) = \frac{1}{1+e^{-z}}\)</span></li>
<li>Hyperbolic tangent: <span class="math inline">\(\phi(z) = tanh(z)\)</span></li>
<li>Rectified linear unit (ReLU): <span class="math inline">\(\phi(z) = \max(0,z)\)</span></li>
<li>Softplus: <span class="math inline">\(\phi(z) =\log(1+e^{z})\)</span></li>
</ul>
<p>ReLU has become popular in deep neural networks in recent years because of its good performance in these applications. Since economic problems usually involve smooth functions, softplus can be a good alternative.</p>
</section>
<section id="a-special-case-perceptron" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="a-special-case-perceptron"><span class="header-section-number">4.2.2</span> A Special Case: Perceptron</h3>
<p>Perceptrons were developed in the 1950s and have only one artificial neuron. Perceptrons use a <strong>step function</strong> as an activation function</p>
<p><span class="math display">\[\phi(z) = \begin{cases}
      1 &amp; \text{if } z \geq 0\\
      0 &amp; \text{otherwise}\,,
    \end{cases}\]</span></p>
<p>Perceptrons can be used for basic classification. However, the step function is usually not used in neural networks because it is not differentiable at <span class="math inline">\(z=0\)</span> and zero everywhere else. This makes it unsuitable for the back-propagation algorithm, which is used for determining the network weights.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mini-Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>What would the decision boundary of a perceptron look like if we have two inputs <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and the weights <span class="math inline">\(w_1=1\)</span>, <span class="math inline">\(w_2=1\)</span>, and <span class="math inline">\(b=-1\)</span>?</p>
</div>
</div>
</section>
</section>
<section id="building-a-neural-network-from-artificial-neurons" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="building-a-neural-network-from-artificial-neurons"><span class="header-section-number">4.3</span> Building a Neural Network from Artificial Neurons</h2>
<p>We can build a neural network by stacking multiple artificial neurons. For this reason, it is sometimes also called a <strong>multilayer perceptron</strong> (MLP). A <strong>single-layer neural network</strong> is a linear combination of <span class="math inline">\(M\)</span> artificial neurons <span class="math inline">\(a_j\)</span></p>
<p><span class="math display">\[ a_j = \phi(z_j) = \phi\left( b_{j}^{1} + \sum_{i=1}^N w_{ji}^{1} x_i \right)\]</span></p>
<p>with the output defined as</p>
<p><span class="math display">\[ g(x ; w) = b^{2}+\sum_{j=1}^{M} w_{j}^{2} a_j\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of inputs, <span class="math inline">\(M\)</span> is the number of neurons in the hidden layer, and <span class="math inline">\(w\)</span> are the weights and biases of the network. The width of the neural network is <span class="math inline">\(M\)</span>.</p>
<p><a href="#fig-basic-neural-network-with-weights" class="quarto-xref">Figure&nbsp;<span>4.5</span></a> shows a single-layer feedforward neural network with <span class="math inline">\(N=2\)</span> inputs, <span class="math inline">\(M=3\)</span> neurons in the hidden layer, and a single output. Note that the biases can be thought of as additional weights that are multiplied by a constant input of 1.</p>
<div id="fig-basic-neural-network-with-weights" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-basic-neural-network-with-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/basic-neural-network-with-weights-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-basic-neural-network-with-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: A Single-Layer Feedforward Neural Network with Biases shown explicitly
</figcaption>
</figure>
</div>
</section>
<section id="relation-to-linear-regression" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="relation-to-linear-regression"><span class="header-section-number">4.4</span> Relation to Linear Regression</h2>
<p>Note that if we use a <strong>linear activation function</strong>, e.g.&nbsp;<span class="math inline">\(\phi(x)=x\)</span>, the neural network <strong>collapses to a linear regression</strong></p>
<p><span class="math display">\[ y \cong g(x ; w) = \tilde{w}_{0} +\sum_{i=1}^{N} \tilde{w}_{i}  x_{i}\]</span></p>
<p>with appropriately defined regression coefficients <span class="math inline">\(\tilde{w}\)</span>.</p>
<p>Recall that in our description of <a href="basics.html#fig-linear-regression-interactive" class="quarto-xref">Figure&nbsp;<span>2.2</span></a> we argued that a machine learning algorithm would automatically turn the slider to find the best fit. This is exactly what the training algorithm has to do to train a neural network.</p>
</section>
<section id="a-simple-example" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="a-simple-example"><span class="header-section-number">4.5</span> A Simple Example</h2>
<p>Suppose we want to approximate <span class="math inline">\(f(x)=exp(x)-x^3\)</span> with 3 neurons. The approximation might be</p>
<p><span class="math display">\[\hat{f}(x)=a_1+a_2-a_3\]</span></p>
<p>where</p>
<p><span class="math display">\[a_1=max(0,-3x-1.5)\]</span></p>
<p><span class="math display">\[a_2=max(0,x+1)\]</span></p>
<p><span class="math display">\[a_3=max(0,3x-3)\]</span></p>
<p>Our neural network in this case uses ReLU activation functions and has all weights equal to one in the output layer. <a href="#fig-neural-network-approximation" class="quarto-xref">Figure&nbsp;<span>4.6</span></a> shows the admittedly poor approximation of <span class="math inline">\(f(x)\)</span> by <span class="math inline">\(\hat{f}(x)\)</span> using this neural network. Given the piecewise linear nature of the ReLU activation function, the approximation is not very good. However, with more neurons, we could get a better approximation.</p>
<div id="fig-neural-network-approximation" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neural-network-approximation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/neural-network-approximation-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neural-network-approximation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.6: Approximation by a Neural Network
</figcaption>
</figure>
</div>
<p><a href="#fig-neural-network-interactive" class="quarto-xref">Figure&nbsp;<span>4.7</span></a> shows an interactive version of <a href="#fig-neural-network-approximation" class="quarto-xref">Figure&nbsp;<span>4.6</span></a> where you can adjust the weights of the neural network to approximate a simple dataset. As you can see, it is quite tricky to find parameters that approximate the function well. This is where the training algorithm comes in. It will automatically adjust the weights to minimize a loss function.</p>
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neural-network-interactive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb1" data-startfrom="370" data-source-offset="-0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 369;"><span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>    <span class="co">//</span></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> dataRange <span class="op">=</span> [<span class="op">-</span><span class="dv">2</span><span class="op">,</span> <span class="dv">2</span>]<span class="op">;</span></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> yDataRange <span class="op">=</span> [<span class="op">-</span><span class="dv">2</span><span class="op">,</span> <span class="dv">9</span>]<span class="op">;</span></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute approximation and true function</span></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">feedforward</span>(inputRange) {</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> loss <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> N <span class="op">=</span> <span class="dv">100</span><span class="op">;</span></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;=</span> <span class="dv">100</span><span class="op">;</span> i <span class="op">+=</span> <span class="dv">1</span>) {</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>            <span class="kw">let</span> x <span class="op">=</span> inputRange[<span class="dv">0</span>] <span class="op">+</span> i <span class="op">*</span> (inputRange[<span class="dv">1</span>] <span class="op">-</span> inputRange[<span class="dv">0</span>])<span class="op">/</span><span class="dv">100</span></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> h1 <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">max</span>(<span class="dv">0</span><span class="op">,</span> w11<span class="op">*</span>x <span class="op">+</span> b11)<span class="op">;</span></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> h2 <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">max</span>(<span class="dv">0</span><span class="op">,</span> w12<span class="op">*</span>x <span class="op">+</span> b12)<span class="op">;</span></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> h3 <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">max</span>(<span class="dv">0</span><span class="op">,</span> w13<span class="op">*</span>x <span class="op">+</span> b13)<span class="op">;</span></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> y <span class="op">=</span> b2 <span class="op">+</span> w21 <span class="op">*</span> h1 <span class="op">+</span> w22 <span class="op">*</span> h2 <span class="op">-</span> w23 <span class="op">*</span> h3<span class="op">;</span></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>            <span class="kw">let</span> dataPoint <span class="op">=</span> {<span class="st">"x"</span><span class="op">:</span> x<span class="op">,</span> <span class="st">"y"</span><span class="op">:</span> y}<span class="op">;</span></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>            loss<span class="op">.</span><span class="fu">push</span>(dataPoint)</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss<span class="op">;</span></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">truefunction</span>(inputRange) {</span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> loss <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> N <span class="op">=</span> <span class="dv">100</span><span class="op">;</span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;=</span> <span class="dv">100</span><span class="op">;</span> i <span class="op">+=</span> <span class="dv">1</span>) {</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>            <span class="kw">let</span> x <span class="op">=</span> inputRange[<span class="dv">0</span>] <span class="op">+</span> i <span class="op">*</span> (inputRange[<span class="dv">1</span>] <span class="op">-</span> inputRange[<span class="dv">0</span>])<span class="op">/</span><span class="dv">100</span></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> y <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">exp</span>(x) <span class="op">-</span> x<span class="op">**</span><span class="dv">3</span><span class="op">;</span></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>            <span class="kw">let</span> dataPoint <span class="op">=</span> {<span class="st">"x"</span><span class="op">:</span> x<span class="op">,</span> <span class="st">"y"</span><span class="op">:</span> y}<span class="op">;</span></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>            loss<span class="op">.</span><span class="fu">push</span>(dataPoint)</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss<span class="op">;</span></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> approxData <span class="op">=</span> <span class="fu">feedforward</span>(dataRange)</span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> trueData <span class="op">=</span> <span class="fu">truefunction</span>(dataRange)</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Plot approximation and true function</span></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Plot<span class="op">.</span><span class="fu">plot</span>({</span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>            <span class="dt">marginLeft</span><span class="op">:</span> <span class="dv">40</span><span class="op">,</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>            <span class="dt">marginTop</span><span class="op">:</span> <span class="dv">20</span><span class="op">,</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>            <span class="dt">width</span><span class="op">:</span> width <span class="op">-</span> <span class="dv">2</span><span class="op">,</span></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>            <span class="dt">height</span><span class="op">:</span> width <span class="op">*</span> <span class="dv">2</span><span class="op">/</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">2</span><span class="op">,</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>            <span class="dt">x</span><span class="op">:</span> { <span class="dt">domain</span><span class="op">:</span> dataRange<span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"x"</span>}<span class="op">,</span></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>            <span class="dt">y</span><span class="op">:</span> { <span class="dt">domain</span><span class="op">:</span> yDataRange<span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"y"</span>}<span class="op">,</span></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>            <span class="dt">marks</span><span class="op">:</span> [</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>                Plot<span class="op">.</span><span class="fu">frame</span>()<span class="op">,</span></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a>                Plot<span class="op">.</span><span class="fu">line</span>(trueData<span class="op">,</span> {<span class="dt">x</span><span class="op">:</span> <span class="st">"x"</span><span class="op">,</span> <span class="dt">y</span><span class="op">:</span> <span class="st">"y"</span><span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span> <span class="st">"red"</span>})<span class="op">,</span></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>                Plot<span class="op">.</span><span class="fu">line</span>(approxData<span class="op">,</span> {<span class="dt">x</span><span class="op">:</span> <span class="st">"x"</span><span class="op">,</span> <span class="dt">y</span><span class="op">:</span> <span class="st">"y"</span><span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span> <span class="st">"steelblue"</span>})</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>            ]<span class="op">,</span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>            <span class="dt">nice</span><span class="op">:</span> <span class="kw">true</span></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="expression">

</div>
</div>
</div>
<div class="sourceCode cell-code hidden" id="cb2" data-startfrom="433" data-source-offset="-1353"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 432;"><span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a>inputRange <span class="op">=</span> [<span class="op">-</span><span class="dv">5</span><span class="op">,</span> <span class="dv">5</span>]<span class="op">;</span></span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a>viewof w11 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="op">-</span><span class="dv">3</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"w₁₁"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a>viewof w12 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"w₁₂"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a>viewof w13 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">3</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"w₁₃"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>viewof w21 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"w₂₁"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a>viewof w22 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"w₂₂"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a>viewof w23 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"w₂₃"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a>viewof b11 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="op">-</span><span class="fl">1.5</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"b₁₁"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>viewof b12 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"b₁₂"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a>viewof b13 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="op">-</span><span class="dv">3</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"b₁₃"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>viewof b2 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>(inputRange<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"b₂"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-9" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-10" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-11" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-12" data-nodetype="declaration">

</div>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neural-network-interactive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.7: Interactive Neural Network Approximation
</figcaption>
</figure>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TensorFlow Playground
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you want to play around with neural networks, you can use the TensorFlow Playground: <a href="https://playground.tensorflow.org">https://playground.tensorflow.org</a>. It is a web-based tool that allows you to experiment with neural networks and see how they learn. <a href="#fig-tensorflow-playground" class="quarto-xref">Figure&nbsp;<span>4.8</span></a> shows the interface of the TensorFlow Playground.</p>
<div id="fig-tensorflow-playground" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-playground-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/tensorflow_playground.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-playground-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.8: Tesorflow Playground
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="deep-neural-networks" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="deep-neural-networks"><span class="header-section-number">4.6</span> Deep Neural Networks</h2>
<div id="fig-deep-neural-network" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deep-neural-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/deep-neural-network-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deep-neural-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.9: Deep Neural Network
</figcaption>
</figure>
</div>
<p><strong>Deep neural networks</strong> have more than one hidden layer. The number of hidden layers is also called the <strong>depth</strong> of the neural network. Deep neural networks can learn more complicated things. For simple function approximation, a single hidden layer is sufficient. <a href="#fig-deep-neural-network" class="quarto-xref">Figure&nbsp;<span>4.9</span></a> shows a deep neural network with two hidden layers.</p>
<p>The first hidden layer consists of <span class="math inline">\(M_1\)</span> artificial neurons with inputs <span class="math inline">\(x_1,x_2,\ldots,x_N\)</span></p>
<p><span class="math display">\[a_j^{1} = \phi\left( b_{j}^{1} + \sum_{i=1}^N w_{ji}^{1} x_i \right)\]</span></p>
<p>The second hidden layer consists of <span class="math inline">\(M_2\)</span> artificial neurons with inputs <span class="math inline">\(a_1^{1},a_2^{1},\ldots,a_{M_1}^{1}\)</span></p>
<p><span class="math display">\[a_k^{2} = \phi\left( b_{k}^{2} + \sum_{j=1}^{M_1} w_{kj}^{2} a_j^{1} \right)\]</span></p>
<p>After <span class="math inline">\(Q\)</span> hidden layers, the output is defined as</p>
<p><span class="math display">\[y \cong g(x ; w) = b^{Q+1}+\sum_{j=1}^{M_{Q}} w_{j}^{Q+1} a_j^{Q}\]</span></p>
<p>Note that the activation functions do not need to be the same everywhere. In principle, we could vary the activation functions even within a layer.</p>
</section>
<section id="universal-approximation-and-the-curse-of-dimensionality" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="universal-approximation-and-the-curse-of-dimensionality"><span class="header-section-number">4.7</span> Universal Approximation and the Curse of Dimensionality</h2>
<p>Recall that we want to <strong>approximate an unknown function</strong> in supervised learning tasks</p>
<p><span class="math display">\[y = f(x)\]</span></p>
<p>where <span class="math inline">\(y=(y_1,y_2,\ldots,y_K)'\)</span> and <span class="math inline">\(x=(x_1,x_2,\ldots,x_N)'\)</span> are vectors. The function <span class="math inline">\(f(x)\)</span> could stand for many different functions in economics (e.g.&nbsp;a value function, a policy function, a conditional expectation, a classifier, <span class="math inline">\(\ldots\)</span>).</p>
<p>It turns out that neural networks are <strong>universal approximators</strong> and <strong>break the curse of dimensionality</strong>. The universal approximation theorem by Hornik, Stinchcombe, and White (1989) states:</p>
<blockquote class="blockquote">
<p>A neural network with at least one hidden layer can approximate any Borel measurable function mapping finite-dimensional spaces to any desired degree of accuracy.</p>
</blockquote>
<p>Breaking the curse of dimensionality (Barron, 1993)</p>
<blockquote class="blockquote">
<p>A one-layer NN achieves integrated square errors of order <span class="math inline">\(O(1/M)\)</span>, where <span class="math inline">\(M\)</span> is the number of nodes. In comparison, for series approximations, the integrated square error is of order <span class="math inline">\(O(1/(M^{2/N}))\)</span> where <span class="math inline">\(N\)</span> is the dimensions of the function to be approximated.</p>
</blockquote>
<!--
## Relation to Other Approximation Methods

\begin{itemize}
  
  \item Recall the expression for our basic neural network
  \begin{equation*}
    y \cong g(x ; w) = b^{2}+\sum_{j=1}^{M} w_{j}^{2} \phi\left(b_{j}^{1}+\sum_{i=1}^{N} w_{ji}^{1} x_{i}\right)
  \end{equation*}
  
  where for notational simplicity we assumed that $y$ is scalar
  
  \item This looks \textbf{similar to a projection}
  \begin{equation*}
    y \cong \tilde{g}(x ; w) = w_{0} +\sum_{j=1}^{M} w_{j} \phi_j\left(x\right)
  \end{equation*}
  
  where $\phi_j$ is, for example, a Chebyshev polynomial
  
  %\item Neural network trades lower number of basis functions for higher number of coefficients

\end{itemize}

Breaking the curse of dimensionality (Barron, 1993)
> A one-layer NN achieves integrated square errors of order $O(1/M)$, where $M$ is the number of nodes. In comparison, for series approximations, the integrated square error is of order $O(1/(M^{2/N}))$ where $N$ is the dimensions of the function to be approximated.

\begin{itemize}
  \item Crucial difference between NN and traditional series approx.
  \begin{itemize}
    \item In series approximation coefficients should increase exponentially with dimensions to preserve a given precision (this is not the case for NN)%. Neural networks are not exposed to this curse.
    
    \item NN require much more samples to train, given a number of parameters, than traditional series approximation
  \end{itemize}

%In traditional series approximation, coefficients should increase exponentially with dimensions to preserve a given precision. Neural networks are not exposed to this curse.
% 
%On the other hand, NN require much more samples to train, given a number of parameters, that traditional linear series approximation.

  %\item NN are also easy to code, stable, and scalable for multiprocessing
  
  %\item All of this makes them interesting for a wide range of fields in economics %such as quantitative macroeconomics or econometrics
  
  \item Large data requirements may make it infeasible to properly train the NN in some applications (e.g. GDP forecasting)
  
  \item In some cases (e.g. macro-modelling), we can generate data

\end{itemize}
-->
</section>
<section id="training-a-neural-network-determining-weights-and-biases" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="training-a-neural-network-determining-weights-and-biases"><span class="header-section-number">4.8</span> Training a Neural Network: Determining Weights and Biases</h2>
<p>We have not yet discussed how to determine the weights and biases. The weights and biases <span class="math inline">\(w\)</span> are selected to <strong>minimize a loss function</strong></p>
<p><span class="math display">\[E(w; X, Y) = \frac{1}{N} \sum_{n=1}^{N} E_n(w; x_n, y_n)\]</span></p>
<p>where <span class="math inline">\(N\)</span> refers to the number of input-output pairs that we use for training and <span class="math inline">\(E_n(w; x_n, y_n)\)</span> refers to the loss of an individual pair <span class="math inline">\(n\)</span>.</p>
<p>For notational simplicity, I will write <span class="math inline">\(E(w)\)</span> and <span class="math inline">\(E_n(w)\)</span> in the following or in some cases even omit argument <span class="math inline">\(w\)</span>.</p>
<section id="choice-of-loss-function" class="level3" data-number="4.8.1">
<h3 data-number="4.8.1" class="anchored" data-anchor-id="choice-of-loss-function"><span class="header-section-number">4.8.1</span> Choice of Loss Function</h3>
<p>The choice of loss function depends on the problem at hand. In regressions, one often uses a <strong>mean squared error (MSE) loss</strong></p>
<p><span class="math display">\[E_n(w; x_n, y_n) = \frac{1}{2} \left\|g\left(x_{n}; w\right)-y_{n}\right\|^{2}\]</span></p>
<p>In classification problems, one often uses a <strong>cross-entropy loss</strong></p>
<p><span class="math display">\[E_n(w; x_n, y_n) = \sum_{k=1}^K y_{nk} \log(g_k(x_n;w))\]</span></p>
<p>where <span class="math inline">\(k\)</span> refers to <span class="math inline">\(k\)</span>th class (or <span class="math inline">\(k\)</span>th element) in the output vector.</p>
</section>
<section id="gradient-descent" class="level3" data-number="4.8.2">
<h3 data-number="4.8.2" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">4.8.2</span> Gradient Descent</h3>
<div id="fig-gradient-descent" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/gradient-descent-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.10: Gradient Descent
</figcaption>
</figure>
</div>
<p>The weights and biases are determined by minimizing the loss function using a <strong>gradient descent algorithm</strong>. The basic idea is to compute how the loss changes with the weights <span class="math inline">\(w\)</span> and step into the direction that reduces the loss. <a href="#fig-gradient-descent" class="quarto-xref">Figure&nbsp;<span>4.10</span></a> shows a simple example of a loss function and the gradient descent algorithm. The basic steps of the algorithm are</p>
<ol type="1">
<li>Initialize weights (e.g.&nbsp;draw from Gaussian distribution)</li>
</ol>
<p><span class="math display">\[w^{(0)} \sim N(0,I)\]</span></p>
<ol start="2" type="1">
<li>Compute the gradient of the loss function with respect to weights</li>
</ol>
<p><span class="math display">\[\nabla E(w^{(i)}) = \frac{1}{N}\sum_{n=1}^N \nabla E_n\left(w^{(i)}\right)\]</span></p>
<ol start="3" type="1">
<li>Update weights (make a small step in the direction of the negative gradient)</li>
</ol>
<p><span class="math display">\[w^{(i+1)} = w^{(i)} - \eta \nabla E\left(w^{(i)}\right)\]</span></p>
<p>where <span class="math inline">\(\eta&gt;0\)</span> is the learning rate.</p>
<ol start="4" type="1">
<li>Repeat Steps 2 and 3 until a terminal condition (e.g.&nbsp;fixed number of iterations) is reached.</li>
</ol>
<p>If we use the batch gradient descent algorithm described above, we might get stuck in a local minimum. To avoid this, we can use</p>
<ul>
<li><p><strong>Stochastic gradient descent:</strong> Use only a single observation to compute the gradient and update the weights for each observation</p>
<p><span class="math display">\[w^{(i+1)} = w^{(i)} - \eta \nabla E_n\left(w^{(i)}\right)\]</span></p></li>
<li><p><strong>Minibatch gradient descent:</strong> Use a small batch of observations (e.g.&nbsp;32) to compute the gradient and update the weights for each minibatch</p></li>
</ul>
<p>These algorithms are less likely to get stuck in a shallow local minimum of the loss function because they are “noisier”. <a href="#fig-gradient-descent-comparison" class="quarto-xref">Figure&nbsp;<span>4.11</span></a> shows a comparison of the different gradient descent algorithms. Minibatch gradient descent is probably the most commonly used and is also what we will be using in our implementation in Python.</p>
<div id="fig-gradient-descent-comparison" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/gradient-descent-comparison-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.11: Comparison of Gradient Descent Types (blue: Full Batch, red: Minibatch, orange: Stochastic)
</figcaption>
</figure>
</div>
</section>
<section id="backpropagation-algorithm" class="level3" data-number="4.8.3">
<h3 data-number="4.8.3" class="anchored" data-anchor-id="backpropagation-algorithm"><span class="header-section-number">4.8.3</span> Backpropagation Algorithm</h3>
<div id="fig-backpropagation" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backpropagation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/backpropagation-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backpropagation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.12: Backpropagation Algorithm
</figcaption>
</figure>
</div>
<p>Computing the gradient seems to be a daunting task since a weight in the first layer in a deep neural network affects the loss function potentially through thousands of “paths”. The <strong>backpropagation algorithm</strong> (Rumelhart et al., 1986) provides an efficient way to evaluate the gradient. The basic idea is to go backward through the network to evaluate the gradient as shown in <a href="#fig-backpropagation" class="quarto-xref">Figure&nbsp;<span>4.12</span></a>. If you are interested in the details, I recommend reading the notes by <span class="citation" data-cites="Nielsen2019">Nielsen (<a href="references.html#ref-Nielsen2019" role="doc-biblioref">2019</a>)</span>.</p>
</section>
</section>
<section id="practical-considerations" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="practical-considerations"><span class="header-section-number">4.9</span> Practical Considerations</h2>
<p>From a practical perspective, there are many more things to consider. Often times it’s beneficial to do some (or all) of the following</p>
<ul>
<li><em>Input/output normalization:</em> (e.g.&nbsp;to have unit variance and mean zero) can improve the performance of the NN</li>
<li><em>Check for overfitting:</em> by splitting the dataset into a <em>training dataset</em> and a <em>test dataset</em></li>
<li><em>Regularization:</em> to avoid overfitting (e.g.&nbsp;add a term to lose function that penalizes large weights)</li>
<li><em>Adjust the learning rate:</em> <span class="math inline">\(\eta\)</span> during training</li>
</ul>
<p>We have already discussed some of these topics in the context of other machine learning algorithms.</p>
</section>
<section id="python-implementation" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="python-implementation"><span class="header-section-number">4.10</span> Python Implementation</h2>
<div class="quarto-embed-nb-cell" data-notebook="/Users/joel/Dropbox (Privat)/Studium/CEMFI/PhD/TA/Data Science for Diploma in Banking Supervision/Local/Lecture Notes/notebooks/neural_networks.ipynb" data-notebook-title="Implementing the Feedforward Part of a Neural Network" data-notebook-cellid="cell-0">
<p>Let’s have a look at how to implement a neural network in Python.</p>
<section id="implementing-the-feedforward-part-of-a-neural-network" class="level3" data-number="4.10.1">
<h3 data-number="4.10.1" class="anchored" data-anchor-id="implementing-the-feedforward-part-of-a-neural-network"><span class="header-section-number">4.10.1</span> Implementing the Feedforward Part of a Neural Network</h3>
<p>As a small programming exercise and to improve our understanding of neural networks, let’s implement the feedforward part of a neural network from scratch. We will have to calculate the output of the network for some given weights and biases, as well as some inputs. Let’s start by importing the necessary libraries</p>
<div id="cell-1" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:33:44.258370Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:33:44.258043Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:33:44.420173Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:33:44.419617Z&quot;}" data-execution_count="1">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we define the activation function for which we use the sigmoid function</p>
<div id="cell-3" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:33:44.423211Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:33:44.422980Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:33:44.425635Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:33:44.425244Z&quot;}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> activation_function(x):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x)) <span class="co"># sigmoid function</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we define the feedforward function which calculates the output of the neural network given some inputs, weights, and biases. The function takes the inputs, weights, and biases as arguments and returns the output of the network</p>
<div id="cell-5" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:33:44.427587Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:33:44.427413Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:33:44.430285Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:33:44.429872Z&quot;}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedforward(inputs, w1, w2, b1, b2):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the pre-activation values for the first layer</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> b1 <span class="op">+</span> np.matmul(w1, inputs)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the post-activation values for the first layer</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> activation_function(z)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine the post-activation values of the first layer to an output</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> b2 <span class="op">+</span> np.matmul(w2, a)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Mathematically, the function computes the following</p>
<p><span class="math inline">\(z = b^{1} + w^1 x\)</span></p>
<p><span class="math inline">\(a = \phi(z)\)</span></p>
<p><span class="math inline">\(g = b^2 + w^2 a\)</span></p>
<p>and returns <span class="math inline">\(g\)</span> at the end. We have written this using matrix notation to make it more compact. Remember that node <span class="math inline">\(j\)</span> in the hidden layer is given by</p>
<p><span class="math inline">\(z_j = b_{j}^{1} + \sum_{i=1}^N w_{ji}^{1} x_i\)</span></p>
<p><span class="math inline">\(a_j = \phi(z_j)\)</span></p>
<p>and the output of the network is given by</p>
<p><span class="math inline">\(g(x ; w) = b^{2}+\sum_{j=1}^{M} w_{j}^{2} a_j.\)</span></p>
<p>Let’s test the function with some example inputs, weights and biases</p>
<div id="cell-7" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:33:44.432323Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:33:44.432140Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:33:44.438919Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:33:44.438187Z&quot;}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the weights and biases</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.2</span>], [<span class="fl">0.3</span>, <span class="fl">0.4</span>]]) <span class="co"># 2x2 matrix</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="fl">0.6</span>]) <span class="co"># 1-d vector</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>]) <span class="co"># 1-d vector</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the inputs</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>]) <span class="co"># 1-d vector</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the output of the network</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>feedforward(inputs, w1, w2, b1, b2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>np.float64(1.0943291429384328)</code></pre>
</div>
</div>
<p>To operationalize this, we would also need to define a loss function and an optimization algorithm to update the weights and biases. However, this is beyond the scope of this course.</p>
</section>
<section id="using-neural-networks-in-sci-kit-learn" class="level3" data-number="4.10.2">
<h3 data-number="4.10.2" class="anchored" data-anchor-id="using-neural-networks-in-sci-kit-learn"><span class="header-section-number">4.10.2</span> Using Neural Networks in Sci-Kit Learn</h3>
<p>Sci-kit learn provides a simple interface to use neural networks. However, it is not as flexible as the more commonly used PyTorch or TensorFlow. We can reuse the <strong>dataset of credit card transactions</strong> from <a href="https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud/data">Kaggle.com</a> to demonstrate how to use neural networks in scikit-learn.</p>
<div id="cell-9" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:33:44.473746Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:33:44.473506Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:33:49.047398Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:33:49.046742Z&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, MinMaxScaler</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, accuracy_score, roc_auc_score, recall_score, precision_score, roc_curve</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="dv">50</span>) <span class="co"># Display up to 50 columns</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> io <span class="im">import</span> BytesIO</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlopen</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> zipfile <span class="im">import</span> ZipFile</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os.path</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the file exists</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.isfile(<span class="st">'data/card_transdata.csv'</span>):</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Downloading dataset...'</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the dataset to be downloaded</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    zipurl <span class="op">=</span> <span class="st">'https://www.kaggle.com/api/v1/datasets/download/dhanushnarayananr/credit-card-fraud'</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Download and unzip the dataset in the data folder</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> urlopen(zipurl) <span class="im">as</span> zipresp:</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> ZipFile(BytesIO(zipresp.read())) <span class="im">as</span> zfile:</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>            zfile.extractall(<span class="st">'data'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'DONE!'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Dataset already downloaded!'</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/card_transdata.csv'</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and test sets</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">'fraud'</span>, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># All variables except `fraud`</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'fraud'</span>] <span class="co"># Only our fraud variables</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, stratify<span class="op">=</span>y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale the features</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scale_features(scaler, df, col_names, only_transform<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the features we want to scale</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> df[col_names] </span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit the scaler to the features and transform them</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> only_transform:</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> scaler.transform(features.values)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> scaler.fit_transform(features.values)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the original features with the scaled features</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>    df[col_names] <span class="op">=</span> features</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>col_names <span class="op">=</span> [<span class="st">'distance_from_home'</span>, <span class="st">'distance_from_last_transaction'</span>, <span class="st">'ratio_to_median_purchase_price'</span>] </span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler() </span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>scale_features(scaler, X_train, col_names)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>scale_features(scaler, X_test, col_names, only_transform<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset already downloaded!</code></pre>
</div>
</div>
<p>Recall that the target variable <span class="math inline">\(y\)</span> is <code>fraud</code>, which indicates whether the transaction is fraudulent or not. The other variables are the features <span class="math inline">\(x\)</span> of the transactions.</p>
<p>To use a neural network for a classification task, we can use the <code>MLPClassifier</code> class from scikit-learn. The following code snippet shows how to use a neural network with one hidden layer with 16 nodes</p>
<div id="cell-11" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:33:49.050383Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:33:49.050095Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:34:58.669043Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:34:58.668402Z&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(<span class="dv">16</span>,), random_state<span class="op">=</span><span class="dv">42</span>, verbose<span class="op">=</span><span class="va">False</span>).fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you would like to use a neural network with multiple hidden layers, you can specify the number of nodes per hidden layer using the <code>hidden_layer_sizes</code> parameter. For example, the following code snippet shows how to use a neural network with two hidden layers, one with 5 nodes and the other with 4 nodes</p>
<div id="cell-13" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:34:58.671583Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:34:58.671370Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:35.196974Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:35.196395Z&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MLPClassifier(alpha<span class="op">=</span><span class="fl">1e-5</span>, hidden_layer_sizes<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">4</span>), activation<span class="op">=</span><span class="st">'logistic'</span>, random_state<span class="op">=</span><span class="dv">42</span>).fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the <code>alpha</code> parameter specifies the regularization strength, the <code>activation</code> parameter specifies the activation function (by default it uses <code>relu</code>) and the <code>random_state</code> parameter specifies the seed for the random number generator (useful for reproducible results).</p>
<p>We can check the loss curve to see how the neural network loss declined during training</p>
<div id="cell-15" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:35.201050Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:35.200807Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:35.365323Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:35.364814Z&quot;}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>plt.plot(clf.loss_curve_)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Loss Curve"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iterations'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/notebooks-neural_networks-cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can then use the same way to evaluate the neural network performance as we did for the other ML models</p>
<div id="cell-17" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:35.367723Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:35.367492Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:35.568067Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:35.567239Z&quot;}" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>y_proba <span class="op">=</span> clf.predict_proba(X_test)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>recall_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ROC AUC: </span><span class="sc">{</span>roc_auc_score(y_test, y_proba[:, <span class="dv">1</span>])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.9955266666666667
Precision: 0.971747127308582
Recall: 0.9772319896266352
ROC AUC: 0.9996638991577014</code></pre>
</div>
</div>
<p>The neural network performs substantially better than the logistic regression. As in the case of the tree-based methods, the ROC AUC score is much closer to the maximum value of 1 and we have an almost perfect classifier</p>
<div id="cell-19" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:35.570987Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:35.570519Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:35.702769Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:35.702129Z&quot;}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the ROC curve</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>fpr, tpr, thresholds <span class="op">=</span> roc_curve(y_test, y_proba[:, <span class="dv">1</span>])</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the ROC curve</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'grey'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'False Positive Rate (FPR)'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Positive Rate (TPR)'</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'ROC Curve'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/notebooks-neural_networks-cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s also check the confusion matrix to see where we still make mistakes</p>
<div id="cell-21" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:35.705945Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:35.705688Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:35.949913Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:35.949015Z&quot;}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>conf_mat <span class="op">=</span> confusion_matrix(y_test, y_pred, labels<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">0</span>]).transpose() <span class="co"># Transpose the sklearn confusion matrix to match the convention in the lecture</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_mat, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, fmt<span class="op">=</span><span class="st">'g'</span>, xticklabels<span class="op">=</span>[<span class="st">'Fraud'</span>, <span class="st">'No Fraud'</span>], yticklabels<span class="op">=</span>[<span class="st">'Fraud'</span>, <span class="st">'No Fraud'</span>])</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Actual"</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Predicted"</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neuralnetworks_files/figure-html/notebooks-neural_networks-cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>There are around 270 false negatives, i.e., a fraudulent transaction that we did not detect. There are also around 980 false positives, i.e., “false alarms”, where non-fraudulent transactions were classified as fraudulent.</p>
</section>
<section id="using-neural-networks-in-pytorch" class="level3" data-number="4.10.3">
<h3 data-number="4.10.3" class="anchored" data-anchor-id="using-neural-networks-in-pytorch"><span class="header-section-number">4.10.3</span> Using Neural Networks in PyTorch</h3>
<p>While it is possible to use neural networks in scikit-learn, it is more common to use PyTorch or TensorFlow for neural networks. PyTorch is a popular deep-learning library that is widely used in academia and industry. In this section, we will show how to use PyTorch to build a simple neural network for the same credit card fraud detection task.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feel Free to Skip This Section
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section might be a bit more challenging than what we have looked at previously. If you think that you are not ready for this, feel free to skip this section. This is mainly meant to be a starting point for those who are interested in learning more about neural networks.</p>
<p>For a more in-depth introduction to PyTorch, I recommend that you check out the <a href="https://pytorch.org/tutorials/">official PyTorch tutorials</a>. This section, in particular, builds on the <a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a> tutorial.</p>
</div>
</div>
<p>Let’s start by importing the necessary libraries</p>
<div id="cell-23" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:35.952828Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:35.952626Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:38.316751Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:38.316262Z&quot;}" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, let’s prepare the data for PyTorch. We need to convert the data in our DataFrame to PyTorch tensors</p>
<div id="cell-25" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:38.319321Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:38.319072Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:38.353325Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:38.352597Z&quot;}" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>X_train_tensor <span class="op">=</span> torch.tensor(X_train.values, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>y_train_tensor <span class="op">=</span> torch.tensor(y_train.values, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we also converted the input values to <code>float32</code> for improved training speed and the target values to <code>long</code> which is a type of integer (remember our target <code>y</code> can only take values zero or one). Next, we need to create a <code>DataLoader</code> object to load the data in mini-batches during the training process</p>
<div id="cell-27" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:38.356217Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:38.355931Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:38.359843Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:38.359265Z&quot;}" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(X_train_tensor, y_train_tensor)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">200</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>dataset_size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we define the neural network model using the <code>nn</code> module from PyTorch</p>
<div id="cell-29" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:38.362347Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:38.362078Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:38.369820Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:38.369181Z&quot;}" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    torch.nn.Linear(<span class="dv">7</span>, <span class="dv">16</span>), <span class="co"># 7 input features, 16 nodes in the hidden layer</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    torch.nn.ReLU(),        <span class="co"># ReLU activation function</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    torch.nn.Linear(<span class="dv">16</span>, <span class="dv">2</span>) <span class="co"># 16 nodes in the hidden layer, 2 output nodes (fraud or no fraud)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We also need to define the loss function and the optimizer. We will use the cross-entropy loss function and the Adam optimizer</p>
<div id="cell-31" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:38.372486Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:38.372269Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:35:39.521619Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:35:39.521133Z&quot;}" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>, weight_decay<span class="op">=</span><span class="fl">1e-5</span>) <span class="co"># Adam optimizer with learning rate of 0.001 and L2 regularization (analogous to alpha in scikit-learn)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now train the neural network using the following code snippet</p>
<div id="cell-33" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:35:39.523956Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:35:39.523668Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:44:21.029710Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:44:21.029036Z&quot;}" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">80</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop over batches in an epoch using DataLoader</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> id_batch, (X_batch, y_batch) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the predicted y using the neural network model with the current weights</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        y_batch_pred <span class="op">=</span> model(X_batch)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the loss</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_batch_pred, y_batch)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset the gradients of the loss function to zero</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the gradient of the loss with respect to model parameters</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the weights by taking a "step" in the direction that reduces the loss</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:&gt;7f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss: 0.024499
Epoch 19 loss: 0.008713
Epoch 29 loss: 0.016122
Epoch 39 loss: 0.007585
Epoch 49 loss: 0.005461
Epoch 59 loss: 0.020942
Epoch 69 loss: 0.016723
Epoch 79 loss: 0.007653</code></pre>
</div>
</div>
<p>Note that here we are updating the model weights for each mini-batch in the dataset and go over the whole dataset 80 times (epochs). We print the loss every epoch to see how the loss decreases over time.</p>
<p>The following snippet shows how to use full-batch gradient descent instead of mini-batch gradient descent</p>
<div id="cell-35" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:44:21.032470Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:44:21.032215Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:47:30.939881Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:47:30.939004Z&quot;}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the predicted y using the neural network model with the current weights</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    y_epoch_pred <span class="op">=</span> model(X_train_tensor)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the loss</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_epoch_pred, y_train_tensor)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reset the gradients of the loss function to zero</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the gradient of the loss with respect to model parameters</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the weights by taking a "step" in the direction that reduces the loss</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print the loss every 100 epochs</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:&gt;7f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 99 loss: 0.009982
Epoch 199 loss: 0.009945
Epoch 299 loss: 0.009928
Epoch 399 loss: 0.009920
Epoch 499 loss: 0.009914
Epoch 599 loss: 0.009910
Epoch 699 loss: 0.009907
Epoch 799 loss: 0.009904
Epoch 899 loss: 0.009901
Epoch 999 loss: 0.009899
Epoch 1099 loss: 0.009897
Epoch 1199 loss: 0.009895
Epoch 1299 loss: 0.009893
Epoch 1399 loss: 0.009891
Epoch 1499 loss: 0.009890
Epoch 1599 loss: 0.009888
Epoch 1699 loss: 0.009886
Epoch 1799 loss: 0.009885
Epoch 1899 loss: 0.009883
Epoch 1999 loss: 0.009881</code></pre>
</div>
</div>
<p>Note that in this version we are updating the model weights 2000 times (epochs) and printing the loss every 100 epochs. We can now evaluate the model on the test set</p>
<div id="cell-37" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2025-06-28T20:47:30.943432Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-06-28T20:47:30.943096Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-06-28T20:47:31.064948Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-06-28T20:47:31.064126Z&quot;}" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>X_test_tensor <span class="op">=</span> torch.tensor(X_test.values, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> torch.argmax(model(X_test_tensor), dim<span class="op">=</span><span class="dv">1</span>).numpy()</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>recall_score(y_test, y_pred)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.9965833333333334
Precision: 0.9775587566338135
Recall: 0.9834865184394188</code></pre>
</div>
</div>
<p>Note that for simplicity we are reusing the sci-kit learn metrics to evaluate the model.</p>
<p>However, our neural network trained in PyTorch does not perform exactly the same as the neural network trained in scikit-learn. This is likely because of different hyperparameters or different initializations of the weights. In practice, it is common to experiment with different hyperparameters to find the best model or to use grid search and cross-validation to try many values and find the best-performing ones.</p>
</section>
<section id="conclusions" class="level3" data-number="4.10.4">
<h3 data-number="4.10.4" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">4.10.4</span> Conclusions</h3>
<p>In this chapter, we have learned about neural networks, which are the foundation of deep learning. We have seen how to implement parts of a simple neural network from scratch and how to use neural networks in scikit-learn and PyTorch.</p>
</section>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Bishop2006" class="csl-entry" role="listitem">
Bishop, Christopher M. 2006. <em><span class="nocase">Pattern Recognition and Machine Learning</span></em>. Edited by Michael Jordan, Jon Kleinberg, and Bernhard Schölkopf. Information Science and Statistics. Springer Science+Business Media, LLC. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a>.
</div>
<div id="ref-Nielsen2019" class="csl-entry" role="listitem">
Nielsen, Michael. 2019. <em><span class="nocase">Neural Networks and Deep Learning</span></em>. <a href="http://neuralnetworksanddeeplearning.com">http://neuralnetworksanddeeplearning.com</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
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
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../..";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./decisiontrees.html" class="pagination-link" aria-label="Decision Trees">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Decision Trees</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./additional.html" class="pagination-link" aria-label="Additional Methods">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Additional Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>