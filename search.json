[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science",
    "section": "",
    "text": "About this Course\nThis course serves as an introduction to machine learning techniques used in data science. While we will cover some of the underlying theory to get a better understanding of the methods we are going to use, the emphasis will be on practical implementation. Throughout the course, we will be using the programming language Python, which is the dominant programming language in this field.\nThe course is divided into two parts. In the first part, we will get a brief overview of the field, cover some basic concepts of machine learning and have a look at some of the most commonly used methods. In the second part, we will apply these methods to real-world problems, which hopefully will give you a starting point for your own projects. The course outline is as follows:\nPart I: Overview and Methods\nPart II: Applications\nThe course is designed to be self-contained, meaning that you do not need any prior knowledge of machine learning to follow along.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#useful-resources",
    "href": "index.html#useful-resources",
    "title": "Data Science",
    "section": "Useful Resources",
    "text": "Useful Resources\nThe course does not follow a particular textbook but has drawn material from several sources such as\n\nHastie, Tibshirani, and Friedman (2009), “The Elements of Statistical Learning”\nMurphy (2012), “Machine Learning: A Probabilistic Perspective”\nMurphy (2022), “Probabilistic Machine Learning: An Introduction”\nMurphy (2023), “Probabilistic Machine Learning: Advanced Topics”\nGoodfellow, Bengio, and Courville (2016), “Deep Learning”\nBishop (2006), “Pattern Recognition And Machine Learning”\nNielsen (2019), “Neural Networks and Deep Learning”\nSutton and Barto (2018), “Reinforcement Learning: An Introduction”\n\nNote that all of these books are officially available for free in the form of PDFs or online versions (see the links in the references). However, you are not required to read them and, as a word of warning, the books go much deeper into the mathematical theory behind the machine learning techniques than we will in this course. Nevertheless, you may find them useful if you want to learn more about the subject.\nRegarding programming in Python, McKinney (2022) “Python for Data Analysis” might serve as a good reference book. The book is available for free online and covers a lot of the material we will be using in this course. You can find it here: Python for Data Analysis.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#software-installation-notes",
    "href": "index.html#software-installation-notes",
    "title": "Data Science",
    "section": "Software Installation Notes",
    "text": "Software Installation Notes\nWe will be using Python for this course. For simplicity, we will be using the Anaconda distribution, which is a popular distribution of Python (and R) that aims to simplify the management of packages. We will also be using the Visual Studio Code (VS Code) as our code editor.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#anaconda-installation",
    "href": "index.html#anaconda-installation",
    "title": "Data Science",
    "section": "Anaconda Installation",
    "text": "Anaconda Installation\nThe first step is to install the Anaconda distribution:\n\nDownload the Anaconda distribution from anaconda.com. Note: If you are using a M1 Mac (or newer), you have to choose the 64-Bit (M1) Graphical Installer. With an older Intel Mac, you can choose the 64-Bit Graphical Installer. With Windows, you can choose the 64-Bit Graphical Installer (i.e., the only Windows option).\nOpen the installer that you have downloaded in the previous step and follow the on-screen instructions.\nIf it asks you to update Anaconda Navigator at the end, you can click Yes (to agree to the update), Yes (to quit Anaconda Navigator) and then Update Now (to actually start the update).\n\nTo confirm that the installation was successful, you can open a terminal window on macOS/Linux or an Anaconda Prompt if you are on Windows and run the following command:\nconda --version\nThis should display the version of Conda that you have installed. If you see an error message, the installation was likely not successful and you should ask for advice from your peers or send me an email.\n\n\n\nTerminal Output after Anaconda Installation",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#creating-a-conda-environment",
    "href": "index.html#creating-a-conda-environment",
    "title": "Data Science",
    "section": "Creating a Conda Environment",
    "text": "Creating a Conda Environment\nNext, we want to create a new environment for this course that contains the correct Python version and all the Python packages we need. We can do this by creating a new Conda environment from the environment.yml provided on Moodle.\n\nOpen a terminal window on macOS/Linux or an Anaconda Prompt if you are on Windows.\nThere are two ways to create the Conda environment:\nOption A: Run the following command from the terminal or Anaconda Prompt:\nconda env create -f https://datascience.joelmarbet.com/environment.yml\nThis downloads the environment.yml file automatically and creates the environment.\nOption B: Download the environment.yml file manually:\n\nNavigate to the folder where you have downloaded the environment.yml file. On macOS/Linux, you can do this by running the following command in the terminal:\ncd ~/Downloads\nwhich will navigate to the Downloads folder in your home directory.\nOn Windows, you can do this by running the following command in the Anaconda Prompt:\ncd \"%userprofile%/Downloads\"\nwhich will navigate to the Downloads folder in your user profile.\nNote that if you use a different path that contains space you need to put the path in quotes, e.g., cd \"~/My Downloads\".\nCreate a new Conda environment from the environment.yml file by running the following command in the terminal or Anaconda Prompt:\nconda env create -f environment.yml\n\nEither option will create a new Conda environment called datascience_course_cemfi with the correct Python version and all the Python packages we need for this course. Note that the installation might take a few minutes.\nActivate the new Conda environment by running the following command in the terminal or Anaconda Prompt:\nconda activate datascience_course_cemfi \n\nTo confirm that the environment was created successfully, you can run the following command in the terminal or Anaconda Prompt:\npython --version\nThis should display Python version 3.8.8. If you see another Python version you might have forgotten to activate the environment or the environment was not created successfully.\n\n\n\nTerminal Output From Environment Creation\n\n\n\n\n\n\n\n\nResetting or Updating a Conda Environment\n\n\n\nIf you accidentally make changes to the environment and want to reset it to the original state, you can do this by navigating to the folder where you have downloaded environment.yml and then running the following command in the terminal or Anaconda Prompt:\nconda env update --file environment.yml --prune\nAlternatively, you can also update the environment by running the following command in the terminal or Anaconda Prompt, which downloads the environment.yml file automatically from the course website:\nconda env update --file https://datascience.joelmarbet.com/environment.yml --prune\nThis can also be used to update the environment if we add new packages to the environment.yml file.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#installing-vs-code",
    "href": "index.html#installing-vs-code",
    "title": "Data Science",
    "section": "Installing VS Code",
    "text": "Installing VS Code\nThe last step is to install the Visual Studio Code (VS Code) editor:\n\nDownload the Visual Studio Code editor from code.visualstudio.com.\nOpen the installer that you have downloaded in the previous step and follow the on-screen instructions.\n\nWe also need to install some VS Code extensions that will help us with Python programming and Jupyter notebooks:\n\nOpen VS Code.\nClick on the Extensions icon on the left sidebar (or press Cmd+Shift+X on macOS or Ctrl+Shift+X on Windows).\n\n\n\nInstalling Extensions in VSCode\n\n\nSearch for Python and click on the Install button for the extension that is provided by Microsoft.\nSearch for Jupyter and click on the Install button for the extension that is provided by Microsoft.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "index.html#testing-the-installation",
    "href": "index.html#testing-the-installation",
    "title": "Data Science",
    "section": "Testing the Installation",
    "text": "Testing the Installation\nTo test the installation, you can download a Juypter notebook from Moodle and open it in VS Code:\n\nOpen the Jupyter notebook in VS Code.\nClick on Select Kernel in the top right corner of the notebook and choose the datascience_course_cemfi kernel.\n\n\n\nVSCode Jupyter Kernel Selection\n\n\nRun the first cell of the notebook by clicking on the Execute Cell button next to the cell on the left.\n\nIf you see the output of the cell (or a green check mark below the cell), the installation was successful.\n\n\n\n\n\n\nRunning Jupyter Notebooks in the Browser\n\n\n\nIf you have issues running Jupyter notebooks in VSCode, you can also run them in the browser. To do this, you can open a terminal window on macOS/Linux or an Anaconda Prompt if you are on Windows and run the following command:\njupyter notebook\nThis will open a new tab in your default browser with the Jupyter notebook interface. You can then navigate to the folder where you have downloaded the course materials and open the notebooks from there.\n\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Edited by Michael Jordan, Jon Kleinberg, and Bernhard Schölkopf. Information Science and Statistics. Springer Science+Business Media, LLC. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.org.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning - Data Mining, Inference, and Prediction. Second Edition. Springer.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter. Third Edition. O’Reilly Media. https://wesmckinney.com/book/.\n\n\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. Cambridge: MIT Press. https://probml.github.io/pml-book/book0.html.\n\n\n———. 2022. Probabilistic Machine Learning: An Introduction. MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\n———. 2023. Probabilistic Machine Learning: Advanced Topics. MIT Press. https://probml.github.io/pml-book/book2.html.\n\n\nNielsen, Michael. 2019. Neural Networks and Deep Learning. http://neuralnetworksanddeeplearning.com.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book-2nd.html.",
    "crumbs": [
      "About this Course"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Taking Advantage of Machine Learning in Banking Supervision\nYou might have heard of some of the well-known advances in the field of AI from recent years such as\nWhile these examples are impressive, you might wonder how these methods can help you in your work. There is a wide range of potential applications. Machine learning methods have been used in practice to\nto just name a few examples. Bank for International Settlements (2021) provides an overview of how machine learning methods have been used at central banks.1 The report also notes how machine learning methods can be used in the context of financial supervision\nTo give you a few more ideas from academic research, machine learning techniques have been used to, for example,\nIn this course, we will only be able to scratch the surface of the field. However, I hope to provide you with the tools to get you started with machine learning and to apply these methods to novel problems.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#taking-advantage-of-machine-learning-in-banking-supervision",
    "href": "intro.html#taking-advantage-of-machine-learning-in-banking-supervision",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.1: Go board (Source: Wikimedia)\n\n\n\n\n\nDeepMind’s AlphaGo can beat the best human Go players\nOpenAI’s ChatGPT responds to complex text prompts\nMidjourney, DALL-E, and Stable Diffusion generate images from text\n…\n\n\n\nPredict loan or firm defaults,\nDetect fraud (e.g., credit card fraud, or money laundering),\nInterpret large quantities of data, or\nForecast economic variables\n\n\n\nThese techniques can support supervisors’ efficiency in: (i) covering traditional supervisory tasks (eg quality reporting, anomaly detection, sending of instructions); (ii) facilitating the assessment of micro-level fragilities; and (iii) identifying and tackling new emerging topics, such as climate-related financial risks, vulnerabilities from the Covid-19 pandemic, or the consequence of increased digitisation in finance (eg the development of fintechs).\n\n\n\nDetect emotions in voices during press conferences after FOMC meetings (Gorodnichenko, Pham, and Talavera 2023),\nIdentify Monetary Policy Shocks using Natural Language Processing (Aruoba and Drechsel 2022),\nSolve macroeconomic models with heterogeneous agents (Maliar, Maliar, and Winant 2021; Fernández-Villaverde, Hurtado, and Nuño 2023; Fernández-Villaverde et al. 2024; Kase, Melosi, and Rottner 2022), or\nEstimate structural models with the help of neural networks (Kaji, Manresa, and Pouliot 2023).",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-machine-learning",
    "href": "intro.html#what-is-machine-learning",
    "title": "1  Introduction",
    "section": "1.2 What Is Machine Learning?",
    "text": "1.2 What Is Machine Learning?\nYou might already have some idea of what machine learning is. In this section, we will provide a more formal definition, distinguish between machine learning, artificial intelligence, and deep learning, and discuss the relation to statistics and econometrics.\n\n1.2.1 Definition\nLet’s start with the straightforward definition provided by Murphy (2012)\n\n[…] a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty […]\n\nTherefore, machine learning provides a range of methods for data analysis. In that sense, it is similar to statistics or econometrics.\nA popular, albeit more technical, definition of ML is due to Mitchell (1997):\n\nA computer program is said to learn from experience \\(E\\) with respect to some class of tasks \\(T\\), and performance measure \\(P\\), if its performance at tasks in \\(T\\), as measured by \\(P\\), improves with experience \\(E\\).\n\nIn the context of this course, experience \\(E\\) is given by a dataset that we feed into a machine-learning algorithm, tasks \\(T\\) are usually some form of prediction that we would like to perform (e.g., loan default prediction), and the performance measure \\(P\\) is the measure assessing the accuracy of our predictions.\n\n\n1.2.2 Common Terminology\nArtificial intelligence (AI), machine learning (ML), and deep learning (DL) are often used interchangeably in the media. However, they describe more narrow subfields (Microsoft 2024):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Artificial intelligence vs. Machine Learning vs. Deep Learning\n\n\n\n\nArtificial Intelligence (AI): Any method allowing computers to imitate human behavior.\nMachine Learning (ML): A subset of AI including methods that allow machines to improve at tasks with experience.\nDeep Learning (DL): A subset of ML using neural networks with many layers allowing machines to learn how to perform tasks.\n\nMore recently, with the rise of large language models (LLMs) such as ChatGPT, the term Generative AI has also become popular. Generative AI refers to AI models that can generate new content, such as text, images, or music, based on the patterns learned from their training data. ChatGPT is an example of a generative AI model that generates human-like text responses based on the input it receives. In this course, we will be more concerned with what is sometimes called Predictive AI. Predictive AI refers to machine learning models used to make predictions or classifications based on input data, such as predicting loan defaults or classifying images.\nA term you may also encounter is Artificial General Intelligence (AGI), which refers to highly autonomous systems that possess the ability to understand, learn, and apply knowledge across a wide range of tasks at a level comparable to human intelligence. Unlike current AI systems, which are specialized for specific tasks, AGI would be capable of general reasoning and problem-solving. While AGI is a topic of significant research and debate, it remains largely theoretical at this stage.2 Please note that AGI and related concepts are beyond the scope of this course and will not be covered.\n\n\n1.2.3 Relation to Statistics and Econometrics\nWe have already mentioned that machine learning is similar to statistics and econometrics, in the sense that it provides a set of methods for data analysis. The focus of machine learning is more on prediction rather than causality meaning that in machine learning we are often interested in whether we can predict A given B rather than whether B truly causes A. For example, we could probably predict the sale of sunburn lotion on a day given the sales of ice cream on the previous day. However, this does not mean that ice cream sales cause sunburn lotion sales, it is just that the sunny weather on the first day causes both.\nVarian (2014) provides another example showing the difference between prediction and causality:\n\nA classic example: there are often more police in precincts with high crime, but that does not imply that increasing the number of police in a precinct would increase crime. […] If our data were generated by policymakers who assigned police to areas with high crime, then the observed relationship between police and crime rates could be highly predictive for the historical data but not useful in predicting the causal impact of explicitly assigning additional police to a precinct.\n\n\nNevertheless, leaving problems aside where we are interested in causality, there is still a very large range of problems where we are interested in mere prediction, such as loan default prediction, or credit card fraud detection.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-has-machine-learning-become-popular-only-recently",
    "href": "intro.html#why-has-machine-learning-become-popular-only-recently",
    "title": "1  Introduction",
    "section": "1.3 Why Has Machine Learning Become Popular Only Recently?",
    "text": "1.3 Why Has Machine Learning Become Popular Only Recently?\nEarly contributions to the field reach back at least to McCulloch and Pitts (1943) and Rosenblatt (1958). They attempted to find mathematical representations of information processing in biological systems (Bishop 2006). The field has grown substantially mainly in recent years due to\n\nAdvances in computational power of personal computers\nIncreased availability of large datasets \\(\\rightarrow\\) “big data”\nImprovements in algorithms\n\nThe need for large data sets still limits the applicability to certain fields. For example, in macroeconomic forecasting, we usually only have quarterly data for 40-50 years. Conventional time series methods (e.g., ARIMA) often still tend to perform better than ML methods (e.g., neural networks).",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-learning",
    "href": "intro.html#types-of-learning",
    "title": "1  Introduction",
    "section": "1.4 Types of Learning",
    "text": "1.4 Types of Learning\nMachine learning methods are commonly distinguished based on the tasks that we would like to perform, and the data that we have access to for learning how to perform said task. For example, our task might be to figure out whether a credit card transaction is fraudulent or not. Based on the data we have access to, two different types of learning might be appropriate:\n\nWe know which transactions are fraudulent meaning that we need to learn a function that maps the transaction data (e.g., value of transaction, location, etc.) to the label “fraudulent” or “not fraudulent”. This is an example of supervised learning.\nWe do not know whether they are fraudulent or not meaning that we might want to find clusters in the data that group similar transactions. This is an example of unsupervised learning.\n\nMore generally, ML methods are commonly categorized into\n\nSupervised Learning: Learn function \\(y=f(x)\\) from data that you observe for \\(x\\) and \\(y\\)\nUnsupervised Learning: “Make sense” of observed data \\(x\\)\nReinforcement Learning: Learn how to interact with the environment\n\nThe focus of this course will be on supervised learning, but we will also have a look at some unsupervised learning techniques if time allows. Let’s have a closer look at the three types of learning.\n\n\n\n\n\n\nTypes of Learning in Practice\n\n\n\nMachine learning models might combine different types of learning. For example, ChatGPT is trained using a combination of self-supervised (a form of unsupervised learning), supervised and reinforcement learning. Furthermore, some machine learning methods, such as neural networks, might be used as part of different types of learning.\n\n\n\n1.4.1 Supervised Learning\nSupervised learning is probably the most common form of machine learning. In supervised learning, we have a training dataset consisting of input-output pairs \\((x_n, y_n)\\) for \\(n=1,\\ldots,N\\). The goal is to learn a function \\(f\\) that maps inputs \\(x\\) to outputs \\(y\\).\nThe type of function \\(f\\) might be incredibly complex, e.g.\n\nFrom images of cats and dogs \\(x\\) to a classification of the image \\(y\\) (\\(\\rightarrow\\) Figure 1.3)\nFrom text input \\(x\\) to some coherent text response \\(y\\) (\\(\\rightarrow\\) ChatGPT)\nFrom text input \\(x\\) to a generated image \\(y\\) (\\(\\rightarrow\\) Midjourney)\nFrom bank loan application form \\(x\\) to a loan decision \\(y\\)\n\nRegarding terminology, note that sometimes\n\nInputs \\(x\\) are called features, predictors, or covariates,\nOutputs \\(y\\) are called labels, targets, or responses.\n\nBased on the type of output, we can distinguish between\n\nClassification: Output \\(y\\) is in a set of mutually exclusive labels (i.e., classes), i.e. \\(\\mathcal{Y}=\\{1,2,3,\\ldots,C\\}\\)\nRegression: Output \\(y\\) is a real-valued quantity, i.e. \\(y\\in\\mathbb{R}\\)\n\nLet’s have a closer look at some examples of classification and regression tasks.\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: Training a machine learning algorithm to classify images of cats and dogs\n\n\n\nFigure 1.3 shows an example of a binary classification task. The algorithm is trained on a dataset of images of cats and dogs. The goal is to predict the label (i.e., “cat” or “dog”) of a new image (new in the sense that the images were not part of the training dataset). After training, the algorithm can predict the label of new images with a certain degree of accuracy. However, if you give the algorithm an image of, e.g., a horse it might mistakenly predict that it is a dog because the algorithm has never seen an image like that before and because it has been trained only for binary classification (it only knows two kinds of classes, “cats” and “dogs”). In this example, \\(x\\) would be an image in the training dataset and \\(y\\) would be the label of that image.\nExtending the training dataset to also include images of horses with a corresponding label would turn the tasks into multiclass classification.\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.4: Linear and Polynomial Regression\n\n\n\nIn regression tasks, the variable that we want to predict is continuous. Linear and polynomial regression in Figure 1.4 are a form of supervised learning. Thus, you are already familiar with some basic ML techniques from the statistics and econometrics courses.\nAnother common way to solve regression tasks is to use neural networks, which can learn highly non-linear relationships. In contrast to, for example, polynomial regression, neural networks can learn these relationships without the need to specify the functional form (i.e., whether it is quadratic as in Figure 1.4) of the relationship. This makes them very flexible and powerful tools. We will have a look at neural networks later on.\n\n\n\n1.4.2 Unsupervised Learning\nAn issue with supervised learning is that we need labeled data which is often not available. Unsupervised learning is used to explore data and to find patterns that are not immediately obvious. For example, unsupervised learning could be used to find groups of customers with similar purchasing behavior in a dataset of customer transactions. Therefore, the task is to learn some structure in the data \\(x\\). Note that we only have features in the dataset and no labels, i.e., the training dataset consists of \\(N\\) data points \\(x_n\\).\nUnsupervised learning tasks could be, for example,\n\nFinding clusters in the data, i.e. finding data points that are “similar” (\\(\\rightarrow\\) clustering)\nFinding latent factors that capture the “essence” of the data (\\(\\rightarrow\\) dimensionality reduction)\n\nLet’s have a look at some examples of clustering and dimensionality reduction.\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Clusters in data on iris flowers (left-hand side: true classes, right-hand side: k-means clusters)\n\n\n\nClustering is a form of unsupervised learning where the goal is to group data points into so-called clusters based on their similarity. We want to find clusters in the data such that observations within a cluster are more similar to each other than to observations in other clusters.\nFigure 1.5 shows an example of a clustering task. The dataset consists of measurements of sepal (and petal) length and width of three species of iris flowers. The goal is to find clusters based on just the similarity in sepal and petal lengths and widths without relying on information about the actual iris flower species. The left-hand panel of Figure 1.5, shows the actual classification of the iris flowers. The right-hand side shows the result of a k-means clustering algorithm that groups the data points into three clusters.\n\n\n\n\n\n\nFigure 1.6: Petal vs Sepal (Source: Wikimedia)\n\n\n\n\n\nDimensionality Reduction\nSuppose you observe data on house prices and many variables describing each house. You might observe, e.g., property size, number of rooms, room sizes, proximity to the closest supermarket, and hundreds of variables more. A ML algorithm (e.g., principal component analysis or autoencoders) could find the unobserved factors that determine house prices. These factors sometimes (but not always) have an interpretation. For example, a factor driving house prices could be amenities. This factor could summarize variables such as proximity to the closest supermarket, number of nearby restaurants, etc. Ultimately, hundreds of explanatory variables in the data set might be represented by a small number of factors.\n\n\n\n1.4.3 Reinforcement Learning\nIn reinforcement learning, an agent learns how to interact with its environment. The agent receives feedback in the form of rewards or penalties for its actions. The goal is to learn a policy that maximizes the total reward.\nFor example, a machine could learn to play chess using reinforcement learning\n\nInput \\(x\\) would be the current position (i.e., the position of pieces on the board)\nAction \\(a\\) would be the next move to make given the position\nOne also needs to define a reward (e.g., winning the game at the end)\nGoal is then to find \\(a=\\pi(x)\\) to maximize the reward\n\nThis is also the principle behind AlphaGo that learned how to play Go.\nAnother example is MarI/O which learned how to play Super Mario World. The algorithm learns to play the game by receiving feedback in the form of rewards (e.g., points for collecting coins, penalties for dying) and then improves in playing the game by “an advanced form of trial and error”.\n\nIn this course, we will focus on supervised learning. However, we will look at some unsupervised learning techniques if time allows. Reinforcement learning is going beyond the scope of this course and will not be covered.\n\n\n\n\n\n\n\nMini-Exercise\n\n\n\nAre the following tasks examples of supervised, unsupervised, or reinforcement learning?\n\nPredicting the price of a house based on its size and location (given a dataset of house prices and features).\nFinding groups of customers with similar purchasing behavior (given a dataset of customer transactions and customer characteristics).\nDetecting fraudulent credit card transactions (given a dataset of unlabeled credit card transactions).\nDetecting fraudulent credit card transactions (given a dataset of labeled credit card transactions).\nRecognizing handwritten digits in the MNIST dataset (see next section).\nGrouping news articles by topic based only on their content (without knowing the topics in advance).\nPredicting whether a customer will cancel their subscription next month, given historical data on customer behavior and cancellations.\nClassifying emails as spam or not spam, using a dataset where each email is labeled as spam or not.\nSegmenting images into regions with similar colors, without any information about what the regions represent.\nTraining a robot to navigate a maze by receiving rewards for reaching the exit and penalties for hitting walls.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#popular-practice-datasets",
    "href": "intro.html#popular-practice-datasets",
    "title": "1  Introduction",
    "section": "1.5 Popular Practice Datasets",
    "text": "1.5 Popular Practice Datasets\nThere are many publicly available datasets that you can use to learn how to implement machine learning methods. Here are some well-known platforms with a large collection of datasets\n\nKaggle,\nHuggingFace, and\nOpenML.\n\nAnother good source for practice datasets is the collection of datasets provided by scikit-learn. These datasets can be easily loaded into Python from the scikit-learn package. Furthermore, Murphy (2022) provides an overview of some well-known datasets that are often used in machine learning. For example, MNIST is a dataset of handwritten digits (see Figure 1.7) that is often used to test machine learning algorithms. The dataset consists of 60,000 training images and 10,000 test images. Each image is a 28x28 pixel image of a handwritten digit. The goal is to predict the digit in the image.\n\n\n\n\n\n\nFigure 1.7: MNIST (Source: Wikimedia)\n\n\n\n\n\n\n\nAruoba, S. Boragan, and Thomas Drechsel. 2022. “Identifying Monetary Policy Shocks: A Natural Language Approach.” CEPR Discussion Paper DP17133. CEPR.\n\n\nBank for International Settlements. 2021. “Machine learning applications in central banking.” IFC Bulletin 57. https://www.bis.org/ifc/publ/ifcb57.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Edited by Michael Jordan, Jon Kleinberg, and Bernhard Schölkopf. Information Science and Statistics. Springer Science+Business Media, LLC. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nFernández-Villaverde, Jesús, Samuel Hurtado, and Galo Nuño. 2023. “Financial Frictions and the Wealth Distribution.” Econometrica 91 (3): 869–901. https://doi.org/10.3982/ecta18180.\n\n\nFernández-Villaverde, Jesús, Joël Marbet, Galo Nuño, and Omar Rachedi. 2024. “Inequality and the Zero Lower Bound.” Working Paper 2407. Banco de España.\n\n\nGorodnichenko, Yuriy, Tho Pham, and Oleksandr Talavera. 2023. “The Voice of Monetary Policy.” American Economic Review 113 (2): 548–84. https://doi.org/10.1257/aer.20220129.\n\n\nKaji, Tetsuya, Elena Manresa, and Guillaume Pouliot. 2023. “An Adversarial Approach to Structural Estimation.” Econometrica 91 (6): 2041–63. https://doi.org/10.3982/ecta18707.\n\n\nKase, Hanno, Leonardo Melosi, and Matthias Rottner. 2022. “Estimating Nonlinear Heterogeneous Agents Models with Neural Networks.” Federal Reserve Bank of Chicago. https://doi.org/10.21033/wp-2022-26.\n\n\nMaliar, Lilia, Serguei Maliar, and Pablo Winant. 2021. “Deep learning for solving dynamic economic models.” Journal of Monetary Economics 122 (September): 76–101. https://doi.org/10.1016/j.jmoneco.2021.07.004.\n\n\nMcCulloch, Warren S., and Walter Pitts. 1943. “A logical calculus of the ideas immanent in nervous activity.” The Bulletin of Mathematical Biophysics 5 (4): 115–33. https://doi.org/10.1007/bf02478259.\n\n\nMicrosoft. 2024. “Deep learning vs. machine learning in Azure Machine Learning.” Website. https://learn.microsoft.com/en-us/azure/machine-learning/concept-deep-learning-vs-machine-learning?view=azureml-api-2.\n\n\nMitchell, Tom. 1997. Machine Learning. McGraw Hill. https://www.cs.cmu.edu/~tom/mlbook.html.\n\n\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. Cambridge: MIT Press. https://probml.github.io/pml-book/book0.html.\n\n\n———. 2022. Probabilistic Machine Learning: An Introduction. MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nRosenblatt, F. 1958. “The perceptron: A probabilistic model for information storage and organization in the brain.” Psychological Review 65 (6): 386–408. https://doi.org/10.1037/h0042519.\n\n\nVarian, Hal R. 2014. “Big Data: New Tricks for Econometrics.” Journal of Economic Perspectives 28 (2): 3–28. https://doi.org/10.1257/jep.28.2.3.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "See https://www.bis.org/ifc/publ/ifcb57.htm for a more detailed overview.↩︎\nFor more information, see, e.g., https://cloud.google.com/discover/what-is-artificial-general-intelligence?hl=en.↩︎",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  Basic Concepts",
    "section": "",
    "text": "2.1 Linear Regression in a ML Context\nYou have already extensively studied linear regressions in the statistics and econometrics course, so we will not discuss it in much detail. In machine learning, it is common to talk about weights \\(w_i\\) and biases \\(b_i\\) instead of coefficients \\(\\beta_i\\) and intercept \\(\\beta_0\\), i.e., the linear regression model would be written as\n\\[y_n = b + \\sum_{i=1}^m w_i x_{i,n} + \\varepsilon_n \\qquad n=1,\\ldots,N\\]\nwhere \\(w_i\\) are the weights, \\(b\\) is the bias and \\(N\\) is the sample size. The weights and biases are found by minimizing the empirical risk function or mean squared error (MSE) loss, which is a measure of how well the model fits the data.\n\\[\\text{MSE}(y,x; w, b) = \\frac{1}{N}\\sum_{n=1}^N (y_n - \\hat{y}_n)^2\\]\nwhere \\(y_n\\) is the true value, \\(\\hat{y}_n\\) is the predicted (or fitted) value for observation \\(n\\).\nIn the case of linear regression, there is a closed-form solution for the weights and biases that minimize the MSE. However, the weights and biases have to be found numerically in many other machine learning models since there is no closed-form solution. One can think of this, as the machine learning algorithm automatically moving a slider for the slope Figure 2.2 until the loss is minimized (i.e., the red dot is at the lowest possible point) and the model fits the data as well as possible.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "basics.html#linear-regression-in-a-ml-context",
    "href": "basics.html#linear-regression-in-a-ml-context",
    "title": "2  Basic Concepts",
    "section": "",
    "text": "{\n    //\n    const randomDataRange = [-4, 4];\n    const lossRange = [0, 11];\n\n    // Generate random data\n    const seed = 0.25160386911120525; // a number in [0,1)\n    const source = d3.randomLcg(seed);\n    const randn = d3.randomNormal.source(source)(0, 1);\n    let xTmp = Array.from({length: 100}, randn);\n    let yTmp = Array.from({length: 100}, randn);\n    let randomData = prepareData(xTmp, yTmp);\n\n    function prepareData(xTmp, yTmp) {\n\n        let data = [];\n\n        for (let i = 0; i &lt; xTmp.length; i += 1) {\n            let dataPoint = {\"x\": xTmp[i], \"y\": xTmp[i] + yTmp[i]};\n            data.push(dataPoint)\n        }\n\n        return data;\n\n    }\n\n    // Loss function\n    function sumOfSquares(slope) {\n\n        let sum = 0;\n\n        for (let i = 0; i &lt; randomData.length; i += 1) {\n            sum += (randomData[i].y - slope * randomData[i].x) ** 2 / randomData.length;\n        }\n\n        return sum;\n\n    }\n\n    function computeLossCurve(inputRange) {\n\n        let loss = [];\n        let N = 100;\n\n        for (let i = 0; i &lt;= 100; i += 1) {\n            let x = inputRange[0] + i * (inputRange[1] - inputRange[0])/100\n            let dataPoint = {\"x\": x, \"y\": sumOfSquares(x)};\n            loss.push(dataPoint)\n        }\n\n        return loss;\n\n    }\n\n    // Data for regression line\n    let slopeData = [\n        {x: (slope &gt; 1 || slope &lt; -1)  ? randomDataRange[0]/slope : randomDataRange[0], y: (slope &gt; 1 || slope &lt; -1)  ? randomDataRange[0] : randomDataRange[0]*slope},\n        {x: (slope &gt; 1 || slope &lt; -1)  ? randomDataRange[1]/slope : randomDataRange[1], y: (slope &gt; 1 || slope &lt; -1)  ? randomDataRange[1] : randomDataRange[1]*slope}\n    ];\n\n    // Data for loss marker\n    let markerData = [\n        {x: slope, y: sumOfSquares(slope) }\n    ];\n\n    // Data for loss curve\n    let lossCurveData = computeLossCurve(inputRange);\n\n    const sets = [1, 2];\n    return html`&lt;div style=\"display:flex; flex-wrap: wrap;\"&gt;\n    ${sets\n    .map((index) =&gt;\n        Plot.plot({\n            marginLeft: 40,\n            marginTop: 20,\n            width: width / 2 - 2,\n            height: width / 3 - 2,\n            x: { domain: index === 1 ? randomDataRange : inputRange, label: index === 1 ? \"x\" : \"Weight\"},\n            y: { domain: index === 1 ? randomDataRange : lossRange, label: index === 1 ? \"y\" : \"Loss\"},\n            marks: [\n                Plot.frame(),\n                index === 1\n                    ? Plot.dot(randomData, {x: \"x\", y: \"y\", fill: \"steelblue\"})\n                    : Plot.line(lossCurveData, {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n                index === 1\n                    ? Plot.line(slopeData, {x: \"x\", y: \"y\", stroke: \"red\"})\n                    : Plot.dot(markerData, {x: \"x\", y: \"y\", fill: \"red\", r: 6}),\n                Plot.text([index], {\n                    x: index === 1 ? (randomDataRange[1] + randomDataRange[0])/2 : (inputRange[1] + inputRange[0])/2,\n                    y: index === 1 ? ((randomDataRange[1] - randomDataRange[0]) * 0.1 + randomDataRange[1]) : ((lossRange[1] - lossRange[0]) * 0.1 + lossRange[1]),\n                    text: (d) =&gt; index === 1 ? \"Data + Regression Line\" : \"Mean Squared Error (MSE)\",\n                    textAnchor: \"middle\"\n                })\n            ],\n            nice: true\n        })\n    )\n    .flat()}`;\n}\n\n\n\n\n\n\n\ninputRange = [-2, 4];\nviewof slope = Inputs.range(inputRange, {label: \"Weight (w)\", step: 0.01})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Interactive Linear Regression With a Single Feature \\(x\\) (i.e., \\(m=1\\)) and Bias \\(b=0\\)",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "basics.html#logistic-regression-in-a-ml-context",
    "href": "basics.html#logistic-regression-in-a-ml-context",
    "title": "2  Basic Concepts",
    "section": "2.2 Logistic Regression in a ML Context",
    "text": "2.2 Logistic Regression in a ML Context\nLogistic regression is a widely used classification model \\(p(y|x;w,b)\\) where \\(x\\in \\mathbb{R}^m\\) is an input vector, and \\(y\\in\\{0,1,\\ldots,C\\}\\) is a class label. We will focus on the binary case, meaning that \\(y\\in\\{0,1\\}\\) but it is also possible to extend this to more than two classes. The probability that \\(y_n\\) is equal to \\(1\\) for observation \\(n\\) is given by \\[p(y_n=1|x_n;w,b)= \\frac{1}{1+\\exp(-b-\\sum_{i=1}^m w_i x_{i,n})}\\]\nwhere \\(w=[w_1, \\ldots, w_n]'\\in \\mathbb{R}^m\\) is a weight vector, and \\(b\\) is a bias term. Combining the probabilities for each observation \\(n\\), we can write the likelihood function as\n\\[\\mathcal{L}(w,b) = \\prod_{n=1}^N p(y_n=1|x_n;w,b)^{y_n} \\left(1-p(y_n=1|x_n;w,b)\\right)^{1-y_n}\\]\nor taking the natural logarithm of the likelihood function, we get the log-likelihood function\n\\[\\log\\mathcal{L}(w,b) = \\sum_{n=1}^N y_n \\log p(y_n=1|x_n;w,b) + (1-y_n) \\log \\left(1-p(y_n=1|x_n;w,b)\\right).\\]\nTo find the weights and biases, we need to numerically maximize the log-likelihood function (or minimize \\(-\\log\\mathcal{L}(w,b)\\)).\nAdding a classification threshold \\(t\\) to a logistic regression yields a decision rule of the form\n\\[\\hat{y}=1 \\Leftrightarrow p(y=1|x;w,b) &gt; t,\\]\ni.e., the model predicts that \\(y=1\\) if \\(p(y=1|x;w,b) &gt; t\\).\n\n\n\n{\n    //\n    const randomDataRange = [-2, 2];\n    const loglikelihoodRange = [-20, -8];\n\n    // Generate random data\n    const seed = 0.25160386911120525; // a number in [0,1)\n    const source = d3.randomLcg(seed);\n    const rand = d3.randomUniform.source(source)();\n    let xTmp = Array.from({length: 100}, rand).map((x) =&gt; 4*x-2);\n    let pTmp = xTmp.map((x) =&gt; 1/(1+Math.exp(-8*x)));\n    let cTmp = xTmp.map((x) =&gt; (1/(1+Math.exp(-slope2*x)) &gt;= thresholdValue) ? 1 : 0); // Classification based on threshold and slope2\n    let yTmp = pTmp.map((p) =&gt; Array.from({length: 1}, rand)[0] &lt; p ? 1 : 0);\n    let randomData = prepareData(xTmp, yTmp, cTmp);\n\n    function prepareData(xTmp, yTmp, cTmp) {\n\n        let data = [];\n\n        for (let i = 0; i &lt; xTmp.length; i += 1) {\n            let dataPoint = {\"x\": xTmp[i], \"y\": yTmp[i], \"class\": cTmp[i]};\n            data.push(dataPoint)\n        }\n\n        return data;\n\n    }\n\n    // Regression line\n    let regData = computeLogisticRegressionLine(slope2, randomDataRange, 100)\n\n    function computeLogisticRegressionLine(slope, inputRange, N) {\n\n        let data = [];\n\n        for (let i = 0; i &lt;= N; i += 1) {\n            let x = inputRange[0] + i * (inputRange[1] - inputRange[0])/100;\n            let dataPoint = {\"x\": x, \"y\": 1/(1+Math.exp(-slope*x))};\n            data.push(dataPoint);\n        }\n\n        return data;\n\n    }\n\n    // Log-likelihood function\n    function computeLogLikelihood(slope, data) {\n\n        let loglikelihood = 0;\n\n        for (let i = 0; i &lt; data.length; i += 1) {\n            loglikelihood += data[i].y * Math.log(1/(1+Math.exp(-slope*data[i].x))) + (1-data[i].y) * Math.log(1-1/(1+Math.exp(-slope*data[i].x)));\n        }\n\n        return loglikelihood;\n\n    }\n\n    function computeLossCurve(inputRange, data, N) {\n\n        let loss = [];\n\n        for (let i = 0; i &lt;= N; i += 1) {\n            let x = inputRange[0] + i * (inputRange[1] - inputRange[0])/100;\n            let dataPoint = {\"x\": x, \"y\": computeLogLikelihood(x, data)};\n            loss.push(dataPoint);\n        }\n\n        return loss;\n\n    }\n\n    let loglikelihoodCurveData = computeLossCurve(inputRange2, randomData, 100);\n    let markerData = [\n        {x: slope2, y: computeLogLikelihood(slope2, randomData) }\n    ];\n\n    // Data for threshold\n    let threshData = [\n        {x: randomDataRange[0], y: thresholdValue},\n        {x: randomDataRange[1], y: thresholdValue}\n    ];\n\n    const sets = [1, 2];\n    return html`&lt;div style=\"display:flex; flex-wrap: wrap;\"&gt;\n    ${sets\n    .map((index) =&gt;\n        Plot.plot({\n            marginLeft: 40,\n            marginTop: 20,\n            width: width / 2 - 2,\n            height: width / 3 - 2,\n            x: { domain: index === 1 ? [-2, 2] : inputRange2, label: index === 1 ? \"X\" : \"Weight\"},\n            y: { domain: index === 1 ? [0, 1] : loglikelihoodRange, label: index === 1 ? \"Y\" : \"log(L(w,b))\"},\n            color: {\n                domain: [0, 1],\n                scheme: \"Paired\"\n            },\n            marks: [\n                Plot.frame(),\n                index === 1\n                    ? Plot.dot(randomData, {x: \"x\", y: \"y\", r: 4, fill: threshold.includes(\"\") ? \"class\" : \"steelblue\"})\n                    : Plot.line(loglikelihoodCurveData, {x: \"x\", y: \"y\", stroke: \"steelblue\"}),\n                index === 1\n                    ? Plot.line(regData, {x: \"x\", y: \"y\", stroke: \"red\"})\n                    : Plot.dot(markerData, {x: \"x\", y: \"y\", fill: \"red\", r: 6}),\n                threshold.includes(\"\")\n                    ? Plot.line(threshData, {x: \"x\", y: \"y\", stroke: \"black\", strokeDasharray: \"10,5\"})\n                    : null,\n                Plot.text([index], {\n                    x: index === 1 ? (randomDataRange[1] + randomDataRange[0])/2 : (inputRange2[1] + inputRange2[0])/2,\n                    y: index === 1 ? 1.1 : ((loglikelihoodRange[1] - loglikelihoodRange[0]) * 0.1 + loglikelihoodRange[1]),\n                    text: (d) =&gt; index === 1 ? \"Data + Regression Line\" : \"Log-Likelihood\",\n                    textAnchor: \"middle\"\n                })\n            ],\n            \n            nice: true\n        })\n    )\n    .flat()}`;\n}\n\n\n\n\n\n\n\ninputRange2 = [-2, 18];\nviewof slope2 = Inputs.range(inputRange2, {label: \"Weight (w)\", step: 0.01})\nviewof threshold = Inputs.checkbox([\"\"], {label: \"Classification Threshold\"})\nviewof thresholdValue = Inputs.range([0,1], {value: 0.5, label: \"Threshold Value (t)\", step: 0.01})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Interactive Logistic Regression With a Single Feature \\(x\\) (i.e., \\(m=1\\)) and Bias \\(b=0\\) (Classification Legend: dark blue = 1, light blue = 0)\n\n\n\n\n\n\n\n\nTerminology: Regression vs. Classification\n\n\n\nDo not get confused about the fact that it is called logistic regression but is used for classification tasks. Logistic regression provides an estimate of the probability that \\(y=1\\) for given \\(x\\), i.e., an estimate for \\(p(y=1|x;w,b)\\). To turn, this into a classification model, we also need a classification threshold value for \\(p(y=1|x;w,b)\\) above which we classify an observation as \\(y=1\\).\n\n\nFigure 2.4 shows an interactive example of a logistic regression model. The left-hand side shows the data points and the regression line. The right-hand side shows the log-likelihood function with the red dot showing the value of the log-likelihood for the current value of \\(w\\). The goal is to find the weight \\(w\\) in the regression line that maximizes the log-likelihood function (we assumed \\(b=0\\) for simplicity).\nIf you enable the classification threshold \\(t\\), a data point is shown as dark blue if \\(p(y=1|x;w,b) &gt; t\\), otherwise, it is shown in light blue. Note how the value of the threshold affects the classification of the data points for points in the middle. Essentially, for each classification threshold, we have a different classification model. But how do we choose the classification threshold? This is a topic that we will discuss in the next section.\nLogistic regression belongs to the class of generalized linear models with logit as the link function. We could write\n\\[\\log\\left(\\frac{p}{1-p}\\right)= b+\\sum_{i=1}^m w_i x_{i,n}\\]\nwhere \\(p=p(y_n=1|x_n;w,b)\\), which separates the linear part on the right-hand side from the logit on the left-hand side.\n\n\n\n{\n    //\n    const randomDataRange = [-2, 2];\n    const loglikelihoodRange = [-20, -8];\n\n    // Generate random data\n    const seed = 0.25160386911120525; // a number in [0,1)\n    const source = d3.randomLcg(seed);\n    const rand = d3.randomUniform.source(source)();\n    let x1Tmp = Array.from({length: 200}, rand).map((x) =&gt; (randomDataRange[1] - randomDataRange[0]) * x + randomDataRange[0]);\n    let x2Tmp = Array.from({length: 200}, rand).map((x) =&gt; (randomDataRange[1] - randomDataRange[0]) * x + randomDataRange[0]);\n    let pTmp = x1Tmp.map((x1, i) =&gt; 1/(1+Math.exp(-0.5 - 3 * x1 - 4 * x2Tmp[i] - 2 * Math.pow(x1, 2)))); // \"True\" model\n    //(usSquare.includes(\"\") ? 2 * Math.pow(x1, 2) : 0)\n    let yTmp = pTmp.map((p) =&gt; Array.from({length: 1}, rand)[0] &lt; p ? 1 : 0);\n    let randomData = prepareData(yTmp, x1Tmp, x2Tmp);\n\n    function prepareData(yTmp, x1Tmp, x2Tmp) {\n\n        let data = [];\n\n        for (let i = 0; i &lt; x1Tmp.length; i += 1) {\n            let dataPoint = {\"x1\": x1Tmp[i], \"x2\": x2Tmp[i], \"y\": yTmp[i]};\n            data.push(dataPoint)\n        }\n\n        return data;\n\n    }\n\n    // Decision boundary\n    let regData = computeDecisionBoundary(weight1, weight2, weight3, bias, 0.5, randomDataRange, 100)\n\n    function computeDecisionBoundary(w1, w2, w3, b, t, inputRange, N) {\n\n        let data = [];\n\n        for (let i = 0; i &lt;= N; i += 1) {\n            let x1 = inputRange[0] + i * (inputRange[1] - inputRange[0])/100;\n            let x2 = 1/w2 * (-b - w1 * x1 - (useSquare.includes(\"\") ? w3 * Math.pow(x1,2)  : 0) - Math.log((1-t)/t));\n            x2 = Math.min(Math.max(x2, inputRange[0]), inputRange[1])\n            let c = 1/(1+Math.exp(-b - w1 * x1 - w2 * x2 - 2 * Math.pow(x1, 2) + 3 * Math.pow(x1, 3)) &lt; t ? 0 : 1);\n            let dataPoint = {\"x1\": x1, \"x2\": x2, \"lb\": inputRange[0], \"ub\": inputRange[1], \"c0\": 0, \"c1\": 1};\n            data.push(dataPoint);\n        }\n\n        return data;\n\n    }\n\n\n    return Plot.plot({\n            marginLeft: 40,\n            marginTop: 20,\n            width: width - 2,\n            height: width * 2/3 - 2,\n            x: { domain: randomDataRange, label: \"x₁\"},\n            y: { domain: randomDataRange, label: \"x₂\"},\n            color: {\n                domain: [0, 1],\n                scheme: \"Category10\"\n            },\n            marks: [\n                Plot.frame(),\n                Plot.dot(randomData, {x: \"x1\", y: \"x2\", r: 4, fill: \"y\"}),\n                Plot.areaY(regData, {x: \"x1\", y: \"x2\", y2: \"lb\", fill: \"c0\", fillOpacity: 0.2}),\n                Plot.areaY(regData, {x: \"x1\", y: \"x2\", y2: \"ub\", fill: \"c1\", fillOpacity: 0.2}),\n                Plot.line(regData, {x: \"x1\", y: \"x2\"})\n            ],\n            \n            nice: true\n        })\n}\n\n\n\n\n\n\n\nviewof weight1 = Inputs.range([-5, 5], {value: 3, label: \"Weight w₁\", step: 0.01})\nviewof weight2 = Inputs.range([0.1, 10], {value: 4, label: \"Weight w₂\", step: 0.01})\nviewof bias = Inputs.range([-10, 10], {value: 0.5, label: \"Bias b\", step: 0.01})\nviewof useSquare = Inputs.checkbox([\"\"], {label: \"Include square of x₁\"})\nviewof weight3 = Inputs.range([-5, 5], {value: 2, label: \"w₃\", step: 0.01})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Decision Boundary - Logistic Regression with Features \\(x_1\\) and \\(x_2\\) (and optionally \\(x_1^2\\))\n\n\nThis linearity also shows up in the linear decision boundary produced by a logistic regression in Figure 2.6. A decision boundary shows how a machine-learning model separates different classes in our data, i.e, how it would classify an arbitrary combination of \\((x_1,x_2)\\). This linearity of the decision boundary can pose a problem if the two classes are not linearly separable as in Figure 2.6. We can remedy this issue by including higher order terms for \\(x_1\\) and \\(x_2\\) such as \\(x_2^2\\) or \\(x_1^3\\), which is a type of feature engineering. However, there are many forms of non-linearity that the decision boundary can have and we cannot try all of them. You might know the following phrase from a Tolstoy book\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.”\n\nIn the context of non-linear functions, people sometimes say\n\n“Linear functions are all alike; every non-linear function is non-linear in its own way.”\n\nDuring the course, we will learn more advanced machine-learning techniques that can produce non-linear decision boundaries without the need for feature engineering.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "basics.html#model-evaluation",
    "href": "basics.html#model-evaluation",
    "title": "2  Basic Concepts",
    "section": "2.3 Model Evaluation",
    "text": "2.3 Model Evaluation\nSuppose our machine learning model has learned the weights and biases that minimize the loss function. How do we know if the model is any good? In this section, we will discuss how to evaluate regression and classification models.\n\n2.3.1 Regression Models\nIn the case of regression models, we can use the mean squared error (MSE) as a measure of how well the model fits the data. The MSE is defined as\n\\[\\text{MSE} = \\frac{1}{N}\\sum_{n=1}^N (y_n - \\hat{y}_n)^2,\\]\nwhere \\(y_n\\) is the true value, \\(\\hat{y}_n\\) is the predicted value for observation \\(n\\) and \\(N\\) is the sample size. A low MSE indicates a good fit, while a high MSE indicates a poor fit. In the ideal case, the MSE is zero, meaning that the model perfectly fits the data. Related to the MSE is the root mean squared error (RMSE), which is the square root of the MSE\n\\[\\text{RMSE} = \\sqrt{\\text{MSE}}.\\]\nThe RMSE is in the same unit as the target variable \\(y\\) and is easier to interpret than the MSE.\nRegression models are sometimes also evaluated based on the coefficient of determination \\(R^2\\). The \\(R^2\\) is defined as\n\\[R^2 = 1 - \\frac{\\sum_{n=1}^N (y_n - \\hat{y}_n)^2}{\\sum_{n=1}^N (y_n - \\bar{y})^2},\\]\nwhere \\(\\bar{y}\\) is the mean of the true values \\(y_n\\). The \\(R^2\\) is a measure of how well the model fits the data compared to a simple model that predicts the mean of the true values for all observations. The \\(R^2\\) can take values between \\(-\\infty\\) and \\(1\\). A value of \\(1\\) indicates a perfect fit, while a value of \\(0\\) indicates that the model does not perform better than the simple model that predicts the mean of the true values for all observations. Note that the \\(R^2\\) is a normalized version of the MSE\n\\[R^2 = 1 - \\frac{N\\times\\text{MSE}}{\\sum_{n=1}^N (y_n - \\bar{y})^2}.\\]\nThus, we would rank models based on the \\(R^2\\) in the same way as we would rank them based on the MSE or the RMSE.\nThere are many more metrics but at this stage, we will only look at one more: the mean-absolute-error (MAE). The MAE is defined as\n\\[\\text{MAE} = \\frac{1}{N}\\sum_{n=1}^N |y_n - \\hat{y}_n|.\\]\nThe MAE is the average of the absolute differences between the true values and the predicted values. Note that the MAE does not penalize large errors as much as the MSE does.\n\n\n2.3.2 Classification Models\nIn the case of classification models, we need different metrics to evaluate the performance of the model. We will discuss some of the most common metrics in the following subsections.\n\nBasic Metrics\nA key measure to evaluate a classification model, both binary and multiclass classification, is to look at how often it predicts the correct class. This is called the accuracy of a model \\[\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}.\\]\nRelated to this, one could also compute the misclassification rate \\[\\text{Missclassification Rate} = \\frac{\\text{Number of incorrect predictions}}{\\text{Total number of predictions}}.\\]\nWhile these measures are probably the most intuitive measures to assess the performance of a classification model, they can be misleading in some cases. For example, if we have a dataset with 95% of the observations in class 1 and 5% in class 0, a model that always predicts \\(y=1\\) (class 1) would have an accuracy of 95%. However, this model would not be very useful.\n\n\nConfusion Matrices\nIn this and the following subsection, we focus on binary classification problems.\nLet \\(\\hat{y}\\) denote the predicted class and \\(y\\) the true class. In a binary classification problem, we can make two types of errors. First, we can make an error because we predicted \\(\\hat{y}=1\\) when \\(y=0\\), which is called a false positive (or a “false alarm”). Sometimes this is also called a type I error. Second, we can make an error because we \\(\\hat{y}=0\\) when \\(y=1\\), which is called a false negative (or a “missed detection”). Sometimes this is referred to as a type II error.\nWe can summarize the predictions of a classification model in a confusion matrix as seen in Figure 2.7. The confusion matrix is a \\(2\\times 2\\) matrix that shows the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) of a binary classification model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: Confusion matrix\n\n\n\nThere is a tradeoff between the two types of errors. For example, you could get fewer false negatives by predicting \\(\\hat{y}=1\\) more often, but this would increase the number of false positives. In the extreme case, if you only predict \\(\\hat{y}=1\\) for all observations, you would have no false negatives at all. However, you would also have no true negatives making the model of questionable usefulness.\n\n\n\n\n\n\nConfusion Matrix: Dependence on Classification Threshold \\(t\\)\n\n\n\nThe number of true positives, true negatives, false positives, and false negatives in the confusion matrix depends on the classification threshold \\(t\\).\n\n\nNote that we can compute the accuracy measure as a function of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN) \\[\\text{Accuracy} = \\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}},\\]\nwhile the missclassification rate is given by \\[\\text{Missclassification Rate} = \\frac{\\text{FP}+\\text{FN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}.\\]\nAnother useful measure that can be derived from the confusion matrix is the precision. It measures the fraction of positive predictions that were actually correct, i.e., \\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\\]\nThe true positive rate (TPR) or recall or sensitivity measures the fraction of actual positives that were correctly predicted, i.e. \\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}.\\]\nAnalogously, true negative rate (TNR) or specificity measures the fraction of actual negatives that were correctly predicted, i.e., \\[\\text{TNR} = \\frac{\\text{TN}}{\\text{FP}+\\text{TN}}\\]\nFinally, the false positive rate (FPR) measures the fraction of actual negatives that were incorrectly predicted to be positive, i.e., \\[\\text{FPR} = 1-\\text{TNR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}\\]\nNote that all of these measures can be computed for a given classification threshold \\(t\\). They capture different aspects of the quality of the predictions of a classification model.\n\n\n\n\n\n\nMulticlass Classification\n\n\n\nIn the case of multiclass classification, the confusion matrix is a \\(K\\times K\\) matrix, where \\(K\\) is the number of classes. The diagonal elements of the confusion matrix represent the number of correct predictions for each class, while the off-diagonal elements represent the number of incorrect predictions.\nNote that we can binarize multiclass classification problems, which allows us to use the same metrics as in binary classification. Two such binarization schemes are\n\nOne-vs-Rest (or One-vs-All): In this scheme, we train \\(K\\) binary classifiers, one for each class to distinguish it from all other classes. We can then use the class with the highest score as the predicted class for a new observation.\nOne-vs-One: In this scheme, we train \\(K(K-1)/2\\) binary classifiers, one for each pair of classes. We can then use a majority vote to determine the class of a new observation.\n\n\n\n\n\nReceiver Operating Characteristic (ROC) Curves and Area Under the Curve (AUC)\nFigure 2.8 shows a Receiver Operating Characteristic (ROC) curve which is a graphical representation of the tradeoff between the true positive rate (TPR) and the false positive rate (FPR) for different classification thresholds. The ROC curve is a useful tool to visualize the performance of a classification model. The diagonal line in the ROC curve represents a random classifier. A classifier that is better than random will have a ROC curve above the diagonal line. The closer the ROC curve is to the top-left corner, the better the classifier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.8: Receiver Operating Characteristic (ROC) Curve\n\n\n\nThe Area Under the Curve (AUC) of the ROC curve is a measure to compare different classification models. The AUC is a value between 0 and 1, where a value of 1 indicates a perfect classifier and a value of 0.5 indicates a random classifier. Figure 2.9 shows the AUC of a classifier as the shaded area under the ROC curve. Note that the AUC summarizes the ROC curve, which itself represents the quality of predictions of our classification model at different thresholds, in a single number.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: Area Under the Curve (AUC)",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "basics.html#generalization-and-overfitting",
    "href": "basics.html#generalization-and-overfitting",
    "title": "2  Basic Concepts",
    "section": "2.4 Generalization and Overfitting",
    "text": "2.4 Generalization and Overfitting\nTypically, we are not just interested in having a good fit for the dataset on which we are training a classification (or regression) model, after all, we already have the actual classes or realization of predicted variables in our dataset. What we are really interested in is that a classification or regression model generalizes to new data.\nHowever, since the models that we are using are highly flexible, it can be the case that we have a very high accuracy during the training of our model but it does not provide good predictions when used on new data. This situation is called overfitting: we have a very good fit in our training dataset, but predictions for new data inputs are bad.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.10: Examples of Overfitting and Underfitting\n\n\n\nFigure 2.10 provides examples of overfitting and underfitting. The blue dots represent the training data \\(x\\) and \\(y\\), the orange curve represents the fit of the model to the training data. The left plot shows an example of underfitting: the model is too simple to capture the underlying structure of the data. The middle plot shows a “good fit”: the model captures the underlying structure of the data. The right plot shows an example of overfitting: the model is too complex and captures the noise in the data.\n\n2.4.1 Bias-Variance Tradeoff\nThe concepts of bias and variance are useful to understand the tradeoff between underfitting and overfitting. Suppose that data is generated from the true model \\(Y=f(X)+\\epsilon\\), where \\(\\epsilon\\) is a random error term such that \\(\\mathbb{E}[\\epsilon]=0\\) and \\(\\text{Var}[\\epsilon]=\\sigma^2\\). Let \\(\\hat{f}(x)\\) be the prediction of the model at \\(x\\). One can show that the expected prediction error (or generalization error) of a model can be decomposed into three parts\n\\[\\text{EPE}(x_0) = \\mathbb{E}[(Y-\\hat{f}(x_0))^2|X=x_0] = \\text{Bias}^2(\\hat{f}(x_0)) + \\text{Var}(\\hat{f}(x_0)) + \\sigma^2,\\]\nwhere \\(\\text{Bias}(\\hat{f}(x_0)) = \\mathbb{E}[\\hat{f}(x_0)]-f(x_0)\\) is the bias at \\(x_0\\), \\(\\text{Var}(\\hat{f}(x_0)) = \\mathbb{E}[\\hat{f}(x_0)^2]-\\mathbb{E}[\\hat{f}(x_0)]^2\\) is the variance at \\(x_0\\), and \\(\\sigma^2\\) is the irreducible error, i.e., the error that cannot be reduced by any model. As model complexity increases, the bias tends to decrease, but the variance tends to increase. The following quote from Cornell lecture notes summarizes the bias-variance tradeoff well:\n\nVariance: Captures how much your classifier changes if you train on a different training set. How “over-specialized” is your classifier to a particular training set (overfitting)? If we have the best possible model for our training data, how far off are we from the average classifier?\nBias: What is the inherent error that you obtain from your classifier even with infinite training data? This is due to your classifier being “biased” to a particular kind of solution (e.g. linear classifier). In other words, bias is inherent to your model.\nNoise: How big is the data-intrinsic noise? This error measures ambiguity due to your data distribution and feature representation. You can never beat this, it is an aspect of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: Model Complexity and Generalization Error (adapted from Hastie, Tibshirani, and Friedman 2009)\n\n\n\nFigure 2.11 shows the relationship between the model complexity and the prediction error. A more complex model can reduce the prediction error only up to a certain point. After this point, the model starts to overfit the training data (it learns noise in the data), and the prediction error for the test data (i.e., data not used for model training) increases. Ideally, we would like to find the model complexity that minimizes the prediction error for the test data.\n\n\n2.4.2 Regularization\nOne approach to avoid overfitting is to use regularization. Regularization adds a penalty term to the loss function that penalizes large weights. The most common regularization techniques are L1 regularization and L2 regularization. L1 regularization adds the sum of the absolute values of the weights to the loss function, while L2 regularization adds the sum of the squared weights to the loss function.\nThese techniques are applicable across a large range of ML models and depending on the type of model additional regularization techniques might be available. For example, in neural networks, dropout regularization is a common regularization technique that randomly removes a set of artificial neurons during training.\nIn the context of linear regressions, L1 regularization is also called LASSO regression. The loss function of LASSO regression is given by\n\\[\\text{Loss} = \\text{MSE}(y,x; w) + \\lambda \\sum_{i=1}^m |w_i|,\\]\nwhere \\(\\text{MSE}(y,x; w)\\) refers to the mean squared error (the standard loss function of a linear regression), \\(\\lambda\\) is a hyperparameter that controls the strength of the regularization. Note that LASSO regression can also be used for feature selection, as it tends to set the weights of irrelevant features to zero. Figure 2.12 shows the LASSO regression loss for different levels of \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: LASSO Regression Loss for Different Levels of \\(\\lambda\\)\n\n\n\nAn L2 regularization in a linear regression context is called a Ridge regression. Its loss function is given by\n\\[\\text{Loss} = \\text{MSE}(y,x; w) + \\lambda \\sum_{i=1}^m w_i^2.\\]\nWe will have a closer look at regularization in the application sections. For now, it is important to understand that regularization works by constraining the weights of the model (i.e., keeping the weights small), which can help to avoid overfitting (which might require some weights to be very large). Figure 2.13 shows the Ridge regression loss for different levels of \\(\\lambda\\). Note how the Ridge regression loss is smoother than the LASSO regression loss and that the weights are never set to exactly zero but just get closer and closer to zero.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.13: Ridge Regression Loss for Different Levels of \\(\\lambda\\)\n\n\n\n\n\n2.4.3 Training, Validation, and Test Datasets\nRegularization discussed in the previous section is a method to directly prevent overfitting. However, another approach to the issues is to adjust our evaluation procedure in a way that allows us to detect overfitting. To do this, we can split the dataset into several parts. The first option shown in Figure 2.14 is to split the dataset into a training dataset and a test dataset. The training dataset is used to train the model, while the test dataset is used to evaluate the model. Why does this help to detect overfitting? If the model performs well on the training dataset but poorly on the test dataset, this is a sign of overfitting. If the model performs well on the test dataset, this is a sign that the model generalizes well to new data.\n\n\n\n\n\n\nDifference with Terminology in Econometrics/Statistics\n\n\n\nIn econometrics/statistics, it is more common to talk about in-sample and out-of-sample performance. The idea is the same: the in-sample performance is the performance of the model on the training dataset, while the out-of-sample performance is the performance of the model on the test dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Option A - Splitting the Whole Dataset into Training, and Test Datasets\n\n\n\nThe second option shown in Figure 2.15 is to split the dataset into a training dataset, a validation dataset, and a test dataset. The training dataset is used to train the model, the validation dataset is used to tune the hyperparameters of the model, and the test dataset is used to evaluate the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Option B - Splitting the Whole Dataset into Training, Test, and Validation Datasets\n\n\n\nCommon splits are 75% training and 30% test, or 80% training and 20% test in Option A. In Option B, a common split is 70% training, 15% validation, and 15% test.\n\n\n2.4.4 Cross-Validation\nAnother approach to detect overfitting is to use cross-validation. There are different types of cross-validation but k-fold cross-validation is probably the most common. In k-fold cross-validation, shown in Figure 2.16, the dataset is split into \\(k\\) parts (called folds). The model is trained on \\(k-1\\) folds and evaluated on the remaining fold. This process is repeated \\(k\\) times, each time using a different fold as the test fold. The performance of the model is then averaged over the \\(k\\) iterations. In practice, \\(k=10\\) is a common choice. If we set \\(k=N\\), where \\(N\\) is the number of observations in the dataset, we call this leave-one-out cross-validation or LOOCV.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.16: 5-Fold Cross-Validation\n\n\n\nThe advantage of cross-validation is that it allows us to use all the data for training and testing. The disadvantage is that it is computationally more expensive than a simple training-test split.\n\n\n\n\n\n\nMini-Exercise\n\n\n\nImplement a 5-fold cross-validation for the logistic regression model in the Python example below. Use the cross_val_score function from the sklearn.model_selection module.\n# Import the cross_val_score function\nfrom sklearn.model_selection import cross_val_score\n\n# Apply 5-fold cross-validation to the classifier clf\ncv_scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n\n# Mean of the cross-validation scores\ncv_scores.mean()",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "basics.html#python-implementation",
    "href": "basics.html#python-implementation",
    "title": "2  Basic Concepts",
    "section": "2.5 Python Implementation",
    "text": "2.5 Python Implementation\n\nLet’s have a look at how to implement a logistic regression model in Python. First, we need to import the required packages\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, recall_score, precision_score, roc_curve\npd.set_option('display.max_columns', 50) # Display up to 50 columns\n\nLet’s download the dataset automatically, unzip it, and place it in a folder called data if you haven’t done so already\n\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\nimport os.path\n\n# Check if the file exists\nif not os.path.isfile('data/card_transdata.csv'):\n\n    print('Downloading dataset...')\n\n    # Define the dataset to be downloaded\n    zipurl = 'https://www.kaggle.com/api/v1/datasets/download/dhanushnarayananr/credit-card-fraud'\n\n    # Download and unzip the dataset in the data folder\n    with urlopen(zipurl) as zipresp:\n        with ZipFile(BytesIO(zipresp.read())) as zfile:\n            zfile.extractall('data')\n\n    print('DONE!')\n\nelse:\n\n    print('Dataset already downloaded!')\n\nDownloading dataset...\nDONE!\n\n\nThen, we can load the data into a DataFrame using the read_csv function from the pandas library\n\ndf = pd.read_csv('data/card_transdata.csv')\n\nNote that it is common to call this variable df which is short for DataFrame.\nThis is a dataset of credit card transactions from Kaggle.com. The target variable \\(y\\) is fraud, which indicates whether the transaction is fraudulent or not. The other variables are the features \\(x\\) of the transactions.\n\n2.5.1 Data Exploration & Preprocessing\nThe first step whenever you load a new dataset is to familiarize yourself with it. You need to understand what the variables represent, what the target variable is, and what the data looks like. This is called data exploration. Depending on the dataset, you might need to preprocess it (e.g., check for missing values and duplicates, or create new variables) before you can use it to train a machine-learning model. This is called data preprocessing.\n\nBasic Dataframe Operations\nLet’s see how many rows and columns the dataset has\n\ndf.shape\n\n(1000000, 8)\n\n\nThe dataset has 1 million rows (observations) and 8 columns (variables)! Now, let’s have a look at the first few rows of the dataset with the head() method\n\ndf.head().T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\ndistance_from_home\n57.877857\n10.829943\n5.091079\n2.247564\n44.190936\n\n\ndistance_from_last_transaction\n0.311140\n0.175592\n0.805153\n5.600044\n0.566486\n\n\nratio_to_median_purchase_price\n1.945940\n1.294219\n0.427715\n0.362663\n2.222767\n\n\nrepeat_retailer\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nused_chip\n1.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nused_pin_number\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nonline_order\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nfraud\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\n\nIf you would like to see more entries in the dataset, you can use the head() method with an argument corresponding to the number of rows, e.g.,\n\ndf.head(20)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n5\n5.586408\n13.261073\n0.064768\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n6\n3.724019\n0.956838\n0.278465\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n7\n4.848247\n0.320735\n1.273050\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n8\n0.876632\n2.503609\n1.516999\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n9\n8.839047\n2.970512\n2.361683\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n10\n14.263530\n0.158758\n1.136102\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n11\n13.592368\n0.240540\n1.370330\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n12\n765.282559\n0.371562\n0.551245\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n13\n2.131956\n56.372401\n6.358667\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n14\n13.955972\n0.271522\n2.798901\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n15\n179.665148\n0.120920\n0.535640\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\n16\n114.519789\n0.707003\n0.516990\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n17\n3.589649\n6.247458\n1.846451\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n18\n11.085152\n34.661351\n2.530758\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n19\n6.194671\n1.142014\n0.307217\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nNote that analogously you can also use the tail() method to see the last few rows of the dataset.\nWe can also check what the variables in our dataset are called\n\ndf.columns\n\nIndex(['distance_from_home', 'distance_from_last_transaction',\n       'ratio_to_median_purchase_price', 'repeat_retailer', 'used_chip',\n       'used_pin_number', 'online_order', 'fraud'],\n      dtype='object')\n\n\nand the data types of the variables\n\ndf.dtypes\n\ndistance_from_home                float64\ndistance_from_last_transaction    float64\nratio_to_median_purchase_price    float64\nrepeat_retailer                   float64\nused_chip                         float64\nused_pin_number                   float64\nonline_order                      float64\nfraud                             float64\ndtype: object\n\n\nIn this case, all our variables are floating-point numbers (float). This means that they are numbers that have a fractional part such as 1.5, 3.14, etc. The number after float, 64 in this case refers to the number of bits that are used to represent this number in the computer’s memory. With 64 bits you can store more decimals than you could with, for example, 32, meaning that the results of computations can be more precise. But for the topics discussed in this course, this is not very important. Other common data types that you might encounter are integers (int) such as 1, 3, 5, etc., or strings (str) such as 'hello', 'world', etc.\nLet’s dig deeper into the dataset and see some summary statistics\n\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ndistance_from_home\n1000000.0\n26.628792\n65.390784\n0.004874\n3.878008\n9.967760\n25.743985\n10632.723672\n\n\ndistance_from_last_transaction\n1000000.0\n5.036519\n25.843093\n0.000118\n0.296671\n0.998650\n3.355748\n11851.104565\n\n\nratio_to_median_purchase_price\n1000000.0\n1.824182\n2.799589\n0.004399\n0.475673\n0.997717\n2.096370\n267.802942\n\n\nrepeat_retailer\n1000000.0\n0.881536\n0.323157\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nused_chip\n1000000.0\n0.350399\n0.477095\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nused_pin_number\n1000000.0\n0.100608\n0.300809\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nonline_order\n1000000.0\n0.650552\n0.476796\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nfraud\n1000000.0\n0.087403\n0.282425\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n\n\n\n\n\n\nWith the describe() method we can see the count, mean, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum values of each variable in the dataset.\n\n\nChecking for Missing Values and Duplicated Rows\nIt is also important to check for missing values and duplicated rows in the dataset. Missing values can be problematic for machine learning models, as they might not be able to handle them. Duplicated rows can also be problematic, as they might introduce bias in the model.\nWe can check for missing values (NA) that are encoded as None or numpy.NaN (Not a Number) with the isna() method. This method returns a boolean DataFrame (i.e., a DataFrame with True and False values) with the same shape as the original DataFrame, where True values indicate missing values.\n\ndf.isna()\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n999995\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n999996\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n999997\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n999998\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n999999\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n1000000 rows × 8 columns\n\n\n\n\nor to make it easier to see, we can sum the number of missing values for each variable\n\ndf.isna().sum()\n\ndistance_from_home                0\ndistance_from_last_transaction    0\nratio_to_median_purchase_price    0\nrepeat_retailer                   0\nused_chip                         0\nused_pin_number                   0\nonline_order                      0\nfraud                             0\ndtype: int64\n\n\nLuckily, there seem to be no missing values. However, you need to be careful! Sometimes missing values are encoded as empty strings '' or numpy.inf (infinity), which are not considered missing values by the isna() method. If you suspect that this might be the case, you need to make additional checks.\nAs an alternative, we could also look at the info() method, which provides a summary of the DataFrame, including the number of non-null values in each column. If there are missing values, the number of non-null values will be less than the number of rows in the dataset.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 8 columns):\n #   Column                          Non-Null Count    Dtype  \n---  ------                          --------------    -----  \n 0   distance_from_home              1000000 non-null  float64\n 1   distance_from_last_transaction  1000000 non-null  float64\n 2   ratio_to_median_purchase_price  1000000 non-null  float64\n 3   repeat_retailer                 1000000 non-null  float64\n 4   used_chip                       1000000 non-null  float64\n 5   used_pin_number                 1000000 non-null  float64\n 6   online_order                    1000000 non-null  float64\n 7   fraud                           1000000 non-null  float64\ndtypes: float64(8)\nmemory usage: 61.0 MB\n\n\nWe can also check for duplicated rows with the duplicated() method.\n\ndf.loc[df.duplicated()]\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n\n\n\n\n\n\nLuckily, there are also no duplicated rows.\n\n\nData Visualization\nLet’s continue with some data visualization. We can use the matplotlib library to create plots. We have already imported the library at the beginning of the notebook.\nLet’s start by plotting the distribution of the target variable fraud which can only take values zero and one. We can type\n\ndf['fraud'].value_counts()\n\nfraud\n0.0    912597\n1.0     87403\nName: count, dtype: int64\n\n\nto get the count of each value. We can also use the normalize=True argument to get the fraction of observations instead of the count\n\ndf['fraud'].value_counts(normalize=True)\n\nfraud\n0.0    0.912597\n1.0    0.087403\nName: proportion, dtype: float64\n\n\nWe can then plot it as follows\n\ndf['fraud'].value_counts(normalize=True).plot(kind='bar')\nplt.xlabel('Fraud')\nplt.ylabel('Fraction of Observations')\nplt.title('Distribution of Fraud')\nax = plt.gca()\nax.set_ylim([0.0, 1.0])\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively, we can plot it as a pie chart\n\ndf.value_counts(\"fraud\").plot.pie(autopct = \"%.1f\")\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nOur dataset seems to be quite imbalanced, as only 8.7% of the transactions are fraudulent. This is a common problem in fraud detection datasets, as fraudulent transactions are usually very rare. We will need to keep this in mind when evaluating our machine learning model: the accuracy measure will be very high even for bad models, as the model can just predict that all transactions are not fraudulent and still get an accuracy of 91.3%.\nLet’s look at some distributions. Most of the variables in the dataset are binary (0 or 1) variables. However, we also have some continuous variables. Let’s plot the distribution of the variable ratio_to_median_purchase_price, which is a continuous variable.\n\ndf['ratio_to_median_purchase_price'].hist(bins = 50, range=[0, 30])\nplt.xlabel('Ratio to Median Purchase Price')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\nWe can also plot the distribution of the variable ratio_to_median_purchase_price by the target variable fraud to see if there are any differences between fraudulent and non-fraudulent transactions\n\nfig, ax = plt.subplots(1,2)\ndf['ratio_to_median_purchase_price'].hist(bins = 50, range=[0, 30], by=df['fraud'], ax = ax)\nax[0].set_xlabel('Ratio to Median Purchase Price')\nax[1].set_xlabel('Ratio to Median Purchase Price')\nax[0].set_ylabel('Count')\nax[0].set_title('No Fraud')\nax[1].set_title('Fraud')\nplt.show()\n\n\n\n\n\n\n\n\nThere are indeed some differences between fraudulent and non-fraudulent transactions. For example, fraudulent transactions seem to have a higher ratio to the median purchase price, which is expected as fraudsters might try to make large transactions to maximize their profit.\nWe can also look at the correlation between the variables in the dataset. The correlation is a measure of how two variables move together\n\ndf.corr() # Pearson correlation (for linear relationships)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\ndistance_from_home\n1.000000\n0.000193\n-0.001374\n0.143124\n-0.000697\n-0.001622\n-0.001301\n0.187571\n\n\ndistance_from_last_transaction\n0.000193\n1.000000\n0.001013\n-0.000928\n0.002055\n-0.000899\n0.000141\n0.091917\n\n\nratio_to_median_purchase_price\n-0.001374\n0.001013\n1.000000\n0.001374\n0.000587\n0.000942\n-0.000330\n0.462305\n\n\nrepeat_retailer\n0.143124\n-0.000928\n0.001374\n1.000000\n-0.001345\n-0.000417\n-0.000532\n-0.001357\n\n\nused_chip\n-0.000697\n0.002055\n0.000587\n-0.001345\n1.000000\n-0.001393\n-0.000219\n-0.060975\n\n\nused_pin_number\n-0.001622\n-0.000899\n0.000942\n-0.000417\n-0.001393\n1.000000\n-0.000291\n-0.100293\n\n\nonline_order\n-0.001301\n0.000141\n-0.000330\n-0.000532\n-0.000219\n-0.000291\n1.000000\n0.191973\n\n\nfraud\n0.187571\n0.091917\n0.462305\n-0.001357\n-0.060975\n-0.100293\n0.191973\n1.000000\n\n\n\n\n\n\n\n\n\ndf.corr('spearman') # Spearman correlation (for monotonic relationships)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\ndistance_from_home\n1.000000\n-0.001068\n-0.000152\n0.559724\n-0.000118\n-0.000338\n-0.001812\n0.095032\n\n\ndistance_from_last_transaction\n-0.001068\n1.000000\n-0.000111\n-0.001352\n-0.000165\n0.000555\n-0.001076\n0.034661\n\n\nratio_to_median_purchase_price\n-0.000152\n-0.000111\n1.000000\n0.001202\n-0.000099\n0.000251\n-0.000376\n0.342838\n\n\nrepeat_retailer\n0.559724\n-0.001352\n0.001202\n1.000000\n-0.001345\n-0.000417\n-0.000532\n-0.001357\n\n\nused_chip\n-0.000118\n-0.000165\n-0.000099\n-0.001345\n1.000000\n-0.001393\n-0.000219\n-0.060975\n\n\nused_pin_number\n-0.000338\n0.000555\n0.000251\n-0.000417\n-0.001393\n1.000000\n-0.000291\n-0.100293\n\n\nonline_order\n-0.001812\n-0.001076\n-0.000376\n-0.000532\n-0.000219\n-0.000291\n1.000000\n0.191973\n\n\nfraud\n0.095032\n0.034661\n0.342838\n-0.001357\n-0.060975\n-0.100293\n0.191973\n1.000000\n\n\n\n\n\n\n\n\nThis is still a bit hard to read. We can visualize the correlation matrix with a heatmap using the Seaborn library, which we have already imported at the beginning of the notebook.\n\ncorr = df.corr('spearman')\ncmap = sns.diverging_palette(10, 255, as_cmap=True) # Create a color map\nmask = np.triu(np.ones_like(corr, dtype=bool)) # Create a mask to only show the lower triangle of the matrix\nsns.heatmap(corr, cmap=cmap, vmax=1, center=0, mask=mask) # Create a heatmap of the correlation matrix (Note: vmax=1 makes sure that the color map goes up to 1 and center=0 are used to center the color map at 0)\nplt.show()\n\n\n\n\n\n\n\n\nNote how ratio_to_median_purchase_price is positively correlated with fraud, which is expected as we saw in the previous plot that fraudulent transactions have a higher ratio to the median purchase price. Furthermore, used_chip and used_pin_number are negatively correlated with fraud, which makes sense as transactions, where the chip or the pin is used, are supposed to be more secure.\nWe can also plot boxplots to visualize the distribution of the variables\n\nselector = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price'] # Select the variables we want to plot\nplt.figure()\nax = sns.boxplot(data = df[selector], orient = 'h') \nax.set(xscale = \"log\") # Set the x-axis to a logarithmic scale to better visualize the data\nplt.show()\n\n\n\n\n\n\n\n\nBoxplots are a good way to visualize the distribution of a variable, as they show the median, the interquartile range, and the outliers. Each of the distributions shown in the boxplots above has a long right tail, which explains the large number of outliers. However, you have to be careful: you cannot just remove these outliers since these are likely to be fraudulent transactions.\nLet’s see how many fraudulent transactions we would remove if we blindly remove the outliers according to the interquartile range\n\n# Compute the interquartile range\nQ1 = df['ratio_to_median_purchase_price'].quantile(0.25)\nQ3 = df['ratio_to_median_purchase_price'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Identify outliers based on the interquartile range\nthreshold = 1.5\noutliers = df[(df['ratio_to_median_purchase_price'] &lt; Q1 - threshold * IQR) | (df['ratio_to_median_purchase_price'] &gt; Q3 + threshold * IQR)]\n\n# Count the number of fraudulent transactions amoung our selected outliers\noutliers['fraud'].value_counts()\n\nfraud\n1.0    53092\n0.0    31294\nName: count, dtype: int64\n\n\n\ndf['fraud'].value_counts()\n\nfraud\n0.0    912597\n1.0     87403\nName: count, dtype: int64\n\n\n53092 of 87403 (more than half!) of our fraudulent transactions would be removed if we would have blindly removed the outliers according to the interquartile range. This is a significant number of observations, which would likely hurt the performance of our machine-learning model. Therefore, we should not remove these outliers. It would make the imbalance of our dataset even worse.\n\n\nSplitting the Data into Training and Test Sets\nBefore we can train a machine learning model, we need to split our dataset into a training set and a test set.\n\nX = df.drop('fraud', axis=1) # All variables except `fraud`\ny = df['fraud'] # Only our fraud variables\n\nThe training set is used to train the model, while the test set is used to evaluate the model. We will use the train_test_split function from the sklearn.model_selection module to split our dataset. We will use 70% of the data for training and 30% for testing. We will also set the stratify argument to y to make sure that the distribution of the target variable is the same in the training and test sets. Otherwise, we might randomly not have any fraudulent transactions in the test set, which would make it impossible to correctly evaluate our model.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3, random_state = 42)\n\n\n\nScaling Features\nTo improve the performance of our machine learning model, we should scale the features. This is especially important for models that are sensitive to the scale of the features, such as logistic regression. We will use the StandardScaler class from the sklearn.preprocessing module to scale the features. The StandardScaler class scales the features so that they have a mean of 0 and a standard deviation of 1. Since we don’t want to scale features that are binary (0 or 1), we will define a small function that scales only the features that we want\n\ndef scale_features(scaler, df, col_names, only_transform=False):\n\n    # Extract the features we want to scale\n    features = df[col_names] \n\n    # Fit the scaler to the features and transform them\n    if only_transform:\n        features = scaler.transform(features.values)\n    else:\n        features = scaler.fit_transform(features.values)\n\n    # Replace the original features with the scaled features\n    df[col_names] = features\n\nThen, we need to run the function\n\ncol_names = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price'] \nscaler = StandardScaler() \nscale_features(scaler, X_train, col_names)\nscale_features(scaler, X_test, col_names, only_transform=True)\n\nNote that we only fit the scaler to the training set and then transform both the training and test set. This ensures that the same values for the features produce the same output in the training and test set. Otherwise, if we fit the scaler to the test data as well, the meaning of certain values in the test set might change, which would make it impossible to evaluate the model correctly.\n\n\n\n\n\n\nMini-Exercise\n\n\n\nTry switching to MinMaxScaler instead of StandardScaler and see how it affects the performance of the model. MinMaxScaler scales the features so that they are between 0 and 1.\n\n\n\n\n\n2.5.2 Implementing Logistic Regression\nNow that we have explored and preprocessed our dataset, we can move on to the next step: training a machine learning model. We will use a logistic regression model to predict whether a transaction is fraudulent or not.\nUsing the LogisticRegression class from the sklearn.linear_model module, fitting the model to the data is straightforward using the fit method\n\nclf = LogisticRegression().fit(X_train, y_train)\n\nWe can then use the predict method to predict the class of the test set\n\nclf.predict(X_test.head(5))\n\narray([0., 0., 0., 0., 1.])\n\n\nThe actual classes of the first five observations in the test dataset are\n\ny_test.head(5)\n\n217309    0.0\n902387    0.0\n175152    0.0\n527113    0.0\n973041    1.0\nName: fraud, dtype: float64\n\n\nThis seems to match quite well. Let’s have a look at different performance metrics\n\ny_pred = clf.predict(X_test)\ny_proba = clf.predict_proba(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")\nprint(f\"ROC AUC: {roc_auc_score(y_test, y_proba[:, 1])}\")\n\nAccuracy: 0.95908\nPrecision: 0.8954682094038908\nRecall: 0.6021128103428549\nROC AUC: 0.9671832218100465\n\n\nAs expected, the accuracy is quite high since we do not have many fraudulent transactions. Recall that the precision (\\(\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\\)) is the fraction of correctly predicted fraudulent transactions among all transactions transactions predicted to be fraudulent. The recall (\\(\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\\)) is the fraction of correctly predicted fraudulent transactions among the actual fraudulent transactions. The ROC AUC is the area under the curve for the receiver operating characteristic (ROC) curve\n\n# Compute the ROC curve\ny_proba = clf.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_proba[:,1])\n\n# Plot the ROC curve\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], linestyle='--', color='grey')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('ROC Curve')\nplt.show()\n\n\n\n\n\n\n\n\nThe confusion matrix for the test set can be computed as follows\n\nconf_mat = confusion_matrix(y_test, y_pred, labels=[1, 0]).transpose() # Transpose the sklearn confusion matrix to match the convention in the lecture\nconf_mat\n\narray([[ 15788,   1843],\n       [ 10433, 271936]])\n\n\nWe can also plot the confusion matrix as a heatmap\n\nsns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=['Fraud', 'No Fraud'], yticklabels=['Fraud', 'No Fraud'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, we have mostly true negatives and true positives. However, there is still a significant number of false negatives, which means that we are missing fraudulent transactions, and a significant number of false positives, which means that we are predicting transactions as fraudulent that are not fraudulent.\nIf we would like to use a threshold other than 0.5 to predict the class of the test set, we can do so as follows\n\n# Alternative threshold\nthreshold = 0.1\n\n# Predict the class of the test set\ny_pred_alt = (y_proba[:, 1] &gt;= threshold).astype(int)\n\n# Show the performance metrics\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_alt)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_alt)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_alt)}\")\n\nAccuracy: 0.9112033333333334\nPrecision: 0.49579121188932296\nRecall: 0.9389420693337401\n\n\nSetting a lower threshold increases the recall but decreases the precision. This is because we are more likely to predict a transaction as fraudulent, which increases the number of true positives but also the number of false positives.\nWhat the correct threshold is depends on the problem at hand. For example, if the cost of missing a fraudulent transaction is very high, you might want to set a lower threshold to increase the recall. If the cost of falsely predicting a transaction as fraudulent is very high, you might want to set a higher threshold to increase the precision.\nWe can also plot the performance metrics for different thresholds\n\nN = 50\nthresholds_array = np.linspace(0.0, 0.999, N)\naccuracy_array = np.zeros(N)\nprecision_array = np.zeros(N)\nrecall_array = np.zeros(N)\n\n# Compute the performance metrics for different thresholds\nfor ii, thresh in enumerate(thresholds_array):\n    y_pred_alt_tmp = (y_proba[:, 1] &gt; thresh).astype(int)\n    accuracy_array[ii] = accuracy_score(y_test, y_pred_alt_tmp)\n    precision_array[ii] = precision_score(y_test, y_pred_alt_tmp)\n    recall_array[ii] = recall_score(y_test, y_pred_alt_tmp)\n\n# Plot the performance metrics\nplt.plot(thresholds_array, accuracy_array, label='Accuracy')\nplt.plot(thresholds_array, precision_array, label='Precision')\nplt.plot(thresholds_array, recall_array, label='Recall')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.5.3 Conclusions\nIn this notebook, we have seen how to implement a logistic regression model in Python. We have loaded a dataset, explored and preprocessed it, and trained a logistic regression model to predict whether a transaction is fraudulent or not. We have evaluated the model using different performance metrics and have seen how the choice of threshold affects the performance of the model.\nThere are many ways to improve the performance of the model. For example, we could try different machine learning models, or engineer new features. We could also try to deal with the imbalanced dataset by using techniques such as oversampling or undersampling. However, this is beyond the scope of this notebook.\n\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning - Data Mining, Inference, and Prediction. Second Edition. Springer.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Concepts</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html",
    "href": "decisiontrees.html",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1 What is a Decision Tree?\nDecision trees, also called Classification and Regression Trees (CART) are a popular supervised learning method. As the name CART suggests, they are used for both classification and regression problems. They are simple to understand and interpret, and the process of building a decision tree is intuitive. Decision trees are also the foundation of more advanced ensemble methods like Random Forests and Boosting.\nFigure 3.1 shows an example of a decision tree for a classification problem, i.e., a classification tree. In this case, the decision tree is used to classify animals into four categories: dogs, snakes, fish, and birds. The tree asks a series of questions about the features of the animal (e.g., number of legs, feathers, and habitat) and uses the answers to classify the animal. This means that the tree partitions the feature space into different regions that are associated with a particular class label.\nFigure 3.2 shows an example of a decision tree for a regression problem, i.e., a regression tree. In this case, the decision tree is used to predict some continuous variable \\(y\\) (e.g., a house price) based on features \\(x_1\\) and \\(x_2\\) (e.g., number of rooms and size of the property). As Figure 3.7 shows, the regression tree partitions the \\((x_1,x_2)\\)-space into different regions that are associated with a predicted value \\(y\\). Mathematically, the prediction of a regression tree can be expressed as\n\\[\\hat{y} = \\sum_{m=1}^{M} c_m \\mathbb{1}(x \\in R_m)\\]\nwhere \\(R_m\\) are the regions of the feature space, \\(c_m\\) are the predicted (i.e., average) values in the regions, \\(\\mathbb{1}(x \\in R_m)\\) is an indicator function that is 1 if \\(x\\) is in region \\(R_m\\) and 0 otherwise, and \\(M\\) is the number of regions.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#what-is-a-decision-tree",
    "href": "decisiontrees.html#what-is-a-decision-tree",
    "title": "3  Decision Trees",
    "section": "",
    "text": "Figure 3.1: Classification Tree - Classification of Dogs, Snakes, Fish, and Birds based on their Features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Regression Tree - Prediction of \\(y\\) based on \\(x_1\\) and \\(x_2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegions\n\n\n\n\n\n\n\nPredictions\n\n\n\n\n\n\n\nFigure 3.3: Regression Tree - Regions and Predictions of Decision Tree in Figure 3.2\n\n\n\n\n\n\n\n\n\nMini-Exercise\n\n\n\nGiven the decision tree in Figure 3.2, what would be the predicted value of \\(y\\) for the following data points?\n\n\\((x_1, x_2) = (1, 1)\\)\n\\((x_1, x_2) = (2, 2)\\)\n\\((x_1, x_2) = (2, 8)\\)\n\\((x_1, x_2) = (10, 4)\\)\n\\((x_1, x_2) = (7, 8)\\)",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#terminology",
    "href": "decisiontrees.html#terminology",
    "title": "3  Decision Trees",
    "section": "3.2 Terminology",
    "text": "3.2 Terminology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: Decision Tree - Terminology\n\n\n\nFigure 3.4 shows some of the terminology that you might encounter in decision trees. The root node is the first node in the tree. The root node is split into decision nodes (or leaf nodes) based on the values of the features. The decision nodes are further split into decision nodes or leaf nodes. The leaf nodes represent the final prediction of the model. A subtree or branch is a part of the tree that starts at a decision node and ends at a leaf node. The depth of a tree is the length of the longest path from the root node to a leaf node.\nFurthermore, one can also differentiate between child and parent nodes. A child node is a node that results from a split (e.g., the first (reading from the top) decision node and leaf node in Figure 3.4 are child nodes of the root node). The parent node is the node that is split to create the child nodes (e.g., the root node in Figure 3.4 is the parent node of the first decision node and leaf node).",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#how-to-grow-a-tree",
    "href": "decisiontrees.html#how-to-grow-a-tree",
    "title": "3  Decision Trees",
    "section": "3.3 How To Grow a Tree",
    "text": "3.3 How To Grow a Tree\nA key question is how to determine the order of variables and thresholds that are used in all the splits of a decision tree. There are different algorithms to grow a decision tree, but the most common one is the CART algorithm. The CART algorithm is a greedy algorithm that grows the tree in a top-down manner. The reason for this algorithm choice is that it is computationally infeasible to consider all possible (fully grown) trees to find the best-performing one. So, the CART algorithm grows the tree in a step-by-step manner choosing the splits in a greedy manner (i.e., choosing the one that performs best at that step). This means that the algorithm does not consider the future consequences of the current split and may not find the optimal tree.\nThe basic idea is to find a split that minimizes some loss function \\(Q^s\\) and to repeat this recursively for all resulting child nodes. Suppose we start from zero, meaning that we first need to determine the root node. We compute the loss function \\(Q^s\\) for all possible splits \\(s\\) that we can make. This means we need to consider all variables in our dataset (and all split thresholds) and choose the one that minimizes the loss \\(Q^s\\). We then repeat this process for each of the child nodes, and so on, until we reach a stopping criterion. Figure 3.5 shows an example of a candidate split.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Example of Decision Tree Split\n\n\n\nLet \\(\\tau\\) denote the index of a leaf node with each leaf node \\(\\tau\\) corresponding to a region \\(R_{\\tau}\\) with \\(N_{\\tau}\\) data points. In the case of a classification problem, the loss function is typically either the Gini impurity\n\\[Q^s_{\\tau} = \\sum_{k=1}^{K} p_{\\tau k} (1 - p_{\\tau k}) = 1- \\sum_{k=1}^K p_{\\tau k}^2\\]\nor the cross-entropy\n\\[Q^s_{\\tau} = -\\sum_{k=1}^{K} p_{\\tau k} \\log(p_{\\tau k})\\]\nwhere \\(p_{\\tau k}\\) is the proportion of observations in region \\(R_{\\tau}\\) that belong to class \\(k\\) and \\(K\\) is the number of classes. Note that both measures become zero when all observations in the region belong to the same class (i.e., \\(p_{\\tau k} = 1\\) or \\(p_{\\tau k} = 0\\)). This is the ideal case for a classification problem: we say that the node is pure.\nIn the case of a regression problem, the loss function is typically the mean squared error (MSE)\n\\[Q^s_{\\tau} = \\frac{1}{N_{\\tau}} \\sum_{i \\in R_{\\tau}} (y_i - \\hat{y}_{\\tau})^2\\]\nwhere \\(\\hat{y}_{\\tau}\\) is the predicted value of the target variable \\(y\\) in region \\(R_{\\tau}\\)\n\\[\\hat{y}_{\\tau} = \\frac{1}{N_{\\tau}} \\sum_{i \\in R_{\\tau}} y_i,\\]\ni.e., the average of the target variable in region \\(R_{\\tau}\\).\nThe total loss of a split \\(Q^s\\) is then the weighted sum of the loss functions of the child nodes\n\\[Q^s = \\frac{N_1}{N_1+N_2}Q^s_{1} + \\frac{N_2}{N_1+N_2}Q^s_{2}\\]\nwhere \\(N_1\\) and \\(N_2\\) are the number of data points in the child nodes.\nOnce we have done this for the root node, we repeat the process for each child node. Then, we repeat it for the child nodes of the child nodes, and so on, until we reach a stopping criterion. The stopping criterion can be, for example, a maximum depth of the tree, a minimum number of data points in a leaf node, or a minimum reduction in the loss function.\n\n3.3.1 Example: Classification Problem\nSuppose you have the data in Table 3.1. The goal is to predict whether a bank will default based on two features: whether the bank is systemically important and its Common Equity Tier 1 (CET1) ratio (i.e., the ratio of CET1 capital to risk-weighted assets). The CET1 ratio is a measure of a bank’s financial strength.\n\n\n\n\nTable 3.1: (Made-up) Data for Classification Problem (Bank Default Prediction)\n\n\n\n\n\n\nDefault\nSystemically Important Bank\nCET1 Ratio (in %)\n\n\n\n\nYes\nNo\n8.6\n\n\nNo\nNo\n9\n\n\nYes\nYes\n10.6\n\n\nYes\nYes\n10.8\n\n\nNo\nNo\n11.2\n\n\nNo\nNo\n11.5\n\n\nNo\nYes\n12.4\n\n\n\n\n\n\n\n\nGiven that you only have two features, CET1 Ratio and whether it is a systemically important bank, you only have two possible variables for the root node. However, since CET1 is a continuous variable, there are potentially many thresholds that you could use to split the data. To find this threshold, we need to calculate the Gini impurity of each possible split and choose the one that minimizes the impurity.\n\n\n\n\nTable 3.2: Gini Impurities for Different CET1 Thresholds\n\n\n\n\n\n\nCET1 Ratio Threshold\nQ₁\nQ₂\nQ\n\n\n\n\n8.8\n0\n0.44\n0.38\n\n\n9.8\n0.5\n0.48\n0.49\n\n\n10.7\n0.44\n0.38\n0.4\n\n\n11\n0.38\n0\n0.21\n\n\n11.35\n0.48\n0\n0.34\n\n\n11.95\n0.5\n0\n0.43\n\n\n\n\n\n\n\n\nAccording to Table 3.2, the best split is at a CET1 ratio of 7.0%. The Gini impurity for \\(\\text{CET1}\\leq 11\\%\\) is 0.38, the Gini impurity of \\(\\text{CET1}&gt;11\\%\\) is 0, and the total impurity is 0.21. However, we could also split based on whether a bank would be systemically important. In this case, the Gini impurity of the split is 0.40. This means that the best split is based on the CET1 ratio. We split the data into two regions: one with a CET1 ratio of 11.0% or less and one with a CET1 ratio of more than 11.0%.\nNote that the child node for a CET1 ratio of more than \\(11.0\\%\\) is already pure, i.e., all banks in this region are not defaulting. However, the child node for a CET1 ratio of \\(11.0\\%\\) or less is not pure meaning that we can do additional splits as shown in Figure 3.6. In particular, both, the split at a CET1 ratio of 8.8% and the split based on whether a bank is systemically important yield a Gini impurity of 0.25. We choose the split based on whether a bank is systemically important as the next split, which means we can do the final split based on the CET1 ratio.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6: Classification Tree for Table 3.1\n\n\n\n\n\n3.3.2 Stopping Criteria and Pruning a Tree\nA potential problem with decision trees is that they can overfit the training data. In principle, we can get the error down to zero if we just make enough splits. This means that the tree can become too complex and capture noise in the data rather than the underlying relationship. To prevent this, we usually set some early stopping criteria like\n\nA maximum depth of the tree,\nA minimum number of data points in a leaf node,\nA minimum number of data points required in a decision node for a split,\nA minimum reduction in the loss function, or\nA maximum number of leaf nodes,\n\nwhich will prevent the tree from growing too large and all the nodes from becoming pure. We can also use a combination of these criteria. In the Python applications, we will see how to set some of these stopping criteria.\nFigure 3.7 shows an example of how stopping criteria affect the fit of a decision tree. Note that without any stopping criteria, the tree fits the data perfectly but is likely to overfit. By setting a maximum depth or a minimum number of data points in a leaf node, we can prevent the tree from overfitting the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.7: Regression Tree - Effect of Stopping Criteria\n\n\n\nAnother way to prevent overfitting is to prune the tree, i.e., to remove nodes from the tree according to certain rules. This is done after (not during) growing the tree. One common approach is to use cost-complexity pruning. The idea is related to regularization that we have seen before, i.e., we add a term to the loss functions above that penalizes tree complexity. The pruning process is controlled by a hyperparameter \\(\\lambda\\) that determines the trade-off between the complexity of the tree and its fit to the training data.\n\n\n\n\n\n\nMini-Exercise\n\n\n\nHow would the decision tree in Figure 3.6 look like if\n\nwe required a minimum of 2 data points in a leaf node?\nwe required a maximum depth of 2?\nwe required a maximum depth of 2 and a minimum of 3 data points in a leaf node?\nwe required a minimum of 3 data points for a split?\nwe required a minimum of 5 data points for a split?",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#advantages-and-disadvantages",
    "href": "decisiontrees.html#advantages-and-disadvantages",
    "title": "3  Decision Trees",
    "section": "3.4 Advantages and Disadvantages",
    "text": "3.4 Advantages and Disadvantages\nAs noted by Murphy (2022), decision trees are popular because of some of the advantages they offer\n\nEasy to interpret\nCan handle mixed discrete and continuous inputs\nInsensitive to monotone transformations of the inputs\nAutomatic variable selection\nRelatively robust to outliers\nFast to fit and scale well to large data sets\nCan handle missing input features1\n\nTheir disadvantages include\n\nNot very accurate at prediction compared to other kinds of models (note, for example, the piece-wise constant nature of the predictions in regression problems)\nThey are unstable: small changes to the input data can have large effects on the structure of the tree (small changes at the top can affect the rest of the tree)",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#random-forests",
    "href": "decisiontrees.html#random-forests",
    "title": "3  Decision Trees",
    "section": "3.5 Random Forests",
    "text": "3.5 Random Forests\nDecision trees can be unstable meaning that small changes in the training data can lead to large changes in the tree structure. One way to address this issue is to use Random Forests. Random Forests is an ensemble method: The idea is to build a large number of trees (also called weak learners in this context), each of which is trained on a random subset of the data. The predictions of the trees are then averaged in regression tasks or determined through majority voting in the case of classification tasks to make the final prediction. Training multiple trees on random subsets of the data is also called bagging (short for bootstrap aggregating). Random Forests adds an additional layer of randomness by selecting a random subset of features for each tree. This means that each tree is trained on a different subset of the data and a different subset of features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.8: Random Forests - Ensemble of Decision Trees with Majority Decision “It’s a dog!”\n\n\n\nThe basic steps of the Random Forest algorithm are as follows:\n\nBootstrapping: Randomly draw \\(N\\) samples with replacement from the training data.\nGrow a tree: For each node of the tree, randomly select \\(m\\) features from the \\(p\\) features in the bootstrap dataset and find the best split based on these \\(m\\) features.\nRepeat: Repeat steps 1 and 2 \\(B\\) times to grow \\(B\\) trees.\nPrediction: To get the prediction for a new data point, average the predictions of all trees in the case of regression or use a majority vote in the case of classification.\n\nNote that because we draw samples with replacement, some samples will not be included in the bootstrap sample. These samples are called out-of-bag (OOB) samples. The OOB samples can be used to estimate the performance of the model without the need for cross-validation since it is “performed along the way” (Hastie, Tibshirani, and Friedman (2009)). The OOB error is almost identical to the error obtained through N-fold cross-validation.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#boosting",
    "href": "decisiontrees.html#boosting",
    "title": "3  Decision Trees",
    "section": "3.6 Boosting",
    "text": "3.6 Boosting\nAnother popular ensemble method is Boosting. The idea behind boosting is to train a sequence of weak learners (e.g., decision trees), each of which tries to correct the mistakes of the previous one. The predictions of the weak learners are then combined to make the final prediction. Note how this differs from Random Forests where the trees are trained independently of each other in parallel, while here we sequentially train the trees to fix the mistakes of the previous ones. The basic steps can be roughly summarized as follows:\n\nInitialize the model: Construct a base tree with just a root node. In the case of a regression problem, the prediction could be the mean of the target variable. In the case of a classification problem, the prediction could be the log odds of the target variable.\nTrain a weak learner: Train a weak learner on the data. The weak learner tries to correct the mistakes of the previous model.\nUpdate the model: Update the model by adding the weak learner to the model. The added weak learner is weighted by a learning rate \\(\\eta\\).\nRepeat: Repeat steps 2 and 3 until we have grown \\(B\\) trees.\n\nXGBoost (eXtreme Gradient Boosting) is a popular implementation of the (gradient) boosting algorithm. It is known for its performance and is widely used in machine learning competitions. The algorithm is based on the idea of gradient boosting, which is a generalization of boosting. We will see how to implement XGBoost in Python but will not go into the details of the algorithm here. Other popular implementations of the boosting algorithm are AdaBoost and LightGBM.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#interpreting-ensemble-methods",
    "href": "decisiontrees.html#interpreting-ensemble-methods",
    "title": "3  Decision Trees",
    "section": "3.7 Interpreting Ensemble Methods",
    "text": "3.7 Interpreting Ensemble Methods\nA downside of using ensemble methods is that you lose the interpretability of a single decision tree. However, there are ways to interpret ensemble methods. One way is to look at the feature importance. Feature importance tells you how much each feature contributes to the reduction in the loss function. The idea is that features that are used in splits that lead to a large reduction in the loss function are more important. Murphy (2022) shows that the feature importance of feature \\(k\\) is\n\\[R_k(b)=\\sum_{j=1}^{J-1} G_j \\mathbb{I}(v_j=k)\\]\nwhere the sum is over all non-leaf (internal) nodes, \\(G_j\\) is the loss reduction (gain) at node \\(j\\), and \\(v_j = k\\) if node \\(j\\) uses feature \\(k\\). Simply put, we sum up all gains of the splits that use feature \\(k\\). Then, we average over all trees in our ensemble to get the feature importance of feature \\(k\\)\n\\[R_k = \\frac{1}{B}\\sum_{b=1}^{B} R_k(b).\\]\nNote that the resulting \\(R_k\\) are sometimes normalized such that the maximum value is 100. This means that the most important feature has a feature importance of 100 and all other features are scaled accordingly. Note that feature importance can in principle also be computed for a single decision tree.\n\n\n\n\n\n\nWarning\n\n\n\nNote that feature importance tends to favor continuous variables and variables with many categories (for an example see here). As an alternative, one can use permutation importance which is a model-agnostic way to compute the importance of different features. The idea is to shuffle the values of a feature in the test data set and see how much the model performance decreases. The more the performance decreases, the more important the feature is.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#python-implementation",
    "href": "decisiontrees.html#python-implementation",
    "title": "3  Decision Trees",
    "section": "3.8 Python Implementation",
    "text": "3.8 Python Implementation\n\nLet’s have a look at how to implement a decision tree in Python. Again, we need to first import the required packages and load the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, recall_score, precision_score, roc_curve\nfrom sklearn.inspection import permutation_importance\npd.set_option('display.max_columns', 50) # Display up to 50 columns\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\nimport os.path\n\n# Check if the file exists\nif not os.path.isfile('data/card_transdata.csv'):\n\n    print('Downloading dataset...')\n\n    # Define the dataset to be downloaded\n    zipurl = 'https://www.kaggle.com/api/v1/datasets/download/dhanushnarayananr/credit-card-fraud'\n\n    # Download and unzip the dataset in the data folder\n    with urlopen(zipurl) as zipresp:\n        with ZipFile(BytesIO(zipresp.read())) as zfile:\n            zfile.extractall('data')\n\n    print('DONE!')\n\nelse:\n\n    print('Dataset already downloaded!')\n\n# Load the data\ndf = pd.read_csv('data/card_transdata.csv')\n\nDataset already downloaded!\n\n\nThis is the dataset of credit card transactions from Kaggle.com which we have used before. Recall that the target variable \\(y\\) is fraud, which indicates whether the transaction is fraudulent or not. The other variables are the features \\(x\\) of the transactions.\n\ndf.head(20)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n5\n5.586408\n13.261073\n0.064768\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n6\n3.724019\n0.956838\n0.278465\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n7\n4.848247\n0.320735\n1.273050\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n8\n0.876632\n2.503609\n1.516999\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n9\n8.839047\n2.970512\n2.361683\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n10\n14.263530\n0.158758\n1.136102\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n11\n13.592368\n0.240540\n1.370330\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n12\n765.282559\n0.371562\n0.551245\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n13\n2.131956\n56.372401\n6.358667\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n14\n13.955972\n0.271522\n2.798901\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n15\n179.665148\n0.120920\n0.535640\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\n16\n114.519789\n0.707003\n0.516990\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n17\n3.589649\n6.247458\n1.846451\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n18\n11.085152\n34.661351\n2.530758\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n19\n6.194671\n1.142014\n0.307217\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\ncount\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n\n\nmean\n26.628792\n5.036519\n1.824182\n0.881536\n0.350399\n0.100608\n0.650552\n0.087403\n\n\nstd\n65.390784\n25.843093\n2.799589\n0.323157\n0.477095\n0.300809\n0.476796\n0.282425\n\n\nmin\n0.004874\n0.000118\n0.004399\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.878008\n0.296671\n0.475673\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n9.967760\n0.998650\n0.997717\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n25.743985\n3.355748\n2.096370\n1.000000\n1.000000\n0.000000\n1.000000\n0.000000\n\n\nmax\n10632.723672\n11851.104565\n267.802942\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 8 columns):\n #   Column                          Non-Null Count    Dtype  \n---  ------                          --------------    -----  \n 0   distance_from_home              1000000 non-null  float64\n 1   distance_from_last_transaction  1000000 non-null  float64\n 2   ratio_to_median_purchase_price  1000000 non-null  float64\n 3   repeat_retailer                 1000000 non-null  float64\n 4   used_chip                       1000000 non-null  float64\n 5   used_pin_number                 1000000 non-null  float64\n 6   online_order                    1000000 non-null  float64\n 7   fraud                           1000000 non-null  float64\ndtypes: float64(8)\nmemory usage: 61.0 MB\n\n\n\n3.8.1 Data Preprocessing\nSince we have already explored the dataset in the previous notebook, we can skip that part and move directly to the data preprocessing.\nWe will again split the data into training and test sets using the train_test_split function\n\nX = df.drop('fraud', axis=1) # All variables except `fraud`\ny = df['fraud'] # Only our fraud variables\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3, random_state = 42)\n\nThen we can do the feature scaling to ensure our non-binary variables have mean zero and variance 1\n\ndef scale_features(scaler, df, col_names, only_transform=False):\n\n    # Extract the features we want to scale\n    features = df[col_names] \n\n    # Fit the scaler to the features and transform them\n    if only_transform:\n        features = scaler.transform(features.values)\n    else:\n        features = scaler.fit_transform(features.values)\n\n    # Replace the original features with the scaled features\n    df[col_names] = features\n\ncol_names = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price'] \nscaler = StandardScaler() \nscale_features(scaler, X_train, col_names)\nscale_features(scaler, X_test, col_names, only_transform=True)\n\n\n\n3.8.2 Implementing a Decision Tree Classifier\nWe can now implement a decision tree model using the DecisionTreeClassifier class from the sklearn.tree module. Fitting the model to the data is almost the same as when we used logistic regression\n\nclf_dt = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n\nWe can visualize the tree using the plot_tree function from the sklearn.tree module\n\nplot_tree(clf_dt, filled=True, feature_names = X_train.columns.to_list())\nplt.show()\n\n\n\n\n\n\n\n\nThe tree is quite large and it’s difficult to see details. Let’s only look at the first level of the tree\n\nplot_tree(clf_dt, max_depth=1, filled=True, feature_names = X_train.columns.to_list(), fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\nRecall from the data exploration that ratio_to_median_purchase_price was highly correlated with fraud. The decision tree model seems to have picked up on this as well since the first split is based on this variable. Also, note that the order in which the variables are split can differ between different branches of the tree.\nWe can also make predictions using the model and evaluate its performance using the same functions as before\n\ny_pred_dt = clf_dt.predict(X_test)\ny_proba_dt = clf_dt.predict_proba(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_dt)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_dt)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_dt)}\")\nprint(f\"ROC AUC: {roc_auc_score(y_test, y_proba_dt[:, 1])}\")\n\nAccuracy: 0.9999833333333333\nPrecision: 0.9999237223493517\nRecall: 0.999885587887571\nROC AUC: 0.999939141362689\n\n\nThe decision tree performs substantially better than the logistic regression. The ROC AUC score is much closer to the maximum value of 1 and we have an almost perfect classifier\n\n# Compute the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba_dt[:, 1])\n\n# Plot the ROC curve\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], linestyle='--', color='grey')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('ROC Curve')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s also check the confusion matrix to see where we still make mistakes\n\nconf_mat = confusion_matrix(y_test, y_pred_dt, labels=[1, 0]).transpose() # Transpose the sklearn confusion matrix to match the convention in the lecture\nsns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=['Fraud', 'No Fraud'], yticklabels=['Fraud', 'No Fraud'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\nThere are only 3 false negatives, i.e., fraudulent transactions that we did not detect. There are also 2 false positives, i.e., “false alarms”, where non-fraudulent transactions were classified as fraudulent. The decision tree classifier is almost perfect which is a bit suspicious. We might have been lucky in the sense that the training and test sets were split in a way that the model performs very well. We should not expect this to be the case in general. It might be better to use cross-validation to get a more reliable estimate of the model’s performance.\n\n\n3.8.3 Implementing a Random Forest Classifier\nWe can also implement a random forest model using the RandomForestClassifier class from the sklearn.ensemble module. Fitting the model to the data is almost the same as when we used logistic regression and decision trees\n\nclf_rf = RandomForestClassifier(random_state = 0).fit(X_train, y_train)\n\nNote that it takes a bit longer to train the Random Forest since we have to train many trees (the default setting is 100). We can also make predictions using the model and evaluate its performance using the same functions as before\n\ny_pred_rf = clf_rf.predict(X_test)\ny_proba_rf = clf_rf.predict_proba(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_rf)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_rf)}\")\nprint(f\"ROC AUC: {roc_auc_score(y_test, y_proba_rf[:, 1])}\")\n\nAccuracy: 0.9999833333333333\nPrecision: 1.0\nRecall: 0.9998093131459517\nROC AUC: 0.9999999993035008\n\n\nAs expected, the Random Forest performs better than the Decision Tree in the metrics we have used. Now, let’s also check the confusion matrix to see where we still make mistakes\n\nconf_mat = confusion_matrix(y_test, y_pred_rf, labels=[1, 0]).transpose() # Transpose the sklearn confusion matrix to match the convention in the lecture\nsns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=['Fraud', 'No Fraud'], yticklabels=['Fraud', 'No Fraud'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\nThere are still some false negatives, but the number of false positives has decreased compared to the Decision Tree model.\n\n\n3.8.4 Implementing a XGBoost Classifier\nLet’s also have a look at the XGBoost classifier. We can implement the model using the XGBClassifier class from the xgboost package. Fitting the model to the data is almost the same as when we used logistic regression, decision trees, and random forests, even though it is not part of the sklearn package. This is because the xgboost package is designed to work well with the sklearn package. Let’s fit the model to the data\n\nclf_xgb = XGBClassifier(random_state = 0).fit(X_train, y_train)\n\n\ny_pred_xgb = clf_xgb.predict(X_test)\ny_proba_xgb = clf_xgb.predict_proba(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_xgb)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_xgb)}\")\nprint(f\"ROC AUC: {roc_auc_score(y_test, y_proba_xgb[:, 1])}\")\n\nAccuracy: 0.9983366666666667\nPrecision: 0.9893835616438356\nRecall: 0.9916097784218756\nROC AUC: 0.999973496046352\n\n\nLet’s also check the confusion matrix to see where we still make mistakes\n\nconf_mat = confusion_matrix(y_test, y_pred_xgb, labels=[1, 0]).transpose() # Transpose the sklearn confusion matrix to match the convention in the lecture\nsns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=['Fraud', 'No Fraud'], yticklabels=['Fraud', 'No Fraud'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\nThe XGBoost model seems to perform a bit worse than the Random Forest model. There are more false negatives and false positives. However, the model is still very good at detecting fraudulent transactions and has a high ROC AUC score. Adjusting the hyperparameters of the model might improve its performance.\n\n\n3.8.5 Feature Importance\nWe can also look at the feature importance of each model. The feature importance is a measure of how much each feature contributes to the model’s predictions. Let’s start with the Decision Tree model\n\n# Create a DataFrame with the feature importance\ndf_feature_importance_dt = pd.DataFrame({'Feature': X_train.columns, 'Importance': clf_dt.feature_importances_})\ndf_feature_importance_dt = df_feature_importance_dt.sort_values('Importance', ascending=False)\n\n# Plot the feature importance\nplt.barh(df_feature_importance_dt['Feature'], df_feature_importance_dt['Importance'])\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance - Decision Tree')\nplt.show()\n\n\n\n\n\n\n\n\nThis shows that the ratio_to_median_purchase_price is the most important feature for determining whether a transaction is fraudulent or not. Whether a transaction is online, is important as well.\nLet’s also look at the feature importance of the Random Forest model\n\n# Create a DataFrame with the feature importance\ndf_feature_importance_rf = pd.DataFrame({'Feature': X_train.columns, 'Importance': clf_rf.feature_importances_})\ndf_feature_importance_rf = df_feature_importance_rf.sort_values('Importance', ascending=False)\n\n# Plot the feature importance\nplt.barh(df_feature_importance_rf['Feature'], df_feature_importance_rf['Importance'])\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance - Random Forest')\nplt.show()\n\n\n\n\n\n\n\n\nSomewhat surprisingly, XGBoost seems to have picked up on different features than the Decision Tree and Random Forest models. The most important feature is online_order, followed by ratio_to_median_purchase_price as you can see from the plot below\n\n# Create a DataFrame with the feature importance\ndf_feature_importance_xgb = pd.DataFrame({'Feature': X_train.columns, 'Importance': clf_xgb.feature_importances_})\ndf_feature_importance_xgb = df_feature_importance_xgb.sort_values('Importance', ascending=False)\n\n# Plot the feature importance\nplt.barh(df_feature_importance_xgb['Feature'], df_feature_importance_xgb['Importance'])\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance - XGBoost')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.8.6 Permuation Importance\nWe can also look at the permutation importance of each model. The permutation importance is a measure of how much each feature contributes to the model’s predictions. The permutation importance is calculated by permuting the values of each feature and measuring how much the model’s performance decreases. Let’s start with the Decision Tree model\n\n# Calculate the permutation importance\nresult_dt = permutation_importance(clf_dt, X_test, y_test, n_repeats=10, random_state=0)\n\n# Create a DataFrame with the permutation importance\ndf_permutation_importance_dt = pd.DataFrame({'Feature': X_train.columns, 'Importance': result_dt.importances_mean})\ndf_permutation_importance_dt = df_permutation_importance_dt.sort_values('Importance', ascending=False)\n\n# Plot the permutation importance\nplt.barh(df_permutation_importance_dt['Feature'], df_permutation_importance_dt['Importance'])\nplt.xlabel('Accuracy Decrease')\nplt.ylabel('Feature')\nplt.title('Permutation Importance - Decision Tree')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s also look at the permutation importance of the Random Forest model\n\n# Calculate the permutation importance\nresult_rf = permutation_importance(clf_rf, X_test, y_test, n_repeats=10, random_state=0)\n\n# Create a DataFrame with the permutation importance\ndf_permutation_importance_rf = pd.DataFrame({'Feature': X_train.columns, 'Importance': result_rf.importances_mean})\ndf_permutation_importance_rf = df_permutation_importance_rf.sort_values('Importance', ascending=False)\n\n# Plot the permutation importance\nplt.barh(df_permutation_importance_rf['Feature'], df_permutation_importance_rf['Importance'])\nplt.xlabel('Accuracy Decrease')\nplt.ylabel('Feature')\nplt.title('Permutation Importance - Random Forest')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s also look at the permutation importance of the XGBoost model\n\n# Calculate the permutation importance\nresult_xgb = permutation_importance(clf_xgb, X_test, y_test, n_repeats=10, random_state=0)\n\n# Create a DataFrame with the permutation importance\ndf_permutation_importance_xgb = pd.DataFrame({'Feature': X_train.columns, 'Importance': result_xgb.importances_mean})\ndf_permutation_importance_xgb = df_permutation_importance_xgb.sort_values('Importance', ascending=False)\n\n# Plot the permutation importance\nplt.barh(df_permutation_importance_xgb['Feature'], df_permutation_importance_xgb['Importance'])\nplt.xlabel('Accuracy Decrease')\nplt.ylabel('Feature')\nplt.title('Permutation Importance - XGBoost')\nplt.show()\n\n\n\n\n\n\n\n\nHere the results for the three models are quite similar. The most important feature is ratio_to_median_purchase_price, followed by online_order.\n\n\n3.8.7 Conclusions\nIn this notebook, we have seen how to implement decision trees, random forests, and XGBoost classifiers in Python. We have also seen how to evaluate the performance of these models using metrics such as accuracy, precision, recall, and ROC AUC. We have seen that the Random Forest and XGBoost models perform better than the Decision Tree model. Furthermore, we looked at the feature and permutation importance of each model to see which features are most important for determining whether a transaction is fraudulent or not.\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning - Data Mining, Inference, and Prediction. Second Edition. Springer.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press. https://probml.github.io/pml-book/book1.html.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "decisiontrees.html#footnotes",
    "href": "decisiontrees.html#footnotes",
    "title": "3  Decision Trees",
    "section": "",
    "text": "Note to handle missing input data one can use “backup” variables that are correlated with the variable of interest and can be used to make a split whenever the data is missing. Such splits are called surrogate splits. In the case of categorical variables, one can also use a separate category for missing values.↩︎",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html",
    "href": "neuralnetworks.html",
    "title": "4  Neural Networks",
    "section": "",
    "text": "4.1 What is a Neural Network?\nNeural networks are at the core of many cutting-edge machine learning models. They can be used as both a supervised and unsupervised learning method. In this course, we will focus on their application in supervised learning where they are used for both regression and classification tasks. While they are conceptually not much more difficult to understand than decision trees, a neural network is not as easy to interpret as a decision tree. For this reason, they are often called black boxes, meaning that it is not so clear what is happening inside. Furthermore, neural networks tend to be more difficult to train and for tabular data, which is the type of structured data that you will typically encounter, gradient-boosted decision trees tend to perform better. Nevertheless, since neural networks are what enabled many of the recent advances in AI, they are an important topic to cover, even if it is only to better understand what has been driving recent innovations.\nIt is common to represent neural networks as directed graphs. Figure 4.1 shows a single-layer feedforward neural network with \\(N=2\\) inputs, \\(M=3\\) neurons in the hidden layer, and a single output. The input layer is connected to the hidden layer, which is connected to the output layer. For simplicity, we will only consider neural networks that are feedforward (i.e. their graphs are acyclical), with dense layers (i.e. each layer is fully connected to the previous), and without connections that skip layers.\nAs we will see later on, under certain (relatively weak) conditions\nThis makes them interesting for a wide range of fields in economics, e.g., quantitative macroeconomics or econometrics. However, neural networks are not a magic bullet, and there are some downsides in terms of the large data requirements, interpretability and training difficulty.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#what-is-a-neural-network",
    "href": "neuralnetworks.html#what-is-a-neural-network",
    "title": "4  Neural Networks",
    "section": "",
    "text": "Figure 4.1: A Single-Layer Feedforward Neural Network\n\n\n\n\n\n\nNeural networks are universal approximators (can approximate any (Borel measurable) function)\nNeural networks break the curse of dimensionality (can handle very high dimensional functions)\n\n\n\n4.1.1 Origins of the Term “Neural Network”\n\n\n\n\n\n\nFigure 4.2: A biological neuron (Source: Wikipedia)\n\n\n\nThe term “neural network” originates in attempts to find mathematical representations of information processing in biological systems (Bishop 2006). The biological interpretation not very important for research anymore and one should not get too hung up on it. However, the interpretation can be useful when starting to learn about neural networks. Figure 4.2 shows a biological neuron.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#an-artificial-neuron",
    "href": "neuralnetworks.html#an-artificial-neuron",
    "title": "4  Neural Networks",
    "section": "4.2 An Artificial Neuron",
    "text": "4.2 An Artificial Neuron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Artificial Neuron\n\n\n\nArtificial neurons are the basic building blocks of neural networks. Figure 4.3 shows a single artificial neuron. The \\(N\\) inputs denoted \\(x=(x_1,x_2,\\ldots,x_N)'\\) are linearly combined into \\(z\\) using weights \\(w\\) and bias \\(b\\)\n\\[z = b + \\sum_{i=1}^N w_i x_i = \\sum_{i=0}^N w_i x_i\\]\nwhere we defined an additional input \\(x_0=1\\) and \\(w_0=b\\).\nThe linear combination \\(z\\) is transformed using an activation function \\(\\phi(z)\\).\n\\[a = \\phi(z) = \\phi\\left( \\sum_{i=0}^N w_i x_i \\right)\\]\nThe activation function introduces non-linearity into the neural network and allows it to learn highly non-linear functions. The particular choice of activation function depends on the application.\nThis should look familiar to you already. If we set \\(\\phi(z)=z\\), we get a linear regression model and if we set \\(\\phi(z)=\\frac{1}{1+e^{-z}}\\), we get a logistic regression model. This is because the basic building block, the artificial neuron, is a generalized linear model.\n\n4.2.1 Activation Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Activation Functions\n\n\n\nCommon activation functions include\n\nSigmoid: \\(\\phi(z) = \\frac{1}{1+e^{-z}}\\)\nHyperbolic tangent: \\(\\phi(z) = tanh(z)\\)\nRectified linear unit (ReLU): \\(\\phi(z) = \\max(0,z)\\)\nSoftplus: \\(\\phi(z) =\\log(1+e^{z})\\)\n\nReLU has become popular in deep neural networks in recent years because of its good performance in these applications. Since economic problems usually involve smooth functions, softplus can be a good alternative.\n\n\n4.2.2 A Special Case: Perceptron\nPerceptrons were developed in the 1950s and have only one artificial neuron. Perceptrons use a step function as an activation function\n\\[\\phi(z) = \\begin{cases}\n      1 & \\text{if } z \\geq 0\\\\\n      0 & \\text{otherwise}\\,,\n    \\end{cases}\\]\nPerceptrons can be used for basic classification. However, the step function is usually not used in neural networks because it is not differentiable at \\(z=0\\) and zero everywhere else. This makes it unsuitable for the back-propagation algorithm, which is used for determining the network weights.\n\n\n\n\n\n\nMini-Exercise\n\n\n\nWhat would the decision boundary of a perceptron look like if we have two inputs \\(x_1\\) and \\(x_2\\) and the weights \\(w_1=1\\), \\(w_2=1\\), and \\(b=-1\\)?",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#building-a-neural-network-from-artificial-neurons",
    "href": "neuralnetworks.html#building-a-neural-network-from-artificial-neurons",
    "title": "4  Neural Networks",
    "section": "4.3 Building a Neural Network from Artificial Neurons",
    "text": "4.3 Building a Neural Network from Artificial Neurons\nWe can build a neural network by stacking multiple artificial neurons. For this reason, it is sometimes also called a multilayer perceptron (MLP). A single-layer neural network is a linear combination of \\(M\\) artificial neurons \\(a_j\\)\n\\[ a_j = \\phi(z_j) = \\phi\\left( b_{j}^{1} + \\sum_{i=1}^N w_{ji}^{1} x_i \\right)\\]\nwith the output defined as\n\\[ g(x ; w) = b^{2}+\\sum_{j=1}^{M} w_{j}^{2} a_j\\]\nwhere \\(N\\) is the number of inputs, \\(M\\) is the number of neurons in the hidden layer, and \\(w\\) are the weights and biases of the network. The width of the neural network is \\(M\\).\nFigure 4.5 shows a single-layer feedforward neural network with \\(N=2\\) inputs, \\(M=3\\) neurons in the hidden layer, and a single output. Note that the biases can be thought of as additional weights that are multiplied by a constant input of 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: A Single-Layer Feedforward Neural Network with Biases shown explicitly",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#relation-to-linear-regression",
    "href": "neuralnetworks.html#relation-to-linear-regression",
    "title": "4  Neural Networks",
    "section": "4.4 Relation to Linear Regression",
    "text": "4.4 Relation to Linear Regression\nNote that if we use a linear activation function, e.g. \\(\\phi(x)=x\\), the neural network collapses to a linear regression\n\\[ y \\cong g(x ; w) = \\tilde{w}_{0} +\\sum_{i=1}^{N} \\tilde{w}_{i}  x_{i}\\]\nwith appropriately defined regression coefficients \\(\\tilde{w}\\).\nRecall that in our description of Figure 2.2 we argued that a machine learning algorithm would automatically turn the slider to find the best fit. This is exactly what the training algorithm has to do to train a neural network.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#a-simple-example",
    "href": "neuralnetworks.html#a-simple-example",
    "title": "4  Neural Networks",
    "section": "4.5 A Simple Example",
    "text": "4.5 A Simple Example\nSuppose we want to approximate \\(f(x)=exp(x)-x^3\\) with 3 neurons. The approximation might be\n\\[\\hat{f}(x)=a_1+a_2-a_3\\]\nwhere\n\\[a_1=max(0,-3x-1.5)\\]\n\\[a_2=max(0,x+1)\\]\n\\[a_3=max(0,3x-3)\\]\nOur neural network in this case uses ReLU activation functions and has all weights equal to one in the output layer. Figure 4.6 shows the admittedly poor approximation of \\(f(x)\\) by \\(\\hat{f}(x)\\) using this neural network. Given the piecewise linear nature of the ReLU activation function, the approximation is not very good. However, with more neurons, we could get a better approximation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Approximation by a Neural Network\n\n\n\nFigure 4.7 shows an interactive version of Figure 4.6 where you can adjust the weights of the neural network to approximate a simple dataset. As you can see, it is quite tricky to find parameters that approximate the function well. This is where the training algorithm comes in. It will automatically adjust the weights to minimize a loss function.\n\n\n\n{\n    //\n  const dataRange = [-2, 2];\n  const yDataRange = [-2, 9];\n\n  // Compute approximation and true function\n    function feedforward(inputRange) {\n\n        let loss = [];\n        let N = 100;\n\n        for (let i = 0; i &lt;= 100; i += 1) {\n            let x = inputRange[0] + i * (inputRange[1] - inputRange[0])/100\n      let h1 = Math.max(0, w11*x + b11);\n      let h2 = Math.max(0, w12*x + b12);\n      let h3 = Math.max(0, w13*x + b13);\n      let y = b2 + w21 * h1 + w22 * h2 - w23 * h3;\n            let dataPoint = {\"x\": x, \"y\": y};\n            loss.push(dataPoint)\n        }\n\n        return loss;\n\n    }\n\n  function truefunction(inputRange) {\n\n        let loss = [];\n        let N = 100;\n\n        for (let i = 0; i &lt;= 100; i += 1) {\n            let x = inputRange[0] + i * (inputRange[1] - inputRange[0])/100\n      let y = Math.exp(x) - x**3;\n            let dataPoint = {\"x\": x, \"y\": y};\n            loss.push(dataPoint)\n        }\n\n        return loss;\n\n    }\n\n\n    let approxData = feedforward(dataRange)\n  let trueData = truefunction(dataRange)\n\n  // Plot approximation and true function\n    return Plot.plot({\n            marginLeft: 40,\n            marginTop: 20,\n            width: width - 2,\n            height: width * 2/3 - 2,\n            x: { domain: dataRange, label: \"x\"},\n            y: { domain: yDataRange, label: \"y\"},\n            marks: [\n                Plot.frame(),\n                Plot.line(trueData, {x: \"x\", y: \"y\", stroke: \"red\"}),\n                Plot.line(approxData, {x: \"x\", y: \"y\", stroke: \"steelblue\"})\n            ],\n            nice: true\n        })\n}\n\n\n\n\n\n\n\ninputRange = [-5, 5];\nviewof w11 = Inputs.range(inputRange, {value: -3, label: \"w₁₁\", step: 0.01})\nviewof w12 = Inputs.range(inputRange, {value: 1, label: \"w₁₂\", step: 0.01})\nviewof w13 = Inputs.range(inputRange, {value: 3, label: \"w₁₃\", step: 0.01})\nviewof w21 = Inputs.range(inputRange, {value: 1, label: \"w₂₁\", step: 0.01})\nviewof w22 = Inputs.range(inputRange, {value: 1, label: \"w₂₂\", step: 0.01})\nviewof w23 = Inputs.range(inputRange, {value: 1, label: \"w₂₃\", step: 0.01})\nviewof b11 = Inputs.range(inputRange, {value: -1.5, label: \"b₁₁\", step: 0.01})\nviewof b12 = Inputs.range(inputRange, {value: 1, label: \"b₁₂\", step: 0.01})\nviewof b13 = Inputs.range(inputRange, {value: -3, label: \"b₁₃\", step: 0.01})\nviewof b2 = Inputs.range(inputRange, {value: 0, label: \"b₂\", step: 0.01})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: Interactive Neural Network Approximation\n\n\n\n\n\n\n\n\nTensorFlow Playground\n\n\n\nIf you want to play around with neural networks, you can use the TensorFlow Playground: https://playground.tensorflow.org. It is a web-based tool that allows you to experiment with neural networks and see how they learn. Figure 4.8 shows the interface of the TensorFlow Playground.\n\n\n\n\n\n\nFigure 4.8: Tesorflow Playground",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#deep-neural-networks",
    "href": "neuralnetworks.html#deep-neural-networks",
    "title": "4  Neural Networks",
    "section": "4.6 Deep Neural Networks",
    "text": "4.6 Deep Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: Deep Neural Network\n\n\n\nDeep neural networks have more than one hidden layer. The number of hidden layers is also called the depth of the neural network. Deep neural networks can learn more complicated things. For simple function approximation, a single hidden layer is sufficient. Figure 4.9 shows a deep neural network with two hidden layers.\nThe first hidden layer consists of \\(M_1\\) artificial neurons with inputs \\(x_1,x_2,\\ldots,x_N\\)\n\\[a_j^{1} = \\phi\\left( b_{j}^{1} + \\sum_{i=1}^N w_{ji}^{1} x_i \\right)\\]\nThe second hidden layer consists of \\(M_2\\) artificial neurons with inputs \\(a_1^{1},a_2^{1},\\ldots,a_{M_1}^{1}\\)\n\\[a_k^{2} = \\phi\\left( b_{k}^{2} + \\sum_{j=1}^{M_1} w_{kj}^{2} a_j^{1} \\right)\\]\nAfter \\(Q\\) hidden layers, the output is defined as\n\\[y \\cong g(x ; w) = b^{Q+1}+\\sum_{j=1}^{M_{Q}} w_{j}^{Q+1} a_j^{Q}\\]\nNote that the activation functions do not need to be the same everywhere. In principle, we could vary the activation functions even within a layer.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#universal-approximation-and-the-curse-of-dimensionality",
    "href": "neuralnetworks.html#universal-approximation-and-the-curse-of-dimensionality",
    "title": "4  Neural Networks",
    "section": "4.7 Universal Approximation and the Curse of Dimensionality",
    "text": "4.7 Universal Approximation and the Curse of Dimensionality\nRecall that we want to approximate an unknown function in supervised learning tasks\n\\[y = f(x)\\]\nwhere \\(y=(y_1,y_2,\\ldots,y_K)'\\) and \\(x=(x_1,x_2,\\ldots,x_N)'\\) are vectors. The function \\(f(x)\\) could stand for many different functions in economics (e.g. a value function, a policy function, a conditional expectation, a classifier, \\(\\ldots\\)).\nIt turns out that neural networks are universal approximators and break the curse of dimensionality. The universal approximation theorem by Hornik, Stinchcombe, and White (1989) states:\n\nA neural network with at least one hidden layer can approximate any Borel measurable function mapping finite-dimensional spaces to any desired degree of accuracy.\n\nBreaking the curse of dimensionality (Barron, 1993)\n\nA one-layer NN achieves integrated square errors of order \\(O(1/M)\\), where \\(M\\) is the number of nodes. In comparison, for series approximations, the integrated square error is of order \\(O(1/(M^{2/N}))\\) where \\(N\\) is the dimensions of the function to be approximated.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#training-a-neural-network-determining-weights-and-biases",
    "href": "neuralnetworks.html#training-a-neural-network-determining-weights-and-biases",
    "title": "4  Neural Networks",
    "section": "4.8 Training a Neural Network: Determining Weights and Biases",
    "text": "4.8 Training a Neural Network: Determining Weights and Biases\nWe have not yet discussed how to determine the weights and biases. The weights and biases \\(w\\) are selected to minimize a loss function\n\\[E(w; X, Y) = \\frac{1}{N} \\sum_{n=1}^{N} E_n(w; x_n, y_n)\\]\nwhere \\(N\\) refers to the number of input-output pairs that we use for training and \\(E_n(w; x_n, y_n)\\) refers to the loss of an individual pair \\(n\\).\nFor notational simplicity, I will write \\(E(w)\\) and \\(E_n(w)\\) in the following or in some cases even omit argument \\(w\\).\n\n4.8.1 Choice of Loss Function\nThe choice of loss function depends on the problem at hand. In regressions, one often uses a mean squared error (MSE) loss\n\\[E_n(w; x_n, y_n) = \\frac{1}{2} \\left\\|g\\left(x_{n}; w\\right)-y_{n}\\right\\|^{2}\\]\nIn classification problems, one often uses a cross-entropy loss\n\\[E_n(w; x_n, y_n) = \\sum_{k=1}^K y_{nk} \\log(g_k(x_n;w))\\]\nwhere \\(k\\) refers to \\(k\\)th class (or \\(k\\)th element) in the output vector.\n\n\n4.8.2 Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.10: Gradient Descent\n\n\n\nThe weights and biases are determined by minimizing the loss function using a gradient descent algorithm. The basic idea is to compute how the loss changes with the weights \\(w\\) and step into the direction that reduces the loss. Figure 4.10 shows a simple example of a loss function and the gradient descent algorithm. The basic steps of the algorithm are\n\nInitialize weights (e.g. draw from Gaussian distribution)\n\n\\[w^{(0)} \\sim N(0,I)\\]\n\nCompute the gradient of the loss function with respect to weights\n\n\\[\\nabla E(w^{(i)}) = \\frac{1}{N}\\sum_{n=1}^N \\nabla E_n\\left(w^{(i)}\\right)\\]\n\nUpdate weights (make a small step in the direction of the negative gradient)\n\n\\[w^{(i+1)} = w^{(i)} - \\eta \\nabla E\\left(w^{(i)}\\right)\\]\nwhere \\(\\eta&gt;0\\) is the learning rate.\n\nRepeat Steps 2 and 3 until a terminal condition (e.g. fixed number of iterations) is reached.\n\nIf we use the batch gradient descent algorithm described above, we might get stuck in a local minimum. To avoid this, we can use\n\nStochastic gradient descent: Use only a single observation to compute the gradient and update the weights for each observation\n\\[w^{(i+1)} = w^{(i)} - \\eta \\nabla E_n\\left(w^{(i)}\\right)\\]\nMinibatch gradient descent: Use a small batch of observations (e.g. 32) to compute the gradient and update the weights for each minibatch\n\nThese algorithms are less likely to get stuck in a shallow local minimum of the loss function because they are “noisier”. Figure 4.11 shows a comparison of the different gradient descent algorithms. Minibatch gradient descent is probably the most commonly used and is also what we will be using in our implementation in Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.11: Comparison of Gradient Descent Types (blue: Full Batch, red: Minibatch, orange: Stochastic)\n\n\n\n\n\n4.8.3 Backpropagation Algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.12: Backpropagation Algorithm\n\n\n\nComputing the gradient seems to be a daunting task since a weight in the first layer in a deep neural network affects the loss function potentially through thousands of “paths”. The backpropagation algorithm (Rumelhart et al., 1986) provides an efficient way to evaluate the gradient. The basic idea is to go backward through the network to evaluate the gradient as shown in Figure 4.12. If you are interested in the details, I recommend reading the notes by Nielsen (2019).",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#practical-considerations",
    "href": "neuralnetworks.html#practical-considerations",
    "title": "4  Neural Networks",
    "section": "4.9 Practical Considerations",
    "text": "4.9 Practical Considerations\nFrom a practical perspective, there are many more things to consider. Often times it’s beneficial to do some (or all) of the following\n\nInput/output normalization: (e.g. to have unit variance and mean zero) can improve the performance of the NN\nCheck for overfitting: by splitting the dataset into a training dataset and a test dataset\nRegularization: to avoid overfitting (e.g. add a term to lose function that penalizes large weights)\nAdjust the learning rate: \\(\\eta\\) during training\n\nWe have already discussed some of these topics in the context of other machine learning algorithms.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neuralnetworks.html#python-implementation",
    "href": "neuralnetworks.html#python-implementation",
    "title": "4  Neural Networks",
    "section": "4.10 Python Implementation",
    "text": "4.10 Python Implementation\n\nLet’s have a look at how to implement a neural network in Python.\n\n4.10.1 Implementing the Feedforward Part of a Neural Network\nAs a small programming exercise and to improve our understanding of neural networks, let’s implement the feedforward part of a neural network from scratch. We will have to calculate the output of the network for some given weights and biases, as well as some inputs. Let’s start by importing the necessary libraries\n\nimport numpy as np\n\nNext, we define the activation function for which we use the sigmoid function\n\ndef activation_function(x):\n    return 1/(1+np.exp(-x)) # sigmoid function\n\nNow, we define the feedforward function which calculates the output of the neural network given some inputs, weights, and biases. The function takes the inputs, weights, and biases as arguments and returns the output of the network\n\ndef feedforward(inputs, w1, w2, b1, b2):\n\n    # Compute the pre-activation values for the first layer\n    z = b1 + np.matmul(w1, inputs)\n\n    # Compute the post-activation values for the first layer\n    a = activation_function(z)\n\n    # Combine the post-activation values of the first layer to an output\n    g = b2 + np.matmul(w2, a)\n\n    return g\n\nMathematically, the function computes the following\n\\(z = b^{1} + w^1 x\\)\n\\(a = \\phi(z)\\)\n\\(g = b^2 + w^2 a\\)\nand returns \\(g\\) at the end. We have written this using matrix notation to make it more compact. Remember that node \\(j\\) in the hidden layer is given by\n\\(z_j = b_{j}^{1} + \\sum_{i=1}^N w_{ji}^{1} x_i\\)\n\\(a_j = \\phi(z_j)\\)\nand the output of the network is given by\n\\(g(x ; w) = b^{2}+\\sum_{j=1}^{M} w_{j}^{2} a_j.\\)\nLet’s test the function with some example inputs, weights and biases\n\n# Define the weights and biases\nw1 = np.array([[0.1, 0.2], [0.3, 0.4]]) # 2x2 matrix\nw2 = np.array([0.5, 0.6]) # 1-d vector\nb1 = np.array([0.1, 0.2]) # 1-d vector\nb2 = 0.3\n\n# Define the inputs\ninputs = np.array([1, 2]) # 1-d vector\n\n# Compute the output of the network\nfeedforward(inputs, w1, w2, b1, b2)\n\nnp.float64(1.0943291429384328)\n\n\nTo operationalize this, we would also need to define a loss function and an optimization algorithm to update the weights and biases. However, this is beyond the scope of this course.\n\n\n4.10.2 Using Neural Networks in Sci-Kit Learn\nSci-kit learn provides a simple interface to use neural networks. However, it is not as flexible as the more commonly used PyTorch or TensorFlow. We can reuse the dataset of credit card transactions from Kaggle.com to demonstrate how to use neural networks in scikit-learn.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, recall_score, precision_score, roc_curve\npd.set_option('display.max_columns', 50) # Display up to 50 columns\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\nimport os.path\n\n# Check if the file exists\nif not os.path.isfile('data/card_transdata.csv'):\n\n    print('Downloading dataset...')\n\n    # Define the dataset to be downloaded\n    zipurl = 'https://www.kaggle.com/api/v1/datasets/download/dhanushnarayananr/credit-card-fraud'\n\n    # Download and unzip the dataset in the data folder\n    with urlopen(zipurl) as zipresp:\n        with ZipFile(BytesIO(zipresp.read())) as zfile:\n            zfile.extractall('data')\n\n    print('DONE!')\n\nelse:\n\n    print('Dataset already downloaded!')\n\n# Load the data\ndf = pd.read_csv('data/card_transdata.csv')\n\n# Split the data into training and test sets\nX = df.drop('fraud', axis=1) # All variables except `fraud`\ny = df['fraud'] # Only our fraud variables\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3, random_state = 42)\n\n# Scale the features\ndef scale_features(scaler, df, col_names, only_transform=False):\n\n    # Extract the features we want to scale\n    features = df[col_names] \n\n    # Fit the scaler to the features and transform them\n    if only_transform:\n        features = scaler.transform(features.values)\n    else:\n        features = scaler.fit_transform(features.values)\n\n    # Replace the original features with the scaled features\n    df[col_names] = features\n\ncol_names = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price'] \nscaler = StandardScaler() \nscale_features(scaler, X_train, col_names)\nscale_features(scaler, X_test, col_names, only_transform=True)\n\nDataset already downloaded!\n\n\nRecall that the target variable \\(y\\) is fraud, which indicates whether the transaction is fraudulent or not. The other variables are the features \\(x\\) of the transactions.\nTo use a neural network for a classification task, we can use the MLPClassifier class from scikit-learn. The following code snippet shows how to use a neural network with one hidden layer with 16 nodes\n\nclf = MLPClassifier(hidden_layer_sizes=(16,), random_state=42, verbose=False).fit(X_train, y_train)\n\nIf you would like to use a neural network with multiple hidden layers, you can specify the number of nodes per hidden layer using the hidden_layer_sizes parameter. For example, the following code snippet shows how to use a neural network with two hidden layers, one with 5 nodes and the other with 4 nodes\n\nclf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5,4), activation='logistic', random_state=42).fit(X_train, y_train)\n\nNote that the alpha parameter specifies the regularization strength, the activation parameter specifies the activation function (by default it uses relu) and the random_state parameter specifies the seed for the random number generator (useful for reproducible results).\nWe can check the loss curve to see how the neural network loss declined during training\n\nplt.plot(clf.loss_curve_)\nplt.title(\"Loss Curve\", fontsize=14)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\nWe can then use the same way to evaluate the neural network performance as we did for the other ML models\n\ny_pred = clf.predict(X_test)\ny_proba = clf.predict_proba(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")\nprint(f\"ROC AUC: {roc_auc_score(y_test, y_proba[:, 1])}\")\n\nAccuracy: 0.9955266666666667\nPrecision: 0.971747127308582\nRecall: 0.9772319896266352\nROC AUC: 0.9996638991577014\n\n\nThe neural network performs substantially better than the logistic regression. As in the case of the tree-based methods, the ROC AUC score is much closer to the maximum value of 1 and we have an almost perfect classifier\n\n# Compute the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1])\n\n# Plot the ROC curve\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], linestyle='--', color='grey')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('ROC Curve')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s also check the confusion matrix to see where we still make mistakes\n\nconf_mat = confusion_matrix(y_test, y_pred, labels=[1, 0]).transpose() # Transpose the sklearn confusion matrix to match the convention in the lecture\nsns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=['Fraud', 'No Fraud'], yticklabels=['Fraud', 'No Fraud'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\nThere are around 270 false negatives, i.e., a fraudulent transaction that we did not detect. There are also around 980 false positives, i.e., “false alarms”, where non-fraudulent transactions were classified as fraudulent.\n\n\n4.10.3 Using Neural Networks in PyTorch\nWhile it is possible to use neural networks in scikit-learn, it is more common to use PyTorch or TensorFlow for neural networks. PyTorch is a popular deep-learning library that is widely used in academia and industry. In this section, we will show how to use PyTorch to build a simple neural network for the same credit card fraud detection task.\n\n\n\n\n\n\nFeel Free to Skip This Section\n\n\n\nThis section might be a bit more challenging than what we have looked at previously. If you think that you are not ready for this, feel free to skip this section. This is mainly meant to be a starting point for those who are interested in learning more about neural networks.\nFor a more in-depth introduction to PyTorch, I recommend that you check out the official PyTorch tutorials. This section, in particular, builds on the Learning PyTorch with Examples tutorial.\n\n\nLet’s start by importing the necessary libraries\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nThen, let’s prepare the data for PyTorch. We need to convert the data in our DataFrame to PyTorch tensors\n\nX_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n\nNote that we also converted the input values to float32 for improved training speed and the target values to long which is a type of integer (remember our target y can only take values zero or one). Next, we need to create a DataLoader object to load the data in mini-batches during the training process\n\ndataset = TensorDataset(X_train_tensor, y_train_tensor)\ndataloader = DataLoader(dataset, batch_size=200, shuffle=True)\ndataset_size = len(dataloader.dataset)\n\nNext, we define the neural network model using the nn module from PyTorch\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(7, 16), # 7 input features, 16 nodes in the hidden layer\n    torch.nn.ReLU(),        # ReLU activation function\n    torch.nn.Linear(16, 2) # 16 nodes in the hidden layer, 2 output nodes (fraud or no fraud)\n)\n\nWe also need to define the loss function and the optimizer. We will use the cross-entropy loss function and the Adam optimizer\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5) # Adam optimizer with learning rate of 0.001 and L2 regularization (analogous to alpha in scikit-learn)\n\nWe can now train the neural network using the following code snippet\n\nfor epoch in range(80):\n\n    # Loop over batches in an epoch using DataLoader\n    for id_batch, (X_batch, y_batch) in enumerate(dataloader):\n\n        # Compute the predicted y using the neural network model with the current weights\n        y_batch_pred = model(X_batch)\n\n        # Compute the loss\n        loss = loss_fn(y_batch_pred, y_batch)\n\n        # Reset the gradients of the loss function to zero\n        optimizer.zero_grad()\n\n        # Compute the gradient of the loss with respect to model parameters\n        loss.backward()\n\n        # Update the weights by taking a \"step\" in the direction that reduces the loss\n        optimizer.step()\n\n    if epoch % 10 == 9:\n        print(f\"Epoch {epoch} loss: {loss.item():&gt;7f}\")\n\nEpoch 9 loss: 0.024499\nEpoch 19 loss: 0.008713\nEpoch 29 loss: 0.016122\nEpoch 39 loss: 0.007585\nEpoch 49 loss: 0.005461\nEpoch 59 loss: 0.020942\nEpoch 69 loss: 0.016723\nEpoch 79 loss: 0.007653\n\n\nNote that here we are updating the model weights for each mini-batch in the dataset and go over the whole dataset 80 times (epochs). We print the loss every epoch to see how the loss decreases over time.\nThe following snippet shows how to use full-batch gradient descent instead of mini-batch gradient descent\n\nfor epoch in range(2000):\n\n    # Compute the predicted y using the neural network model with the current weights\n    y_epoch_pred = model(X_train_tensor)\n\n    # Compute the loss\n    loss = loss_fn(y_epoch_pred, y_train_tensor)\n\n    # Reset the gradients of the loss function to zero\n    optimizer.zero_grad()\n\n    # Compute the gradient of the loss with respect to model parameters\n    loss.backward()\n\n    # Update the weights by taking a \"step\" in the direction that reduces the loss\n    optimizer.step()\n\n    # Print the loss every 100 epochs\n    if epoch % 100 == 99:\n        print(f\"Epoch {epoch} loss: {loss.item():&gt;7f}\")\n\nEpoch 99 loss: 0.009982\nEpoch 199 loss: 0.009945\nEpoch 299 loss: 0.009928\nEpoch 399 loss: 0.009920\nEpoch 499 loss: 0.009914\nEpoch 599 loss: 0.009910\nEpoch 699 loss: 0.009907\nEpoch 799 loss: 0.009904\nEpoch 899 loss: 0.009901\nEpoch 999 loss: 0.009899\nEpoch 1099 loss: 0.009897\nEpoch 1199 loss: 0.009895\nEpoch 1299 loss: 0.009893\nEpoch 1399 loss: 0.009891\nEpoch 1499 loss: 0.009890\nEpoch 1599 loss: 0.009888\nEpoch 1699 loss: 0.009886\nEpoch 1799 loss: 0.009885\nEpoch 1899 loss: 0.009883\nEpoch 1999 loss: 0.009881\n\n\nNote that in this version we are updating the model weights 2000 times (epochs) and printing the loss every 100 epochs. We can now evaluate the model on the test set\n\nX_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\ny_pred = torch.argmax(model(X_test_tensor), dim=1).numpy()\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")\n\nAccuracy: 0.9965833333333334\nPrecision: 0.9775587566338135\nRecall: 0.9834865184394188\n\n\nNote that for simplicity we are reusing the sci-kit learn metrics to evaluate the model.\nHowever, our neural network trained in PyTorch does not perform exactly the same as the neural network trained in scikit-learn. This is likely because of different hyperparameters or different initializations of the weights. In practice, it is common to experiment with different hyperparameters to find the best model or to use grid search and cross-validation to try many values and find the best-performing ones.\n\n\n4.10.4 Conclusions\nIn this chapter, we have learned about neural networks, which are the foundation of deep learning. We have seen how to implement parts of a simple neural network from scratch and how to use neural networks in scikit-learn and PyTorch.\n\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Edited by Michael Jordan, Jon Kleinberg, and Bernhard Schölkopf. Information Science and Statistics. Springer Science+Business Media, LLC. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nNielsen, Michael. 2019. Neural Networks and Deep Learning. http://neuralnetworksanddeeplearning.com.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "additional.html",
    "href": "additional.html",
    "title": "5  Additional Methods",
    "section": "",
    "text": "5.1 K-Nearest Neighbors\nThe K-Nearest Neighbors (KNN) algorithm is a simple and intuitive method for classification and regression meaning that it belongs to the class of supervised learning methods. The KNN algorithm uses the \\(K\\) nearest neighbors of a data point to make a prediction. For example, in the case of a regression task, the prediction \\(\\hat{y}\\) for a new data point \\(x\\) is\n\\[\\hat{y} = \\frac{1}{K}\\sum_{x_i\\in N_k(x)} y_i\\]\ni.e., the average of the \\(K\\) nearest neighbors of \\(x\\). In the case of a classification task, the prediction \\(\\hat{y}\\) is the majority class of the \\(K\\) nearest neighbors of \\(x\\).\nFigure 5.1 shows an example of the K-Nearest Neighbors algorithm applied to a dataset with two classes. The decision boundary is shown as a shaded area.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Additional Methods</span>"
    ]
  },
  {
    "objectID": "additional.html#k-nearest-neighbors",
    "href": "additional.html#k-nearest-neighbors",
    "title": "5  Additional Methods",
    "section": "",
    "text": "Figure 5.1: K-Nearest Neighbors Classification with \\(K=5\\) (Classification shown as Shaded Area)",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Additional Methods</span>"
    ]
  },
  {
    "objectID": "additional.html#k-means-clustering",
    "href": "additional.html#k-means-clustering",
    "title": "5  Additional Methods",
    "section": "5.2 K-means Clustering",
    "text": "5.2 K-means Clustering\nK-means is a method that is used for finding clusters in a set of unlabeled data meaning that it is an unsupervised learning method. For the algorithm to work, one has to choose a fixed number of clusters \\(K\\) for which the algorithm will then try to find the cluster centers (i.e., the means) using an iterative procedure. The basic algorithm proceeds as follows given a set of initial guesses for the \\(K\\) cluster centers:\n\nAssign each data point to the nearest cluster center\nRecompute the cluster centers as the mean of the data points assigned to each cluster\n\nThe algorithm iterates over these two steps until the cluster centers do not change or the change is below a certain threshold. As an initial guess, one can use, for example, \\(K\\) randomly chosen observations as cluster centers.\nWe need some measure of disimilarity (or distance) to assign data points to the nearest cluster center. The most common choice is the Euclidean distance. The squared Euclidean distance between two points \\(x\\) and \\(y\\) in \\(p\\)-dimensional space is defined as\n\\[d(x_i, x_j) = \\sum_{n=1}^p (x_{in} - x_{jn})^2=\\lVert x_i - x_j \\rVert^2\\]\nwhere \\(x_{in}\\) and \\(x_{jn}\\) are the \\(n\\)-th feature of the \\(i\\)-th and \\(j\\)-th observation in our dataset, respectively.\nThe objective function of the K-means algorithm is to minimize the sum of squared distances between the data points and their respective cluster centers\n\\[\\min_{C, \\{m_k\\}_{k=1}^K}\\sum_{k=1}^K \\sum_{C(i)=k} \\lVert x_i - m_k \\rVert^2\\]\nwhere second sum sums up over all elements \\(i\\) in cluster \\(k\\) and \\(\\mu_k\\) is the cluster center of cluster \\(k\\).\nThe K-means algorithm is sensitive to the initial choice of cluster centers. To mitigate this, one can run the algorithm multiple times with different initial guesses and choose the solution with the smallest objective function value.\nThe scale of the data can also have an impact on the clustering results. Therefore, it is often recommended to standardize the data before applying the K-means algorithm. Furthermore, the Euclidean distance is not well suited for binary or categorical data. Therefore, one should only use the K-means algorithm for continuous data.\nHow to choose the number of clusters \\(K\\)? One can use the so-called elbow method to find a suitable number of clusters. The elbow method plots the sum of squared distances (i.e., the objective function of K-means) for different \\(K\\). The idea is to choose the number of clusters at the “elbow” of the curve, i.e., the point where the curve starts to flatten out. Note that the curve starts to flatten out when adding more clusters does not significantly reduce the sum of squared distances anymore. This usually happens to be the case when the number of clusters exceeds the “true” number of clusters in the data. However, this is just a heuristic and it might not always be easy to identify the “elbow” in the curve.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: K-Means Clusters and Elbow Method\n\n\n\nFigure 5.2 shows an example of the K-means clustering algorithm applied to a dataset with 3 clusters. The left-hand side shows the clusters found by the K-means algorithm, while the right-hand side shows the elbow method to find the optimal number of clusters. The elbow method suggests that the optimal number of clusters is 3, which is the true number of clusters in the dataset.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Additional Methods</span>"
    ]
  },
  {
    "objectID": "additional.html#python-implementation",
    "href": "additional.html#python-implementation",
    "title": "5  Additional Methods",
    "section": "5.3 Python Implementation",
    "text": "5.3 Python Implementation\n\nLet’s have a look at how to implement KNN and K-means in Python. Again, we need to first import the required packages and load the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, recall_score, precision_score, roc_curve\npd.set_option('display.max_columns', 50) # Display up to 50 columns\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\nimport os.path\n\n# Check if the file exists\nif not os.path.isfile('data/card_transdata.csv'):\n\n    print('Downloading dataset...')\n\n    # Define the dataset to be downloaded\n    zipurl = 'https://www.kaggle.com/api/v1/datasets/download/dhanushnarayananr/credit-card-fraud'\n\n    # Download and unzip the dataset in the data folder\n    with urlopen(zipurl) as zipresp:\n        with ZipFile(BytesIO(zipresp.read())) as zfile:\n            zfile.extractall('data')\n\n    print('DONE!')\n\nelse:\n\n    print('Dataset already downloaded!')\n\n# Load the data\ndf = pd.read_csv('data/card_transdata.csv')\n\nDataset already downloaded!\n\n\nThis is the dataset of credit card transactions from Kaggle.com which we have used before. Recall that the target variable \\(y\\) is fraud, which indicates whether the transaction is fraudulent or not. The other variables are the features \\(x\\) of the transactions.\n\ndf.head(20)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n5\n5.586408\n13.261073\n0.064768\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n6\n3.724019\n0.956838\n0.278465\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n7\n4.848247\n0.320735\n1.273050\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n8\n0.876632\n2.503609\n1.516999\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n9\n8.839047\n2.970512\n2.361683\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n10\n14.263530\n0.158758\n1.136102\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n11\n13.592368\n0.240540\n1.370330\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n12\n765.282559\n0.371562\n0.551245\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n13\n2.131956\n56.372401\n6.358667\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n14\n13.955972\n0.271522\n2.798901\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n15\n179.665148\n0.120920\n0.535640\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\n16\n114.519789\n0.707003\n0.516990\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n17\n3.589649\n6.247458\n1.846451\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n18\n11.085152\n34.661351\n2.530758\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n19\n6.194671\n1.142014\n0.307217\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\ncount\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n1000000.000000\n\n\nmean\n26.628792\n5.036519\n1.824182\n0.881536\n0.350399\n0.100608\n0.650552\n0.087403\n\n\nstd\n65.390784\n25.843093\n2.799589\n0.323157\n0.477095\n0.300809\n0.476796\n0.282425\n\n\nmin\n0.004874\n0.000118\n0.004399\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.878008\n0.296671\n0.475673\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n9.967760\n0.998650\n0.997717\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n25.743985\n3.355748\n2.096370\n1.000000\n1.000000\n0.000000\n1.000000\n0.000000\n\n\nmax\n10632.723672\n11851.104565\n267.802942\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 8 columns):\n #   Column                          Non-Null Count    Dtype  \n---  ------                          --------------    -----  \n 0   distance_from_home              1000000 non-null  float64\n 1   distance_from_last_transaction  1000000 non-null  float64\n 2   ratio_to_median_purchase_price  1000000 non-null  float64\n 3   repeat_retailer                 1000000 non-null  float64\n 4   used_chip                       1000000 non-null  float64\n 5   used_pin_number                 1000000 non-null  float64\n 6   online_order                    1000000 non-null  float64\n 7   fraud                           1000000 non-null  float64\ndtypes: float64(8)\nmemory usage: 61.0 MB\n\n\n\n5.3.1 Data Preprocessing\nSince we have already explored the dataset in the previous notebook, we can skip that part and move directly to the data preprocessing.\nWe will again split the data into training and test sets using the train_test_split function\n\nX = df.drop('fraud', axis=1) # All variables except `fraud`\ny = df['fraud'] # Only our fraud variables\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3, random_state = 42)\n\nThen we can do the feature scaling to ensure our non-binary variables have mean zero and variance 1\n\ndef scale_features(scaler, df, col_names, only_transform=False):\n\n    # Extract the features we want to scale\n    features = df[col_names] \n\n    # Fit the scaler to the features and transform them\n    if only_transform:\n        features = scaler.transform(features.values)\n    else:\n        features = scaler.fit_transform(features.values)\n\n    # Replace the original features with the scaled features\n    df[col_names] = features\n\n\n# Define which features to scale with the StandardScaler and MinMaxScaler\nfor_standard_scaler = [\n    'distance_from_home', \n    'distance_from_last_transaction', \n    'ratio_to_median_purchase_price',\n]\n\n# Apply the standard scaler (Note: we use the same mean and std for scaling the test set)\nstandard_scaler = StandardScaler() \nscale_features(standard_scaler, X_train, for_standard_scaler)\nscale_features(standard_scaler, X_test, for_standard_scaler, only_transform=True)\n\n\n\n5.3.2 K-Nearest Neighbors (KNN)\nWe can now implement the KNN algorithm using the KNeighborsClassifier class from the sklearn.neighbors module. We will use the default value of \\(k=5\\) for the number of neighbors.\n\nclf_knn = KNeighborsClassifier().fit(X_train, y_train)\n\nWe can now use the trained model to make predictions on the test set and evaluate the model performance using the confusion matrix and accuracy score.\n\ny_pred_knn = clf_knn.predict(X_test)\ny_proba_knn = clf_knn.predict_proba(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_knn)}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_knn)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_knn)}\")\nprint(f\"ROC AUC: {roc_auc_score(y_test, y_proba_knn[:, 1])}\")\n\nAccuracy: 0.9987\nPrecision: 0.9935419771485345\nRecall: 0.991571641051066\nROC AUC: 0.9997341251520317\n\n\nThis seems to work quite well with a ROC AUC of 0.9997. We seem to have an almost perfect classifier. We can also plot the ROC curve to visualize the performance of the classifier\n\n# Compute the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba_knn[:, 1])\n\n# Plot the ROC curve\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], linestyle='--', color='grey')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('ROC Curve')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s also check the confusion matrix to see where we still make mistakes\n\nconf_mat = confusion_matrix(y_test, y_pred_knn, labels=[1, 0]).transpose() # Transpose the sklearn confusion matrix to match the convention in the lecture\nsns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=['Fraud', 'No Fraud'], yticklabels=['Fraud', 'No Fraud'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.3.3 K-Means\nThis is the first example of an unsupervised learning algorithm meaning that we will ignore the labels in the training set. We will use the KMeans class from the sklearn.cluster module to implement the K-means algorithm. Note that we can not use categorical variables in the K-means algorithm, so we will only use the continuous variables in this example. Furthermore, to simplify interpretability we will only use two variables\n\ncontinuous_variables = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']\nn_clusters=2\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(X_train[continuous_variables])\n\nWe can check the cluster centers using the cluster_centers_ attribute of the trained model\n\nkmeans.cluster_centers_\n\narray([[-2.01860525e-05, -1.55050548e-03, -1.68843633e-01],\n       [ 3.66287246e-04,  2.81347918e-02,  3.06376244e+00]])\n\n\nSince we only have two variables we can easily visualize the clusters using a scatter plot. We first need to unscale the data to make the plot more interpretable\n\n# Unscale the data\nX_train_unscaled = X_train.copy()\nX_train_unscaled[for_standard_scaler] = standard_scaler.inverse_transform(X_train[for_standard_scaler])\nX_test_unscaled = X_test.copy()\nX_test_unscaled[for_standard_scaler] = standard_scaler.inverse_transform(X_test[for_standard_scaler])\ncluster_centers = standard_scaler.inverse_transform(kmeans.cluster_centers_)\n\nThen, we can create the scatter plot to see what the clusters look like\n\n_, ax = plt.subplots()\nscatter = ax.scatter(X_train_unscaled[continuous_variables[0]], X_train_unscaled[continuous_variables[2]], c=kmeans.labels_)\nscatter = ax.scatter(cluster_centers[:, 0], cluster_centers[:, 2], c='black', marker='x', s=100, label = 'K-means Centers')\nax.set(xlabel=continuous_variables[0], ylabel=continuous_variables[2])\nax.set_xscale('log')\nax.set_yscale('log')\nax.legend()\nplt.title('K-Means Clusters')\nplt.show()\n\n\n\n\n\n\n\n\nNote that the centers might look a bit off because we are using log scales on the x and y-axis. In the other dimension, we don’t have such a nice separation of the clusters\n\n_, ax = plt.subplots()\nscatter = ax.scatter(X_train_unscaled[continuous_variables[0]], X_train_unscaled[continuous_variables[1]], c=kmeans.labels_)\nscatter = ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='black', marker='x', s=100, label = 'K-means Centers')\nax.set(xlabel=continuous_variables[0], ylabel=continuous_variables[1])\nax.set_xscale('log')\nax.set_yscale('log')\nax.legend()\nplt.title('K-Means Clusters')\nplt.show()\n\n\n\n\n\n\n\n\nBut what do these two clusters represent? We can check the mean of the target variable fraud for each cluster to get an idea of what the clusters represent\n\nX_train_unscaled['cluster'] = kmeans.labels_\nX_train_unscaled.query('cluster == 1').describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ndistance_from_home\n36679.0\n26.727628\n63.910540\n0.032026\n3.895800\n10.098703\n25.760347\n3353.002414\n\n\ndistance_from_last_transaction\n36679.0\n5.780037\n71.723799\n0.000966\n0.296198\n1.000376\n3.357238\n11851.104565\n\n\nratio_to_median_purchase_price\n36679.0\n10.470287\n6.811775\n2.209891\n6.871869\n8.384989\n11.466176\n267.802942\n\n\nrepeat_retailer\n36679.0\n0.879522\n0.325524\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nused_chip\n36679.0\n0.351754\n0.477524\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nused_pin_number\n36679.0\n0.102756\n0.303645\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nonline_order\n36679.0\n0.649063\n0.477270\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\ncluster\n36679.0\n1.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\nX_train_unscaled.query('cluster == 0').describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ndistance_from_home\n663321.0\n26.694233\n66.097113\n0.004874\n3.880252\n9.969293\n25.807909\n10632.723672\n\n\ndistance_from_last_transaction\n663321.0\n4.988030\n22.054240\n0.000118\n0.296681\n0.998050\n3.351187\n3437.278746\n\n\nratio_to_median_purchase_price\n663321.0\n1.347721\n1.226094\n0.004399\n0.455014\n0.929070\n1.838841\n5.921543\n\n\nrepeat_retailer\n663321.0\n0.881468\n0.323238\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nused_chip\n663321.0\n0.350518\n0.477133\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nused_pin_number\n663321.0\n0.100512\n0.300682\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nonline_order\n663321.0\n0.650643\n0.476767\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\ncluster\n663321.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\ny_train[X_train_unscaled['cluster'] == 0].mean()\n\n0.057474435454327545\n\n\n\ny_train[X_train_unscaled['cluster'] == 1].mean()\n\n0.6286430927778838\n\n\nThere does not seem to be a clear difference between the two clusters except for the difference in the mean of the ratio_to_median_purchase_price variable. This is not necessarily very surprising since we only used three variables in the clustering algorithm. However, due to the correlation of ratio_to_median_purchase_price we have more fraudulent transactions in one cluster than the other. To be able to carry out a more meaningful clustering analysis using K-means we would need a different dataset with more quantitative variables. Nevertheless, let’s also check the elbow method to how many clusters it would suggest\n\ninterias = [KMeans(n_clusters=n, n_init=10).fit(X_train).inertia_ for n in range(1, 11)]\n_, ax = plt.subplots()\nax.plot(range(1, 11), interias, marker='o')\nax.set(xlabel='Number of clusters', ylabel='Objective Function')\nplt.title('Elbow Method')\nplt.show()\n\n\n\n\n\n\n\n\nThere does not seem to be a clear elbow in the plot. Finally, we can also make predictions on the test set using the trained K-means model\n\nkmeans.predict(X_test[continuous_variables])\n\narray([0, 0, 0, ..., 0, 0, 1], dtype=int32)\n\n\nThis assigns each observation in the test set to one of the two clusters.\n\n\n5.3.4 Conclusions\nWe have seen how to implement a KNN algorithm for classification and a K-means algorithm for clustering in Python using the sklearn package. We have also seen how to evaluate the performance of the KNN algorithm using the confusion matrix, accuracy score, precision, recall, and ROC AUC. We have also seen how to visualize the clusters created by the K-means algorithm and tried to apply the ellow method.",
    "crumbs": [
      "Overview and Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Additional Methods</span>"
    ]
  },
  {
    "objectID": "applicationI.html#problem-setup",
    "href": "applicationI.html#problem-setup",
    "title": "6  Loan Default Prediction",
    "section": "6.1 Problem Setup",
    "text": "6.1 Problem Setup\nThe dataset that we will be using was used in the Kaggle competition “Give Me Some Credit”. The description of the competition reads as follows:\n\nBanks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit.\nCredit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.\nThe goal of this competition is to build a model that borrowers can use to help make the best financial decisions.\nHistorical data are provided on 250,000 borrowers and the prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third).\n\nUnfortunately, there won’t be any prize money today. However, the experience that you can gain from working through an application like this can be invaluable. So, in a way, you are still winning!",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#dataset",
    "href": "applicationI.html#dataset",
    "title": "6  Loan Default Prediction",
    "section": "6.2 Dataset",
    "text": "6.2 Dataset\nLet’s download the dataset automatically, unzip it, and place it in a folder called data if you haven’t done so already\n\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\nimport os.path\n\n# Check if the file exists\nif not os.path.isfile('data/Data Dictionary.xl') or not os.path.isfile('data/cs-training.csv'):\n\n    print('Downloading dataset...')\n\n    # Define the dataset to be downloaded\n    zipurl = 'https://www.kaggle.com/api/v1/datasets/download/brycecf/give-me-some-credit-dataset'\n\n    # Download and unzip the dataset in the data folder\n    with urlopen(zipurl) as zipresp:\n        with ZipFile(BytesIO(zipresp.read())) as zfile:\n            zfile.extractall('data')\n\n    print('DONE!')\n\nelse:\n\n    print('Dataset already downloaded!')\n\nDownloading dataset...\nDONE!\n\n\nThen, we can have a look at the data dictionary that is provided with the dataset. This will give us an idea of the variables that are available in the dataset and what they represent\n\nimport pandas as pd\ndata_dict = pd.read_excel('data/Data Dictionary.xls', header=1)\ndata_dict.style.hide()\n\n\n\n\n\n\n\nVariable Name\nDescription\nType\n\n\n\n\nSeriousDlqin2yrs\nPerson experienced 90 days past due delinquency or worse\nY/N\n\n\nRevolvingUtilizationOfUnsecuredLines\nTotal balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits\npercentage\n\n\nage\nAge of borrower in years\ninteger\n\n\nNumberOfTime30-59DaysPastDueNotWorse\nNumber of times borrower has been 30-59 days past due but no worse in the last 2 years.\ninteger\n\n\nDebtRatio\nMonthly debt payments, alimony,living costs divided by monthy gross income\npercentage\n\n\nMonthlyIncome\nMonthly income\nreal\n\n\nNumberOfOpenCreditLinesAndLoans\nNumber of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)\ninteger\n\n\nNumberOfTimes90DaysLate\nNumber of times borrower has been 90 days or more past due.\ninteger\n\n\nNumberRealEstateLoansOrLines\nNumber of mortgage and real estate loans including home equity lines of credit\ninteger\n\n\nNumberOfTime60-89DaysPastDueNotWorse\nNumber of times borrower has been 60-89 days past due but no worse in the last 2 years.\ninteger\n\n\nNumberOfDependents\nNumber of dependents in family excluding themselves (spouse, children etc.)\ninteger\n\n\n\n\n\n\nThe variable \\(y\\) that we want to predict is SeriousDlqin2yrs which indicates whether a person has been 90 days past due on a loan payment (serious delinquency) in the past two years. This target variable is \\(1\\) if the loan defaults (i.e., serious delinquency occured) and \\(0\\) if the loan does not default (i.e., no serious delinquency occured) . The other variables are features that we can use to predict this target variable such as the age of the borrower and the monthly income of the borrower.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#putting-the-problem-into-the-context-of-the-course",
    "href": "applicationI.html#putting-the-problem-into-the-context-of-the-course",
    "title": "6  Loan Default Prediction",
    "section": "6.3 Putting the Problem into the Context of the Course",
    "text": "6.3 Putting the Problem into the Context of the Course\nGiven the description of the competition and the dataset, we can see that this is a supervised learning problem. We have a target variable that we want to predict, and we have features that we can use to predict this target variable. The target variable is binary, i.e., it can take two values: 0 or 1. The value 0 indicates that the loan will not default, while the value 1 indicates that the loan will default. Thus, this is a binary classification problem.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#setting-up-the-environment",
    "href": "applicationI.html#setting-up-the-environment",
    "title": "6  Loan Default Prediction",
    "section": "6.4 Setting up the Environment",
    "text": "6.4 Setting up the Environment\nWe will start by setting up the environment by importing the necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nand loading the dataset\n\ndf = pd.read_csv('data/cs-training.csv')\n\nLet’s also download some precomputed models that we will use later on\n\nfor file_name in ['clf_nn.joblib', 'clf_nn2.joblib']:\n\n    if not os.path.isfile(file_name):\n\n        print(f'Downloading {file_name}...')\n\n        # Generate the download link\n        url = f'https://github.com/jmarbet/data-science-course/raw/main/notebooks/{file_name}'\n\n        # Download the file\n        with urlopen(url) as response, open(file_name, 'wb') as out_file:\n            data = response.read()\n            out_file.write(data)\n\n        print('DONE!')\n\n    else:\n\n        print(f'{file_name} already downloaded!')\n\nclf_nn.joblib already downloaded!\nclf_nn2.joblib already downloaded!",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#data-preprocessing",
    "href": "applicationI.html#data-preprocessing",
    "title": "6  Loan Default Prediction",
    "section": "6.5 Data Preprocessing",
    "text": "6.5 Data Preprocessing\nThe dataset is now loaded into a pandas DataFrame. Let’s have a look at the first few rows of the dataset to get an idea of what the data looks like.\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nSeriousDlqin2yrs\nRevolvingUtilizationOfUnsecuredLines\nage\nNumberOfTime30-59DaysPastDueNotWorse\nDebtRatio\nMonthlyIncome\nNumberOfOpenCreditLinesAndLoans\nNumberOfTimes90DaysLate\nNumberRealEstateLoansOrLines\nNumberOfTime60-89DaysPastDueNotWorse\nNumberOfDependents\n\n\n\n\n0\n1\n1\n0.766127\n45\n2\n0.802982\n9120.0\n13\n0\n6\n0\n2.0\n\n\n1\n2\n0\n0.957151\n40\n0\n0.121876\n2600.0\n4\n0\n0\n0\n1.0\n\n\n2\n3\n0\n0.658180\n38\n1\n0.085113\n3042.0\n2\n1\n0\n0\n0.0\n\n\n3\n4\n0\n0.233810\n30\n0\n0.036050\n3300.0\n5\n0\n0\n0\n0.0\n\n\n4\n5\n0\n0.907239\n49\n1\n0.024926\n63588.0\n7\n0\n1\n0\n0.0\n\n\n\n\n\n\n\n\nThe column Unnamed: 0 seems to be a superfluous index column that we could drop. Let’s do that\n\ndf = df.drop('Unnamed: 0', axis=1)\n\nFurthermore, the order of the column names in the dataset is not very intuitive. Let’s reorder the columns in the dataset\n\norderedList = [\n    'SeriousDlqin2yrs',\n    'age', \n    'NumberOfDependents',\n    'MonthlyIncome', \n    'DebtRatio',\n    'RevolvingUtilizationOfUnsecuredLines',\n    'NumberOfOpenCreditLinesAndLoans',\n    'NumberRealEstateLoansOrLines',\n    'NumberOfTime30-59DaysPastDueNotWorse', \n    'NumberOfTime60-89DaysPastDueNotWorse', \n    'NumberOfTimes90DaysLate'\n]\n\ndf = df.loc[:, orderedList]\n\nLet’s also have a look at the data types of the columns in the dataset and whether there are any missing values\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150000 entries, 0 to 149999\nData columns (total 11 columns):\n #   Column                                Non-Null Count   Dtype  \n---  ------                                --------------   -----  \n 0   SeriousDlqin2yrs                      150000 non-null  int64  \n 1   age                                   150000 non-null  int64  \n 2   NumberOfDependents                    146076 non-null  float64\n 3   MonthlyIncome                         120269 non-null  float64\n 4   DebtRatio                             150000 non-null  float64\n 5   RevolvingUtilizationOfUnsecuredLines  150000 non-null  float64\n 6   NumberOfOpenCreditLinesAndLoans       150000 non-null  int64  \n 7   NumberRealEstateLoansOrLines          150000 non-null  int64  \n 8   NumberOfTime30-59DaysPastDueNotWorse  150000 non-null  int64  \n 9   NumberOfTime60-89DaysPastDueNotWorse  150000 non-null  int64  \n 10  NumberOfTimes90DaysLate               150000 non-null  int64  \ndtypes: float64(4), int64(7)\nmemory usage: 12.6 MB\n\n\nNote that the column MonthlyIncome and NumberOfDependents seem to have missing values. Before we drop these missing values or impute them, let’s have a look at the distribution of our target variable SeriousDlqin2yrs\n\ndf['SeriousDlqin2yrs'].value_counts(normalize=True)\n\nSeriousDlqin2yrs\n0    0.93316\n1    0.06684\nName: proportion, dtype: float64\n\n\nAs with the example that we have seen during one of our previous lectures, the dataset seems to be quite imbalanced. Only about 6.7% of the loans have defaulted. This is something that we need to keep in mind when treating the missing values and when building our models.\nLet’s see what happens to the distribution of the target variable if we drop the missing values\n\ndf.dropna().value_counts(\"SeriousDlqin2yrs\", normalize=True)\n\nSeriousDlqin2yrs\n0    0.930514\n1    0.069486\nName: proportion, dtype: float64\n\n\nIt seems to have almost no impact on the distribution of the target variable. This is good news. Let’s compare some other statistics of the dataset before and after dropping the missing values\n\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSeriousDlqin2yrs\n150000.0\n0.066840\n0.249746\n0.0\n0.000000\n0.000000\n0.000000\n1.0\n\n\nage\n150000.0\n52.295207\n14.771866\n0.0\n41.000000\n52.000000\n63.000000\n109.0\n\n\nNumberOfDependents\n146076.0\n0.757222\n1.115086\n0.0\n0.000000\n0.000000\n1.000000\n20.0\n\n\nMonthlyIncome\n120269.0\n6670.221237\n14384.674215\n0.0\n3400.000000\n5400.000000\n8249.000000\n3008750.0\n\n\nDebtRatio\n150000.0\n353.005076\n2037.818523\n0.0\n0.175074\n0.366508\n0.868254\n329664.0\n\n\nRevolvingUtilizationOfUnsecuredLines\n150000.0\n6.048438\n249.755371\n0.0\n0.029867\n0.154181\n0.559046\n50708.0\n\n\nNumberOfOpenCreditLinesAndLoans\n150000.0\n8.452760\n5.145951\n0.0\n5.000000\n8.000000\n11.000000\n58.0\n\n\nNumberRealEstateLoansOrLines\n150000.0\n1.018240\n1.129771\n0.0\n0.000000\n1.000000\n2.000000\n54.0\n\n\nNumberOfTime30-59DaysPastDueNotWorse\n150000.0\n0.421033\n4.192781\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTime60-89DaysPastDueNotWorse\n150000.0\n0.240387\n4.155179\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTimes90DaysLate\n150000.0\n0.265973\n4.169304\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\n\n\n\n\n\n\n\ndf.dropna().describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSeriousDlqin2yrs\n120269.0\n0.069486\n0.254280\n0.0\n0.000000\n0.000000\n0.000000\n1.0\n\n\nage\n120269.0\n51.289792\n14.426684\n0.0\n40.000000\n51.000000\n61.000000\n103.0\n\n\nNumberOfDependents\n120269.0\n0.851832\n1.148391\n0.0\n0.000000\n0.000000\n2.000000\n20.0\n\n\nMonthlyIncome\n120269.0\n6670.221237\n14384.674215\n0.0\n3400.000000\n5400.000000\n8249.000000\n3008750.0\n\n\nDebtRatio\n120269.0\n26.598777\n424.446457\n0.0\n0.143388\n0.296023\n0.482559\n61106.5\n\n\nRevolvingUtilizationOfUnsecuredLines\n120269.0\n5.899873\n257.040685\n0.0\n0.035084\n0.177282\n0.579428\n50708.0\n\n\nNumberOfOpenCreditLinesAndLoans\n120269.0\n8.758475\n5.172835\n0.0\n5.000000\n8.000000\n11.000000\n58.0\n\n\nNumberRealEstateLoansOrLines\n120269.0\n1.054519\n1.149273\n0.0\n0.000000\n1.000000\n2.000000\n54.0\n\n\nNumberOfTime30-59DaysPastDueNotWorse\n120269.0\n0.381769\n3.499234\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTime60-89DaysPastDueNotWorse\n120269.0\n0.187829\n3.447901\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTimes90DaysLate\n120269.0\n0.211925\n3.465276\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\n\n\n\n\n\n\nIt looks like the statistics before and after dropping the missing values are quite similar, except for the variable DebtRatio, where we have substantially lower means and standard deviation. Let’s also have a look at the distribution of the variables for the rows that we have dropped\n\ndf.loc[df.isna().any(axis=1)].describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSeriousDlqin2yrs\n29731.0\n0.056137\n0.230189\n0.0\n0.000000\n0.000000\n0.000000\n1.0\n\n\nage\n29731.0\n56.362349\n15.438786\n21.0\n46.000000\n57.000000\n67.000000\n109.0\n\n\nNumberOfDependents\n25807.0\n0.316310\n0.809944\n0.0\n0.000000\n0.000000\n0.000000\n9.0\n\n\nMonthlyIncome\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nDebtRatio\n29731.0\n1673.396556\n4248.372895\n0.0\n123.000000\n1159.000000\n2382.000000\n329664.0\n\n\nRevolvingUtilizationOfUnsecuredLines\n29731.0\n6.649421\n217.814854\n0.0\n0.016027\n0.081697\n0.440549\n22198.0\n\n\nNumberOfOpenCreditLinesAndLoans\n29731.0\n7.216071\n4.842720\n0.0\n4.000000\n6.000000\n10.000000\n45.0\n\n\nNumberRealEstateLoansOrLines\n29731.0\n0.871481\n1.034291\n0.0\n0.000000\n1.000000\n1.000000\n23.0\n\n\nNumberOfTime30-59DaysPastDueNotWorse\n29731.0\n0.579866\n6.255361\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTime60-89DaysPastDueNotWorse\n29731.0\n0.452995\n6.242076\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTimes90DaysLate\n29731.0\n0.484612\n6.250408\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\n\n\n\n\n\n\nAgain, the mean of the dropped rows seems to be substantially higher for the variable DebtRatio suggesting that the missing values are not missing entirely at random. Note, however, that the standard deviation is lower meaning that the dropped observations are more similar to each other in the DebtRatio dimension. From our data dictionary, we know that the DebtRatio is defined as\n\ndata_dict.loc[data_dict['Variable Name'] == 'DebtRatio'].style.hide()\n\n\n\n\n\n\n\nVariable Name\nDescription\nType\n\n\n\n\nDebtRatio\nMonthly debt payments, alimony,living costs divided by monthy gross income\npercentage\n\n\n\n\n\n\nSo, it seems that the DebtRatio is the ratio of the monthly debt payments to the monthly gross income. We actually have MonthlyIncome in our dataset!\n\ndata_dict.loc[data_dict['Variable Name'] == 'MonthlyIncome'].style.hide()\n\n\n\n\n\n\n\nVariable Name\nDescription\nType\n\n\n\n\nMonthlyIncome\nMonthly income\nreal\n\n\n\n\n\n\nIt does not say whether this is in gross or net terms though. Nevertheless, let’s have a look at the relationship between the DebtRatio and the MonthlyIncome\n\nax = df.plot.scatter(x='MonthlyIncome', y='DebtRatio')\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlabel('MonthlyIncome')\nax.set_ylabel('DebtRatio')\nax.set_title('DebtRatio vs. MonthlyIncome')\nplt.show()\n\n\n\n\n\n\n\n\nThis looks rather odd. Note how there are a lot of monthly incomes that are close to zero. Furthermore, there is a weird gap going through the scatter points. We can look at the descriptive statistics of the rows with MonthlyIncome less than 100\n\ndf.query('MonthlyIncome &lt;= 100').describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSeriousDlqin2yrs\n2301.0\n0.036506\n0.187586\n0.0\n0.000000\n0.000000\n0.000000\n1.0\n\n\nage\n2301.0\n47.740113\n16.199176\n21.0\n35.000000\n46.000000\n60.000000\n103.0\n\n\nNumberOfDependents\n2301.0\n0.778792\n1.192441\n0.0\n0.000000\n0.000000\n2.000000\n10.0\n\n\nMonthlyIncome\n2301.0\n1.837027\n11.408271\n0.0\n0.000000\n0.000000\n1.000000\n100.0\n\n\nDebtRatio\n2301.0\n1370.529300\n2752.843610\n0.0\n79.000000\n732.000000\n1850.500000\n61106.5\n\n\nRevolvingUtilizationOfUnsecuredLines\n2301.0\n3.604101\n125.553453\n0.0\n0.022243\n0.113149\n0.486629\n5893.0\n\n\nNumberOfOpenCreditLinesAndLoans\n2301.0\n7.171230\n4.869628\n0.0\n4.000000\n6.000000\n10.000000\n31.0\n\n\nNumberRealEstateLoansOrLines\n2301.0\n0.742721\n0.904984\n0.0\n0.000000\n1.000000\n1.000000\n9.0\n\n\nNumberOfTime30-59DaysPastDueNotWorse\n2301.0\n0.549326\n6.132226\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTime60-89DaysPastDueNotWorse\n2301.0\n0.428509\n6.124274\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\nNumberOfTimes90DaysLate\n2301.0\n0.438505\n6.128357\n0.0\n0.000000\n0.000000\n0.000000\n98.0\n\n\n\n\n\n\n\n\nThese observations seem to have a higher debtRatio than the rest of the dataset but are less likely to default on their loans (the mean of SeriousDlqin2yrs is equal to the fraction of defaulting loans). Given that they have no income (or essentially no income), this seems rather odd and is likely due to an error during data entry/collection. Since there are only a small number of observations with MonthlyIncome less than 100, we can probably drop them. Let’s look at the same figure for MonthlyIncome greater than 100\n\nax = df.query('MonthlyIncome &gt; 100').plot.scatter(x='MonthlyIncome', y='DebtRatio')\nax.set_xscale('log')\nax.set_yscale('log')\nax.axvline(10**3, color='black', linestyle=':')\nax.axhline(10**(-3), color='black', linestyle=':')\nax.plot([10, 10**5], [10**(-1), 10**(-5)], color='black', linestyle='--')\nax.set_xlabel('MonthlyIncome')\nax.set_ylabel('DebtRatio')\nax.set_title('DebtRatio vs. MonthlyIncome')\nplt.show()\n\n\n\n\n\n\n\n\nThis looks better but note how the scatter points below the gap seem to line up with the line \\(\\frac{1}{\\text{MonthlyIncome}}\\). Thus, there seems to be another potential data entry/collection error since the debt in the raw data has likely been just set to \\(1\\) for these observations. If this was a real dataset, we would need to investigate this further and maybe talk to the people who have sent us the data. However, given that this is just an example, we leave it as is.\nLet’s also have a look at the distribution of DebtRatio variable\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\ndf['DebtRatio'].plot.hist(bins=1000, ax=ax[0])\ndf.dropna()['DebtRatio'].plot.hist(bins=1000, ax=ax[1])\ndf.loc[df.isna().any(axis=1), 'DebtRatio'].plot.hist(bins=1000, ax=ax[2])\n\nax[0].set_xscale('log')\nax[1].set_xscale('log')\nax[2].set_xscale('log')\n\nax[0].set_title('DebtRatio (Whole Dataset)')\nax[1].set_title('DebtRatio (Rows with Missing Values Dropped)')\nax[2].set_title('DebtRatio (Dropped Rows)')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhere we can again see the high DebtRatio values for the rows with missing values. We can also have a look at the distribution of all the variables in the dataset\n\nfig, ax = plt.subplots(df.shape[1], 3, figsize=(10, 20))\n\nfor ii, col in enumerate(df.columns):\n\n    # Plot the distribution of the variable for the whole dataset, the dataset with missing values dropped, and the dropped rows\n    if col in ('SeriousDlqin2yrs', 'age', 'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents'): \n\n        # Use a bar plot for discrete variables\n        df[col].value_counts(normalize=True).sort_index().plot.bar(ax=ax[ii,0])\n        df.dropna()[col].value_counts(normalize=True).sort_index().plot.bar(ax=ax[ii,1])\n        df.loc[df.isna().any(axis=1), col].value_counts(normalize=True).sort_index().plot.bar(ax=ax[ii,2])\n\n        # Set the y-axis label\n        ax[ii,0].set_ylabel('Fraction')\n        ax[ii,1].set_ylabel('')\n        ax[ii,2].set_ylabel('')\n\n    else:\n\n        # Use a histogram for continuous variables\n        df[col].plot.hist(bins=1000, ax=ax[ii,0])\n        df.dropna()[col].plot.hist(bins=1000, ax=ax[ii,1])\n        df.loc[df.isna().any(axis=1), col].plot.hist(bins=1000, ax=ax[ii,2])\n\n        # Set the x-axis to a logarithmic scale for the continuous variables\n        ax[ii,0].set_xscale('log')\n        ax[ii,1].set_xscale('log')\n        ax[ii,2].set_xscale('log')\n\n        # Set the x-axis label\n        ax[ii,0].set_xlabel(col)\n        ax[ii,1].set_xlabel(col)\n        ax[ii,2].set_xlabel(col)\n\n        # Set the y-axis label\n        ax[ii,0].set_ylabel('Frequency')\n        ax[ii,1].set_ylabel('')\n        ax[ii,2].set_ylabel('')\n\nax[0,0].set_title('Whole Dataset')\nax[0,1].set_title('Rows with Missing Values Dropped')\nax[0,2].set_title('Dropped Rows')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis shows another potential issue with our dataset. Checkout the variable NumberOfTime30-59DaysPastDueNotWorse. It seems that there are a some observations with values greater than 90. This seems rather odd. Let’s have a look at the data dictionary\n\ndata_dict.loc[data_dict['Variable Name'].isin(['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate'])].style.hide()\n\n\n\n\n\n\n\nVariable Name\nDescription\nType\n\n\n\n\nNumberOfTime30-59DaysPastDueNotWorse\nNumber of times borrower has been 30-59 days past due but no worse in the last 2 years.\ninteger\n\n\nNumberOfTimes90DaysLate\nNumber of times borrower has been 90 days or more past due.\ninteger\n\n\nNumberOfTime60-89DaysPastDueNotWorse\nNumber of times borrower has been 60-89 days past due but no worse in the last 2 years.\ninteger\n\n\n\n\n\n\nThe data dictionary does not mention anything about values above 90. These values may have a special meaning such as being a flag for missing values. Let’s have a look at the distribution of the target variable for the rows with values greater than 90\n\ndf.loc[df['NumberOfTime30-59DaysPastDueNotWorse'] &gt; 90, 'SeriousDlqin2yrs'].value_counts()\n\nSeriousDlqin2yrs\n1    147\n0    122\nName: count, dtype: int64\n\n\n\ndf.loc[df['NumberOfTime60-89DaysPastDueNotWorse'] &gt; 90, 'SeriousDlqin2yrs'].value_counts()\n\nSeriousDlqin2yrs\n1    147\n0    122\nName: count, dtype: int64\n\n\n\ndf.loc[df['NumberOfTimes90DaysLate'] &gt; 90, 'SeriousDlqin2yrs'].value_counts()\n\nSeriousDlqin2yrs\n1    147\n0    122\nName: count, dtype: int64\n\n\n\ndf.loc[(df['NumberOfTimes90DaysLate'] &gt; 90) & (df['NumberOfTime60-89DaysPastDueNotWorse'] &gt; 90) & (df['NumberOfTime30-59DaysPastDueNotWorse'] &gt; 90), 'SeriousDlqin2yrs'].value_counts()\n\nSeriousDlqin2yrs\n1    147\n0    122\nName: count, dtype: int64\n\n\nThere seems to be a very high number of defaults for these observations (more than half), which makes sense given the meaning of these variables. Furthermore, the observations with above 90 in one category have it above 90 in the other categories as well. Thus, this might not be a data entry/collection error and these are just borrowers who commonly fail to make loan payments.\nGiven that Alonso Robisco and Carbó Martínez (2022) seem to be dropping the missing values, let’s do the same for our dataset\n\ndf = df.dropna()\n\nand let’s also drop the rows with MonthlyIncome less than (or equal) 100\n\ndf = df.query('MonthlyIncome &gt; 100')\n\nto eliminate some of the potential data entry/collection errors.\nThen double-check that we have no missing values left\n\ndf.isna().sum()\n\nSeriousDlqin2yrs                        0\nage                                     0\nNumberOfDependents                      0\nMonthlyIncome                           0\nDebtRatio                               0\nRevolvingUtilizationOfUnsecuredLines    0\nNumberOfOpenCreditLinesAndLoans         0\nNumberRealEstateLoansOrLines            0\nNumberOfTime30-59DaysPastDueNotWorse    0\nNumberOfTime60-89DaysPastDueNotWorse    0\nNumberOfTimes90DaysLate                 0\ndtype: int64\n\n\nor, alternatively,\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 117968 entries, 0 to 149999\nData columns (total 11 columns):\n #   Column                                Non-Null Count   Dtype  \n---  ------                                --------------   -----  \n 0   SeriousDlqin2yrs                      117968 non-null  int64  \n 1   age                                   117968 non-null  int64  \n 2   NumberOfDependents                    117968 non-null  float64\n 3   MonthlyIncome                         117968 non-null  float64\n 4   DebtRatio                             117968 non-null  float64\n 5   RevolvingUtilizationOfUnsecuredLines  117968 non-null  float64\n 6   NumberOfOpenCreditLinesAndLoans       117968 non-null  int64  \n 7   NumberRealEstateLoansOrLines          117968 non-null  int64  \n 8   NumberOfTime30-59DaysPastDueNotWorse  117968 non-null  int64  \n 9   NumberOfTime60-89DaysPastDueNotWorse  117968 non-null  int64  \n 10  NumberOfTimes90DaysLate               117968 non-null  int64  \ndtypes: float64(4), int64(7)\nmemory usage: 10.8 MB\n\n\nAll good! We should also check for duplicated rows with the duplicated() method\n\ndf.loc[df.duplicated()]\n\n\n\n\n\n\n\n\n\nSeriousDlqin2yrs\nage\nNumberOfDependents\nMonthlyIncome\nDebtRatio\nRevolvingUtilizationOfUnsecuredLines\nNumberOfOpenCreditLinesAndLoans\nNumberRealEstateLoansOrLines\nNumberOfTime30-59DaysPastDueNotWorse\nNumberOfTime60-89DaysPastDueNotWorse\nNumberOfTimes90DaysLate\n\n\n\n\n7920\n0\n22\n0.0\n820.0\n0.0\n1.0\n1\n0\n0\n0\n0\n\n\n8840\n0\n23\n0.0\n820.0\n0.0\n1.0\n1\n0\n0\n0\n0\n\n\n15546\n0\n22\n0.0\n929.0\n0.0\n0.0\n2\n0\n0\n0\n0\n\n\n17265\n0\n22\n0.0\n820.0\n0.0\n1.0\n1\n0\n0\n0\n0\n\n\n21190\n0\n22\n0.0\n820.0\n0.0\n1.0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n143750\n0\n23\n0.0\n820.0\n0.0\n1.0\n1\n0\n0\n0\n0\n\n\n144153\n0\n28\n0.0\n2200.0\n0.0\n1.0\n0\n0\n0\n0\n0\n\n\n144922\n0\n40\n0.0\n3500.0\n0.0\n0.0\n1\n0\n0\n0\n0\n\n\n148419\n0\n22\n0.0\n1500.0\n0.0\n0.0\n2\n0\n0\n0\n0\n\n\n149993\n0\n22\n0.0\n820.0\n0.0\n1.0\n1\n0\n0\n0\n0\n\n\n\n\n72 rows × 11 columns\n\n\n\n\nand look at the statistics of the duplicated rows\n\ndf.loc[df.duplicated()].describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSeriousDlqin2yrs\n72.0\n0.013889\n0.117851\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\nage\n72.0\n24.902778\n8.868618\n21.0\n22.0\n22.5\n24.0\n70.000000\n\n\nNumberOfDependents\n72.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\nMonthlyIncome\n72.0\n1031.527778\n542.007873\n764.0\n820.0\n820.0\n929.0\n3500.000000\n\n\nDebtRatio\n72.0\n0.017594\n0.104816\n0.0\n0.0\n0.0\n0.0\n0.633374\n\n\nRevolvingUtilizationOfUnsecuredLines\n72.0\n0.500000\n0.503509\n0.0\n0.0\n0.5\n1.0\n1.000000\n\n\nNumberOfOpenCreditLinesAndLoans\n72.0\n1.458333\n0.749413\n0.0\n1.0\n1.0\n2.0\n4.000000\n\n\nNumberRealEstateLoansOrLines\n72.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\nNumberOfTime30-59DaysPastDueNotWorse\n72.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\nNumberOfTime60-89DaysPastDueNotWorse\n72.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\nNumberOfTimes90DaysLate\n72.0\n0.013889\n0.117851\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\n\n\n\n\n\n\nThere are indeed 72 duplicated rows in the dataset. However, given the variables in our dataset, which are mostly discrete, the fact that monthly income seems to be generally rounded, it does not seem implausible that some rows might appear multiple times in the dataset, simply because some observations have the same values for all variables. Thus, we will keep the duplicated rows in the dataset.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#data-exploration",
    "href": "applicationI.html#data-exploration",
    "title": "6  Loan Default Prediction",
    "section": "6.6 Data Exploration",
    "text": "6.6 Data Exploration\nLet’s start by looking at the distribution of the target variable SeriousDlqin2yrs in our preprocessed dataset\n\ndf.value_counts(\"SeriousDlqin2yrs\").plot.pie(autopct = \"%.1f\")\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nWe have already looked at some variables selectively. To do it more broadly, we can look at the pair plot of the dataset. A pair plot shows the pairwise relationships between the variables in our dataset. On the diagonal, we are plotting the kernel density estimate\n\nsns.pairplot(df.sample(1000), hue='SeriousDlqin2yrs')\n\n\n\n\n\n\n\n\nNote that we are plotting all variables in different colors based on whether our target variable SeriousDlqin2yrs is \\(0\\) or \\(1\\). Furthermore, since it is computationally quite demanding to create this plot, we have sampled only 1000 rows from the dataset. Since we have many variables, some of them with very skewed distributions, and also several discrete variables, it might make sense to look only at a subset\n\npp = sns.pairplot(df[['age', 'MonthlyIncome', 'DebtRatio', 'SeriousDlqin2yrs']], hue='SeriousDlqin2yrs')\n\n# Fix the x-axis and y-axis scales\nfor ax in pp.axes.flat:\n    if ax.get_xlabel() == 'MonthlyIncome':\n        ax.set(xscale=\"log\")\n        ax.set_xlim(10**1, 10**7)\n    if ax.get_ylabel() == 'MonthlyIncome':\n        ax.set(yscale=\"log\")\n        ax.set_ylim(10**1, 10**7)\n\n    if ax.get_xlabel() == 'DebtRatio':\n        ax.set(xscale=\"log\")\n        ax.set_xlim(10**(-5), 10**3)\n    if ax.get_ylabel() == 'DebtRatio':\n        ax.set(yscale=\"log\")\n        ax.set_ylim(10**(-5), 10**3)\n\n\n\n\n\n\n\n\nWe can continue with the analysis of our dataset by looking at the correlation matrix of the variables in the dataset. We will calculate both the Pearson correlation (linear relationship) and the Spearman correlation (monotonic relationship) and create a heatmap of both correlation matrices\n\ncorr = df.corr() # Calculate the Pearson correlation (linear relationship)\ncmap = sns.diverging_palette(10, 255, as_cmap=True) # Create a color map\nmask = np.triu(np.ones_like(corr, dtype=bool)) # Create a mask to only show the lower triangle of the matrix\nsns.heatmap(corr, cmap=cmap, vmax=1, center=0, mask=mask) # Create a heatmap of the correlation matrix (Note: vmax=1 makes sure that the color map goes up to 1 and center=0 are used to center the color map at 0)\nplt.show()\n\n\n\n\n\n\n\n\n\ncorr = df.corr('spearman') # Calculate the Spearman correlation (monotonic relationship)\ncmap = sns.diverging_palette(10, 255, as_cmap=True) # Create a color map\nmask = np.triu(np.ones_like(corr, dtype=bool)) # Create a mask to only show the lower triangle of the matrix\nsns.heatmap(corr, cmap=cmap, vmax=1, center=0, mask=mask) # Create a heatmap of the correlation matrix (Note: vmax=1 makes sure that the color map goes up to 1 and center=0 are used to center the color map at 0)\nplt.show()\n\n\n\n\n\n\n\n\nIt seems that age is negatively correlated with default (SeriousDlqin2yrs) which we can also see in the kernel density estimate of the age variable\n\nsns.kdeplot(data=df, x='age', hue='SeriousDlqin2yrs', cut=0, fill=True, common_norm=False)\nplt.show()\n\n\n\n\n\n\n\n\nbut then MonthlyIncome is also negatively correlated with default and with age. Thus, likely the relationship between age and default is driven by MonthlyIncome.\nFurthermore, the variables NumberOfTime30-59DaysPastDueNotWorse, NumberOfTime60-89DaysPastDueNotWorse, and NumberOfTimes90DaysLate are highly correlated with each other and with the target variable SeriousDlqin2yrs. This is not surprising given that these variables are all related to the number of times a borrower has been past due on a loan payment. RevolvingUtilizationOfUnsecuredLines is also highly correlated with the target variable and with the number of times a borrower has been past due on a loan payment. This is also not surprising given that the RevolvingUtilizationOfUnsecuredLines is the ratio of the amount of money owed to the amount of credit available.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#implementation-of-loan-default-prediction-models",
    "href": "applicationI.html#implementation-of-loan-default-prediction-models",
    "title": "6  Loan Default Prediction",
    "section": "6.7 Implementation of Loan Default Prediction Models",
    "text": "6.7 Implementation of Loan Default Prediction Models\nWe have explored our dataset and are now ready to implement machine learning algorithms for loan default prediction. Let’s start by importing the required libraries\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, recall_score, precision_score, roc_curve\nfrom joblib import dump, load\n\n\n6.7.1 Splitting the Data into Training and Test Sets\nBefore we can train a machine learning model, we need to split our dataset into a training set and a test set.\n\nX = df.drop('SeriousDlqin2yrs', axis=1) # All variables except `SeriousDlqin2yrs`\ny = df['SeriousDlqin2yrs'] # Only SeriousDlqin2yrs\n\nWe follow Alonso Robisco and Carbó Martínez (2022) and use 80% of the data for training and 20% for testing. We will also set the stratify argument to y to make sure that the distribution of the target variable is the same in the training and test sets. Otherwise, we might randomly not have any defaulted loans in the test set, which would make it impossible to correctly evaluate our model.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.2, random_state = 42)\n\n\n\n6.7.2 Scaling Features\nTo improve the performance of our machine learning model, we should scale the features. This is especially important for models that are sensitive to the scale of the features. We will use the MinMaxScaler class from the sklearn.preprocessing module to scale the features. The MinMaxScaler class scales the features so that they have a minimum of 0 and a maximum of 1.\n\ndef scale_features(scaler, df, col_names, only_transform=False):\n\n    # Extract the features we want to scale\n    features = df[col_names] \n\n    # Fit the scaler to the features and transform them\n    if only_transform:\n        features = scaler.transform(features.values)\n    else:\n        features = scaler.fit_transform(features.values)\n\n    # Replace the original features with the scaled features\n    df[col_names] = features\n\nscaler = MinMaxScaler() \nscale_features(scaler, X_train, X_train.columns)\nscale_features(scaler, X_test, X_test.columns, only_transform=True)\n\nNote that we have very skewed distributions for some variables in our dataset. This might make the MinMaxScaler less effective and there might be gains from more carefully scaling different variables. However, for the sake of simplicity, we will use the MinMaxScaler for all variables.\nWe have fully preprocessed and explored our dataset. The next step will be our main task: the implementation of machine learning algorithms for loan default prediction.\n\n\n6.7.3 Evaluation Criertia\nWe will evaluate the performance of our machine-learning models using the following metrics:\n\nAccuracy: The proportion of correctly classified instances\nPrecision: The proportion of true positive predictions among all positive predictions\nRecall: The proportion of true positive predictions among all actual positive instances\nROC AUC: The area under the receiver operating characteristic curve\n\nFurthermore, we will plot the ROC curve for each model to visualize the trade-off between the true positive rate and the false positive rate. To make the evaluation of our models more convenient, we will define a function that computes these metrics and plots the ROC curve for a given model\n\ndef evaluate_model(clf, X_train, y_train, X_test, y_test, label=''):\n\n    # Compute predictions and probabilities for tha training and test set\n    y_pred_train = clf.predict(X_train)\n    y_proba_train = clf.predict_proba(X_train)\n    y_pred_test = clf.predict(X_test)\n    y_proba_test = clf.predict_proba(X_test)\n\n    # Print accuracy measures\n    print(f\"--------------------------------------------------------------\")\n    print(f\"Metrics: {label}\")\n    print(f\"--------------------------------------------------------------\")\n    print(f\"Accuracy (Train): {accuracy_score(y_train, y_pred_train)}\")\n    print(f\"Precision (Train): {precision_score(y_train, y_pred_train)}\")\n    print(f\"Recall (Train): {recall_score(y_train, y_pred_train)}\")\n    print(f\"ROC AUC (Train): {roc_auc_score(y_train, y_proba_train[:, 1])}\")\n    print(f\"--------------------------------------------------------------\")\n    print(f\"Accuracy (Test): {accuracy_score(y_test, y_pred_test)}\")\n    print(f\"Precision (Test): {precision_score(y_test, y_pred_test)}\")\n    print(f\"Recall (Test): {recall_score(y_test, y_pred_test)}\")\n    print(f\"ROC AUC (Test): {roc_auc_score(y_test, y_proba_test[:, 1])}\")\n    print(f\"--------------------------------------------------------------\")\n\n    # Compute the ROC curve\n    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_proba_train[:, 1])\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_proba_test[:, 1])\n\n    # Plot the ROC curve\n    plt.plot(fpr_train, tpr_train, label = \"Train\") \n    plt.plot(fpr_test, tpr_test, label = \"Test\")\n    plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n    plt.xlabel('False Positive Rate (FPR)')\n    plt.ylabel('True Positive Rate (TPR)')\n    plt.title(f'ROC Curve: {label}')\n    plt.legend()\n    plt.show()\n\nWhile we compute all of these metrics, we will focus on the ROC AUC score as our main evaluation metric.\n\n\n6.7.4 Logistic Regression\nLet’s start with a simple logistic regression model. We will use the LogisticRegression class from the sklearn.linear_model module to train a logistic regression model. We will use the lbfgs solver and set the max_iter parameter to 5000 to make sure that the optimization algorithm converges. We will also set the penalty parameter to None to avoid regularization.\n\nclf_logistic = LogisticRegression(penalty = None, solver = 'lbfgs', max_iter = 5000).fit(X_train, y_train)\n\nLet’s evaluate the performance of the logistic regression model\n\nevaluate_model(clf_logistic, X_train, y_train, X_test, y_test, label = 'Logistic Regression')\n\n--------------------------------------------------------------\nMetrics: Logistic Regression\n--------------------------------------------------------------\nAccuracy (Train): 0.9306588679085341\nPrecision (Train): 0.5787234042553191\nRecall (Train): 0.041100030220610456\nROC AUC (Train): 0.6936171984517471\n--------------------------------------------------------------\nAccuracy (Test): 0.9302788844621513\nPrecision (Test): 0.5490196078431373\nRecall (Test): 0.033836858006042296\nROC AUC (Test): 0.6926238627317243\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nThe model does not perform as well as what we have seen in previous lectures. The ROC AUC score is only around 0.7. Note again that the accuracy score is quite high but this is due to the imbalanced nature of the dataset.\n\n\n6.7.5 Decision Tree\nLet’s now train a decision tree classifier. We will use the DecisionTreeClassifier class from the sklearn.tree module to train a decision tree classifier. We will set the max_depth parameter to 7 as in Alonso Robisco and Carbó Martínez (2022) to avoid overfitting.\n\nclf_tree = DecisionTreeClassifier(max_depth=7).fit(X_train, y_train)\n\nThen, let’s evaluate the performance of the decision tree classifier\n\nevaluate_model(clf_tree, X_train, y_train, X_test, y_test, label = 'Decision Tree')\n\n--------------------------------------------------------------\nMetrics: Decision Tree\n--------------------------------------------------------------\nAccuracy (Train): 0.936348994426431\nPrecision (Train): 0.6407185628742516\nRecall (Train): 0.2101843457237836\nROC AUC (Train): 0.8265896261153018\n--------------------------------------------------------------\nAccuracy (Test): 0.9323556836483852\nPrecision (Test): 0.5529622980251346\nRecall (Test): 0.18610271903323264\nROC AUC (Test): 0.8159823399376106\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nThe decision tree classifier performs better than the logistic regression model with a ROC AUC score of around 0.77. This is not surprising given that decision trees are more flexible models that can capture non-linear relationships in the data.\n\n\n6.7.6 Random Forest\nLet’s now train a random forest classifier. We will use the RandomForestClassifier class from the sklearn.ensemble module to train a random forest classifier. We will set the max_depth parameter to 20 and the n_estimators parameter to 100 as in Alonso Robisco and Carbó Martínez (2022).\n\nclf_forest = RandomForestClassifier(max_depth=20, n_estimators = 100).fit(X_train, y_train)\n\nThen, let’s evaluate the performance of the random forest classifier\n\nevaluate_model(clf_forest, X_train, y_train, X_test, y_test, label = 'Random Forest')\n\n--------------------------------------------------------------\nMetrics: Random Forest\n--------------------------------------------------------------\nAccuracy (Train): 0.976126899357874\nPrecision (Train): 1.0\nRecall (Train): 0.6595648232094289\nROC AUC (Train): 0.9956114701590723\n--------------------------------------------------------------\nAccuracy (Test): 0.9321013817072137\nPrecision (Test): 0.5495327102803739\nRecall (Test): 0.17764350453172206\nROC AUC (Test): 0.8400146575047622\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nThis is a good example of the dangers of not using a test set for the evaluation of a model. The random forest classifier performs very well on the training set with a ROC AUC score of close to 1.0. However, it performs much worse on the test set with a ROC AUC score of around 0.83. Nevertheless, the random forest classifier still outperforms the logistic regression and decision tree classifiers.\n\n\n6.7.7 XGBoost\nLet’s now train an XGBoost classifier. We will use the XGBClassifier class from the xgboost module to train an XGBoost classifier. We will set the max_depth parameter to 5 and the n_estimators parameter to 40 as in Alonso Robisco and Carbó Martínez (2022).\n\nclf_xgb = XGBClassifier(max_depth = 5, n_estimators = 40, random_state = 0).fit(X_train, y_train)\n\nThen, let’s evaluate the performance of the XGBoost classifier\n\nevaluate_model(clf_xgb, X_train, y_train, X_test, y_test, label = 'XGBoost')\n\n--------------------------------------------------------------\nMetrics: XGBoost\n--------------------------------------------------------------\nAccuracy (Train): 0.9389238561468201\nPrecision (Train): 0.6947992700729927\nRecall (Train): 0.23012994862496222\nROC AUC (Train): 0.8778826700467908\n--------------------------------------------------------------\nAccuracy (Test): 0.9330762058150377\nPrecision (Test): 0.5701107011070111\nRecall (Test): 0.18670694864048337\nROC AUC (Test): 0.8514290998289821\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nThe XGBoost classifier performs quite well with an ROC AUC score of around 0.83. This is the best performance we have seen so far.\n\n\n6.7.8 Neural Network\nFinally, let’s train a neural network classifier. We will use the MLPClassifier class from the sklearn.neural_network module to train a neural network classifier. We will set the activation parameter to relu, the solver parameter to adam, and the hidden_layer_sizes parameter to (300,200,100) as in Alonso Robisco and Carbó Martínez (2022). We will also set the random_state parameter to 42 to make the results reproducible.\n\n#clf_nn = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(300,200,100), random_state=42, max_iter = 300, verbose=True).fit(X_train, y_train)\n#dump(clf_nn, 'clf_nn.joblib') \n\nSince training the neural network classifier can take a long time, we have saved the trained model to a file called clf_nn.joblib. We can load the model from the file using the load function from the joblib module\n\nclf_nn = load('clf_nn.joblib') \n\nLet’s check the loss curve of the neural network classifier\n\nplt.plot(clf_nn.loss_curve_)\nplt.title(\"Loss Curve\", fontsize=14)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.show()\n\n\n\n\n\n\n\n\nThen, let’s evaluate the performance of the neural network classifier\n\nevaluate_model(clf_nn, X_train, y_train, X_test, y_test, label = 'Neural Network')\n\n--------------------------------------------------------------\nMetrics: Neural Network\n--------------------------------------------------------------\nAccuracy (Train): 0.9465742683366182\nPrecision (Train): 0.8080531665363565\nRecall (Train): 0.31233000906618313\nROC AUC (Train): 0.8682599095370773\n--------------------------------------------------------------\nAccuracy (Test): 0.9282020852759176\nPrecision (Test): 0.4694835680751174\nRecall (Test): 0.18126888217522658\nROC AUC (Test): 0.7983637135044449\n--------------------------------------------------------------",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#overview-of-the-results",
    "href": "applicationI.html#overview-of-the-results",
    "title": "6  Loan Default Prediction",
    "section": "6.8 Overview of the Results",
    "text": "6.8 Overview of the Results\nLooking at all the models side by side, we can see that\n\nresults = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost', 'Neural Network'],\n    'ROC AUC (Train)': [roc_auc_score(y_train, clf_logistic.predict_proba(X_train)[:, 1]), \n                        roc_auc_score(y_train, clf_tree.predict_proba(X_train)[:, 1]), \n                        roc_auc_score(y_train, clf_forest.predict_proba(X_train)[:, 1]), \n                        roc_auc_score(y_train, clf_xgb.predict_proba(X_train)[:, 1]), \n                        roc_auc_score(y_train, clf_nn.predict_proba(X_train)[:, 1])],\n    'ROC AUC (Test)': [roc_auc_score(y_test, clf_logistic.predict_proba(X_test)[:, 1]), \n                        roc_auc_score(y_test, clf_tree.predict_proba(X_test)[:, 1]), \n                        roc_auc_score(y_test, clf_forest.predict_proba(X_test)[:, 1]), \n                        roc_auc_score(y_test, clf_xgb.predict_proba(X_test)[:, 1]), \n                        roc_auc_score(y_test, clf_nn.predict_proba(X_test)[:, 1])]\n})\nresults\n\n\n\n\n\n\n\n\n\nModel\nROC AUC (Train)\nROC AUC (Test)\n\n\n\n\n0\nLogistic Regression\n0.693617\n0.692624\n\n\n1\nDecision Tree\n0.826590\n0.815982\n\n\n2\nRandom Forest\n0.995611\n0.840015\n\n\n3\nXGBoost\n0.877883\n0.851429\n\n\n4\nNeural Network\n0.868260\n0.798364\n\n\n\n\n\n\n\n\nBut can we do better? Alonso Robisco and Carbó Martínez (2022) have also applied feature engineering to the dataset. Let’s see if we can improve the performance of our models by adding some additional features.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#feature-engineering-and-model-improvement",
    "href": "applicationI.html#feature-engineering-and-model-improvement",
    "title": "6  Loan Default Prediction",
    "section": "6.9 Feature Engineering and Model Improvement",
    "text": "6.9 Feature Engineering and Model Improvement\nWe will add the square of each feature to the dataset to create additional features as in Alonso Robisco and Carbó Martínez (2022). We will use the assign method of the pandas DataFrame to add the squared features to the dataset\n\nX2 = df.drop('SeriousDlqin2yrs', axis=1) # All variables except `SeriousDlqin2yrs`\ny2 = df['SeriousDlqin2yrs'] # Only SeriousDlqin2yrs\nX2 = X2.assign(**X2.pow(2).add_suffix('_sq')) # Add the squared features to the dataset\n\nThen, we will split the dataset into a training set and a test set and scale the features\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, stratify=y2, test_size = 0.2, random_state = 42)\nscaler = MinMaxScaler() \nscale_features(scaler, X2_train, X2_train.columns)\nscale_features(scaler, X2_test, X2_test.columns, only_transform=True)\n\nLet’s train the models again with the new dataset and evaluate their performance\n\nclf_logistic2 = LogisticRegression(penalty = None, solver = 'lbfgs', max_iter = 5000).fit(X2_train, y2_train)\nclf_tree2 = DecisionTreeClassifier(max_depth=7).fit(X2_train, y2_train)\nclf_forest2 = RandomForestClassifier(max_depth=20, n_estimators = 100).fit(X2_train, y2_train)\nclf_xgb2 = XGBClassifier(max_depth = 5, n_estimators = 40, random_state = 0).fit(X2_train, y2_train)\n#clf_nn2 = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(300,200,100), random_state=42, max_iter = 300, verbose=True).fit(X2_train, y2_train)\n#dump(clf_nn2, 'clf_nn2.joblib') \n\nThe neural network classifier takes a long time to train, so we will load the model from the file clf_nn2.joblib that we saved earlier\n\nclf_nn2 = load('clf_nn2.joblib') \n\nFurthermore, we will also add a LASSO penalty to the logistic regression model to see if we can improve its performance\n\nclf_logistic_lasso2 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X2_train, y2_train)\n\nLet’s evaluate the performance of the models with the new features in the dataset\n\nresults2 = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost', 'Neural Network', 'Logistic LASSO'],\n    'ROC AUC (Train)': [roc_auc_score(y2_train, clf_logistic2.predict_proba(X2_train)[:, 1]), \n                        roc_auc_score(y2_train, clf_tree2.predict_proba(X2_train)[:, 1]), \n                        roc_auc_score(y2_train, clf_forest2.predict_proba(X2_train)[:, 1]), \n                        roc_auc_score(y2_train, clf_xgb2.predict_proba(X2_train)[:, 1]), \n                        roc_auc_score(y2_train, clf_nn2.predict_proba(X2_train)[:, 1]),\n                        roc_auc_score(y2_train, clf_logistic_lasso2.predict_proba(X2_train)[:, 1])\n    ],\n    'ROC AUC (Test)': [roc_auc_score(y2_test, clf_logistic2.predict_proba(X2_test)[:, 1]), \n                        roc_auc_score(y2_test, clf_tree2.predict_proba(X2_test)[:, 1]), \n                        roc_auc_score(y2_test, clf_forest2.predict_proba(X2_test)[:, 1]), \n                        roc_auc_score(y2_test, clf_xgb2.predict_proba(X2_test)[:, 1]), \n                        roc_auc_score(y2_test, clf_nn2.predict_proba(X2_test)[:, 1]),\n                        roc_auc_score(y2_test, clf_logistic_lasso2.predict_proba(X2_test)[:, 1])\n    ]\n})\nresults2\n\n\n\n\n\n\n\n\n\nModel\nROC AUC (Train)\nROC AUC (Test)\n\n\n\n\n0\nLogistic Regression\n0.810256\n0.813330\n\n\n1\nDecision Tree\n0.826549\n0.814273\n\n\n2\nRandom Forest\n0.994439\n0.838450\n\n\n3\nXGBoost\n0.877883\n0.851429\n\n\n4\nNeural Network\n0.871913\n0.794420\n\n\n5\nLogistic LASSO\n0.804410\n0.810687\n\n\n\n\n\n\n\n\nThe models with the new features in the dataset perform better than the models without the new features. The random forest classifier and the XGBoost classifier have the best performance with ROC AUC scores of around 0.84 and 0.85, respectively.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#feature-importance",
    "href": "applicationI.html#feature-importance",
    "title": "6  Loan Default Prediction",
    "section": "6.10 Feature Importance",
    "text": "6.10 Feature Importance\nWe can also look at the feature importance of the random forest classifier and the XGBoost classifier to see which features are most important for predicting loan defaults. We will use the feature_importances_ attribute of the random forest classifier and the XGBoost classifier to get the feature importances\n\nfeature_importances_forest = clf_forest2.feature_importances_\nfeature_importances_xgb = clf_xgb2.feature_importances_\n\nThen, we will create a bar plot of the feature importances for the random forest classifier and the XGBoost classifier\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\ndf_feature_importance_forest = pd.DataFrame({'Feature': X_train.columns, 'Importance': clf_forest.feature_importances_})\ndf_feature_importance_forest = df_feature_importance_forest.sort_values('Importance', ascending=False)\n\ndf_feature_importance_xgb = pd.DataFrame({'Feature': X_train.columns, 'Importance': clf_xgb.feature_importances_})\ndf_feature_importance_xgb = df_feature_importance_xgb.set_index('Feature')\ndf_feature_importance_xgb = df_feature_importance_xgb.loc[df_feature_importance_forest['Feature'], ]\n\n# Random Forest\nax[0].barh(df_feature_importance_forest['Feature'], df_feature_importance_forest['Importance'])\nax[0].set_title('Random Forest')\nax[0].set_xlabel('Feature Importance')\nax[0].set_ylabel('Feature')\n\n# XGBoost\nax[1].barh(df_feature_importance_forest['Feature'], df_feature_importance_xgb['Importance'])\nax[1].set_title('XGBoost')\nax[1].set_xlabel('Feature Importance')\nax[1].set_ylabel('Feature')\n\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationI.html#conclusions",
    "href": "applicationI.html#conclusions",
    "title": "6  Loan Default Prediction",
    "section": "6.11 Conclusions",
    "text": "6.11 Conclusions\nWe have successfully implemented machine learning algorithms for loan default prediction. We have explored the dataset, preprocessed the data, trained several machine learning models, and evaluated their performance. We have also applied feature engineering to the dataset and improved the performance of the models. The random forest classifier and the XGBoost classifier have the best performance with ROC AUC scores of around 0.84 and 0.85, respectively. We have also looked at the feature importance of the random forest classifier and the XGBoost classifier to see which features are most important for predicting loan defaults.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Loan Default Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#problem-setup",
    "href": "applicationII.html#problem-setup",
    "title": "7  House Price Prediction",
    "section": "7.1 Problem Setup",
    "text": "7.1 Problem Setup\nThe dataset that we will be using is the Kaggle dataset called “House Sales in King County, USA”. As far as I know, this was not used in a Kaggle competition. However, it is a quite popular dataset on Kaggle. The description reads:\n\nThis dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\nIt’s a great dataset for evaluating simple regression models.\n\nThis means that the dataset is a snapshot of house prices in King County, USA, between May 2014 and May 2015. The task, then, is quite straightforward: given a set of features, we want to predict the price of a house.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#dataset",
    "href": "applicationII.html#dataset",
    "title": "7  House Price Prediction",
    "section": "7.2 Dataset",
    "text": "7.2 Dataset\nUnfortunately, the dataset does not have a detailed description of the variables. However, in the comment section, some users found references with variable descriptions. The variables in the dataset should be as follows:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nUnique ID for each home sold\n\n\ndate\nDate of the home sale\n\n\nprice\nPrice of each home sold\n\n\nbedrooms\nNumber of bedrooms\n\n\nbathrooms\nNumber of bathrooms, where .5 accounts for a room with a toilet but no shower\n\n\nsqft_living\nSquare footage of the apartments’ interior living space\n\n\nsqft_lot\nSquare footage of the land space\n\n\nfloors\nNumber of floors\n\n\nwaterfront\nA dummy variable for whether the apartment was overlooking the waterfront or not\n\n\nview\nAn index from 0 to 4 of how good the view of the property was\n\n\ncondition\nAn index from 1 to 5 on the condition of the apartment\n\n\ngrade\nAn index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n\n\nsqft_above\nThe square footage of the interior housing space that is above ground level\n\n\nsqft_basement\nThe square footage of the interior housing space that is below ground level\n\n\nyr_built\nThe year the house was initially built\n\n\nyr_renovated\nThe year of the house’s last renovation\n\n\nzipcode\nWhat zipcode area the house is in\n\n\nlat\nLattitude\n\n\nlong\nLongitude\n\n\nsqft_living15\nThe square footage of interior housing living space for the nearest 15 neighbors\n\n\nsqft_lot15\nThe square footage of the land lots of the nearest 15 neighbors",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#putting-the-problem-into-the-context-of-the-course",
    "href": "applicationII.html#putting-the-problem-into-the-context-of-the-course",
    "title": "7  House Price Prediction",
    "section": "7.3 Putting the Problem into the Context of the Course",
    "text": "7.3 Putting the Problem into the Context of the Course\nThe problem of predicting house prices is a regression problem which belongs to the type of supervised learning problems. We will use the same tools that we have used in the previous examples to solve this problem. The main difference is that we will be using regression models instead of classification models.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#setting-up-the-environment",
    "href": "applicationII.html#setting-up-the-environment",
    "title": "7  House Price Prediction",
    "section": "7.4 Setting up the Environment",
    "text": "7.4 Setting up the Environment\nWe will start by setting up the environment by importing the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nLet’s download the dataset automatically, unzip it, and place it in a folder called data if you haven’t done so already\n\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\nimport os.path\n\n# Check if the file exists\nif not os.path.isfile('data/kc_house_data.csv'):\n\n    print('Downloading dataset...')\n\n    # Define the dataset to be downloaded\n    zipurl = 'https://www.kaggle.com/api/v1/datasets/download/harlfoxem/housesalesprediction'\n\n    # Download and unzip the dataset in the data folder\n    with urlopen(zipurl) as zipresp:\n        with ZipFile(BytesIO(zipresp.read())) as zfile:\n            zfile.extractall('data')\n\n    print('DONE!')\n\nelse:\n\n    print('Dataset already downloaded!')\n\nDataset already downloaded!\n\n\nThen, we can load the data into a DataFrame using the read_csv function from the pandas library\n\ndf = pd.read_csv('data/kc_house_data.csv')\n\nLet’s also download some precomputed models that we will use later on\n\nfor file_name in ['reg_nn.joblib', 'reg_nn_cv.joblib', 'reg_xgb_cv.joblib', 'reg_rf_cv.joblib.zip']:\n\n    if not os.path.isfile(file_name):\n\n        print(f'Downloading {file_name}...')\n\n        # Generate the download link\n        url = f'https://github.com/jmarbet/data-science-course/raw/main/notebooks/{file_name}'\n\n        if file_name.endswith('.zip'):\n\n            # Download and unzip the file\n            with urlopen(url) as zipresp:\n                with ZipFile(BytesIO(zipresp.read())) as zfile:\n                    zfile.extractall('')\n\n        else:\n\n            # Download the file\n            with urlopen(url) as response, open(file_name, 'wb') as out_file:\n                data = response.read()\n                out_file.write(data)\n\n        print('DONE!')\n\n    else:\n\n        print(f'{file_name} already downloaded!')\n\nreg_nn.joblib already downloaded!\nreg_nn_cv.joblib already downloaded!\nreg_xgb_cv.joblib already downloaded!\nreg_rf_cv.joblib.zip already downloaded!",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#data-exploration",
    "href": "applicationII.html#data-exploration",
    "title": "7  House Price Prediction",
    "section": "7.5 Data Exploration",
    "text": "7.5 Data Exploration\nAs with any new dataset, we first need to familiarize ourselves with the data. We will start by looking at the first few rows of the dataset.\n\ndf.head(4).T # Transpose the dataframe for readability\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nid\n7129300520\n6414100192\n5631500400\n2487200875\n\n\ndate\n20141013T000000\n20141209T000000\n20150225T000000\n20141209T000000\n\n\nprice\n221900.0\n538000.0\n180000.0\n604000.0\n\n\nbedrooms\n3\n3\n2\n4\n\n\nbathrooms\n1.0\n2.25\n1.0\n3.0\n\n\nsqft_living\n1180\n2570\n770\n1960\n\n\nsqft_lot\n5650\n7242\n10000\n5000\n\n\nfloors\n1.0\n2.0\n1.0\n1.0\n\n\nwaterfront\n0\n0\n0\n0\n\n\nview\n0\n0\n0\n0\n\n\ncondition\n3\n3\n3\n5\n\n\ngrade\n7\n7\n6\n7\n\n\nsqft_above\n1180\n2170\n770\n1050\n\n\nsqft_basement\n0\n400\n0\n910\n\n\nyr_built\n1955\n1951\n1933\n1965\n\n\nyr_renovated\n0\n1991\n0\n0\n\n\nzipcode\n98178\n98125\n98028\n98136\n\n\nlat\n47.5112\n47.721\n47.7379\n47.5208\n\n\nlong\n-122.257\n-122.319\n-122.233\n-122.393\n\n\nsqft_living15\n1340\n1690\n2720\n1360\n\n\nsqft_lot15\n5650\n7639\n8062\n5000\n\n\n\n\n\n\n\n\nand for reference, we can also run df.info() again to see the data types of the variables\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 21613 entries, 0 to 21612\nData columns (total 21 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   id             21613 non-null  int64  \n 1   date           21613 non-null  object \n 2   price          21613 non-null  float64\n 3   bedrooms       21613 non-null  int64  \n 4   bathrooms      21613 non-null  float64\n 5   sqft_living    21613 non-null  int64  \n 6   sqft_lot       21613 non-null  int64  \n 7   floors         21613 non-null  float64\n 8   waterfront     21613 non-null  int64  \n 9   view           21613 non-null  int64  \n 10  condition      21613 non-null  int64  \n 11  grade          21613 non-null  int64  \n 12  sqft_above     21613 non-null  int64  \n 13  sqft_basement  21613 non-null  int64  \n 14  yr_built       21613 non-null  int64  \n 15  yr_renovated   21613 non-null  int64  \n 16  zipcode        21613 non-null  int64  \n 17  lat            21613 non-null  float64\n 18  long           21613 non-null  float64\n 19  sqft_living15  21613 non-null  int64  \n 20  sqft_lot15     21613 non-null  int64  \ndtypes: float64(5), int64(15), object(1)\nmemory usage: 3.5+ MB\n\n\nWhat immediately stands out is that the date column does not seem to be a proper datetime object. So, let’s fix that\n\ndf['date'] = pd.to_datetime(df['date'])\n\n\ndf.head().T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nid\n7129300520\n6414100192\n5631500400\n2487200875\n1954400510\n\n\ndate\n2014-10-13 00:00:00\n2014-12-09 00:00:00\n2015-02-25 00:00:00\n2014-12-09 00:00:00\n2015-02-18 00:00:00\n\n\nprice\n221900.0\n538000.0\n180000.0\n604000.0\n510000.0\n\n\nbedrooms\n3\n3\n2\n4\n3\n\n\nbathrooms\n1.0\n2.25\n1.0\n3.0\n2.0\n\n\nsqft_living\n1180\n2570\n770\n1960\n1680\n\n\nsqft_lot\n5650\n7242\n10000\n5000\n8080\n\n\nfloors\n1.0\n2.0\n1.0\n1.0\n1.0\n\n\nwaterfront\n0\n0\n0\n0\n0\n\n\nview\n0\n0\n0\n0\n0\n\n\ncondition\n3\n3\n3\n5\n3\n\n\ngrade\n7\n7\n6\n7\n8\n\n\nsqft_above\n1180\n2170\n770\n1050\n1680\n\n\nsqft_basement\n0\n400\n0\n910\n0\n\n\nyr_built\n1955\n1951\n1933\n1965\n1987\n\n\nyr_renovated\n0\n1991\n0\n0\n0\n\n\nzipcode\n98178\n98125\n98028\n98136\n98074\n\n\nlat\n47.5112\n47.721\n47.7379\n47.5208\n47.6168\n\n\nlong\n-122.257\n-122.319\n-122.233\n-122.393\n-122.045\n\n\nsqft_living15\n1340\n1690\n2720\n1360\n1800\n\n\nsqft_lot15\n5650\n7639\n8062\n5000\n7503\n\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 21613 entries, 0 to 21612\nData columns (total 21 columns):\n #   Column         Non-Null Count  Dtype         \n---  ------         --------------  -----         \n 0   id             21613 non-null  int64         \n 1   date           21613 non-null  datetime64[ns]\n 2   price          21613 non-null  float64       \n 3   bedrooms       21613 non-null  int64         \n 4   bathrooms      21613 non-null  float64       \n 5   sqft_living    21613 non-null  int64         \n 6   sqft_lot       21613 non-null  int64         \n 7   floors         21613 non-null  float64       \n 8   waterfront     21613 non-null  int64         \n 9   view           21613 non-null  int64         \n 10  condition      21613 non-null  int64         \n 11  grade          21613 non-null  int64         \n 12  sqft_above     21613 non-null  int64         \n 13  sqft_basement  21613 non-null  int64         \n 14  yr_built       21613 non-null  int64         \n 15  yr_renovated   21613 non-null  int64         \n 16  zipcode        21613 non-null  int64         \n 17  lat            21613 non-null  float64       \n 18  long           21613 non-null  float64       \n 19  sqft_living15  21613 non-null  int64         \n 20  sqft_lot15     21613 non-null  int64         \ndtypes: datetime64[ns](1), float64(5), int64(15)\nmemory usage: 3.5 MB\n\n\nMuch better! Note how the variable type changed for date. On the topic of variable types, it seems surprising that bathrooms and floors are of type float64. Let’s check if there is anything unusual about these variables\n\ndf['bathrooms'].value_counts()\n\nbathrooms\n2.50    5380\n1.00    3852\n1.75    3048\n2.25    2047\n2.00    1930\n1.50    1446\n2.75    1185\n3.00     753\n3.50     731\n3.25     589\n3.75     155\n4.00     136\n4.50     100\n4.25      79\n0.75      72\n4.75      23\n5.00      21\n5.25      13\n0.00      10\n5.50      10\n1.25       9\n6.00       6\n0.50       4\n5.75       4\n6.75       2\n8.00       2\n6.25       2\n6.50       2\n7.50       1\n7.75       1\nName: count, dtype: int64\n\n\n\ndf['floors'].value_counts()\n\nfloors\n1.0    10680\n2.0     8241\n1.5     1910\n3.0      613\n2.5      161\n3.5        8\nName: count, dtype: int64\n\n\nIt seems that the number of bathrooms and floors is not always an integer. This is a bit surprising, but a possible interpretation is that in the case of bathrooms, smaller bathrooms with e.g., only a toilet and a sink are counted as 0.5 bathrooms, while a full bathroom would also need a shower or a bathtub. The same logic could apply to floors, where a split-level house could have, e.g., 1.5 floors. This is just a guess, but it seems plausible.\nNote also that there do not seem to be any missing values, at least none were encoded as such. Now, let’s look at the summary statistics of the dataset\n\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\n\n\nid\n21613.0\n4580301520.864988\n1000102.0\n2123049194.0\n3904930410.0\n7308900445.0\n9900000190.0\n2876565571.312049\n\n\ndate\n21613\n2014-10-29 04:38:01.959931648\n2014-05-02 00:00:00\n2014-07-22 00:00:00\n2014-10-16 00:00:00\n2015-02-17 00:00:00\n2015-05-27 00:00:00\nNaN\n\n\nprice\n21613.0\n540088.141767\n75000.0\n321950.0\n450000.0\n645000.0\n7700000.0\n367127.196483\n\n\nbedrooms\n21613.0\n3.370842\n0.0\n3.0\n3.0\n4.0\n33.0\n0.930062\n\n\nbathrooms\n21613.0\n2.114757\n0.0\n1.75\n2.25\n2.5\n8.0\n0.770163\n\n\nsqft_living\n21613.0\n2079.899736\n290.0\n1427.0\n1910.0\n2550.0\n13540.0\n918.440897\n\n\nsqft_lot\n21613.0\n15106.967566\n520.0\n5040.0\n7618.0\n10688.0\n1651359.0\n41420.511515\n\n\nfloors\n21613.0\n1.494309\n1.0\n1.0\n1.5\n2.0\n3.5\n0.539989\n\n\nwaterfront\n21613.0\n0.007542\n0.0\n0.0\n0.0\n0.0\n1.0\n0.086517\n\n\nview\n21613.0\n0.234303\n0.0\n0.0\n0.0\n0.0\n4.0\n0.766318\n\n\ncondition\n21613.0\n3.40943\n1.0\n3.0\n3.0\n4.0\n5.0\n0.650743\n\n\ngrade\n21613.0\n7.656873\n1.0\n7.0\n7.0\n8.0\n13.0\n1.175459\n\n\nsqft_above\n21613.0\n1788.390691\n290.0\n1190.0\n1560.0\n2210.0\n9410.0\n828.090978\n\n\nsqft_basement\n21613.0\n291.509045\n0.0\n0.0\n0.0\n560.0\n4820.0\n442.575043\n\n\nyr_built\n21613.0\n1971.005136\n1900.0\n1951.0\n1975.0\n1997.0\n2015.0\n29.373411\n\n\nyr_renovated\n21613.0\n84.402258\n0.0\n0.0\n0.0\n0.0\n2015.0\n401.67924\n\n\nzipcode\n21613.0\n98077.939805\n98001.0\n98033.0\n98065.0\n98118.0\n98199.0\n53.505026\n\n\nlat\n21613.0\n47.560053\n47.1559\n47.471\n47.5718\n47.678\n47.7776\n0.138564\n\n\nlong\n21613.0\n-122.213896\n-122.519\n-122.328\n-122.23\n-122.125\n-121.315\n0.140828\n\n\nsqft_living15\n21613.0\n1986.552492\n399.0\n1490.0\n1840.0\n2360.0\n6210.0\n685.391304\n\n\nsqft_lot15\n21613.0\n12768.455652\n651.0\n5100.0\n7620.0\n10083.0\n871200.0\n27304.179631\n\n\n\n\n\n\n\n\nLet’s have a look at the pair plot of some of the quantitative variables\n\nsns.pairplot(df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot']], diag_kind='kde')\n\n\n\n\n\n\n\n\nUnsurprisingly, there seems to be a positive correlation between the square footage of the living area (or number of bedrooms, or number of bathrooms) and the price of a house. However, there does not seem to be such a relationship between the square footage of the lot and the price. This is more surprising given that land prices can be very high in some areas. However, if these “houses” include many apartments (that do not include the land they are built on), this could explain the lack of a relationship. There also seems to be one house with more than 30 bedrooms. This seems a bit unusual, so let’s have a closer look\n\ndf.query('bedrooms &gt; 30').T\n\n\n\n\n\n\n\n\n\n15870\n\n\n\n\nid\n2402100895\n\n\ndate\n2014-06-25 00:00:00\n\n\nprice\n640000.0\n\n\nbedrooms\n33\n\n\nbathrooms\n1.75\n\n\nsqft_living\n1620\n\n\nsqft_lot\n6000\n\n\nfloors\n1.0\n\n\nwaterfront\n0\n\n\nview\n0\n\n\ncondition\n5\n\n\ngrade\n7\n\n\nsqft_above\n1040\n\n\nsqft_basement\n580\n\n\nyr_built\n1947\n\n\nyr_renovated\n0\n\n\nzipcode\n98103\n\n\nlat\n47.6878\n\n\nlong\n-122.331\n\n\nsqft_living15\n1330\n\n\nsqft_lot15\n4700\n\n\n\n\n\n\n\n\nWhat a bargain! A house with 33 bedrooms for only $640000! However, it just has 1.75 bathrooms. It’s maybe not that good of a deal after all. Considering that 1040 square feet corresponds to around 96 m^2. This seems like an error in the data. We will remove this observation from the dataset\n\ndf = df.query('bedrooms &lt; 30')\n\nWe could also look at the distribution of the number of bedrooms and floors and how if affects prices\n\nsns.boxplot(x=df['bedrooms'],y=df['price'])\n\n\n\n\n\n\n\n\n\nsns.boxplot(x=df['floors'],y=df['price'])\n\n\n\n\n\n\n\n\nThere seems to be great variability in the prices for a given number of bedrooms or floors.\nInterestingly, we also have latitudinal and longitudinal information. We can use this to plot the houses on a map. Let’s do that\n\nimport folium\nfrom folium.plugins import HeatMap\n\n# Initalize the map\nm = folium.Map(location=[47.5112, -122.257])\n\n# Create Layers and add them to the map\nlayer_heat_map = folium.FeatureGroup(name='Heat Map').add_to(m)\nlayer_most_expensive = folium.FeatureGroup(name='10 Most Expensive Houses').add_to(m)\nfolium.LayerControl().add_to(m)\n\n# Add a heatmap to a layer\ndata = df[['lat', 'long', 'price']].groupby(['lat','long']).mean().reset_index().values.tolist() # Note for latitudes and longitudes that show up multiple times, we take the mean()\nHeatMap(data, radius=8).add_to(layer_heat_map)\n\n# Add the 10 most expensive houses to a layer\ndf_most_expensive_houses = df.sort_values(by=['price'], ascending=False).head(10)\nfor indice, row in df_most_expensive_houses.iterrows():\n    folium.Marker(\n        location=[row[\"lat\"], row[\"long\"]],\n        popup=f\"Price: {row['price']}\",\n        icon=folium.map.Icon(color='red')\n    ).add_to(layer_most_expensive)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe 10 most expensive houses seem to be close to the waterfront and looking at the actual data, we can see that about half of them are indeed overlooking the waterfront\n\ndf_most_expensive_houses['waterfront']\n\n7252    0\n3914    1\n9254    0\n4411    0\n1448    0\n1315    1\n1164    1\n8092    1\n2626    1\n8638    0\nName: waterfront, dtype: int64\n\n\nThe heatmap also shows that the most expensive houses are located in the north-western part of the county, in or near Seattle.\nFinally, let’s look at the distribution of some of the discrete variables in the dataset\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nvariables = ['waterfront', 'view', 'condition', 'grade']\n\nfor var, ax in zip(variables, axes.flatten()):\n    sns.countplot(x=var, data=df, ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(15, 10))\nvariables = ['yr_built', 'yr_renovated']\n\nfor var, ax in zip(variables, axes.flatten()):\n    sns.countplot(x=var, data=df, ax=ax)\n\nfor ax in axes.flatten():\n    if ax.get_xlabel() in ('yr_built', 'yr_renovated'):\n        ax.tick_params(axis='x', labelrotation=90)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThere seems to be some cyclicality in yr_built. We could probably infer housing booms and busts if we analyze it carefully. yr_renovated seems to have a lot of zeros, which could mean that many houses have never been renovated. Let’s check what’s going on here\n\ndf['yr_renovated'].value_counts()\n\nyr_renovated\n0       20698\n2014       91\n2013       37\n2003       36\n2005       35\n        ...  \n1951        1\n1959        1\n1948        1\n1954        1\n1944        1\nName: count, Length: 70, dtype: int64\n\n\nIndeed, almost all of the houses seem to have a zero. However, some houses have values different from zero, so it might indeed be the case that houses with a value of zero have never been renovated. We could also check if the year of renovation is after the year the house was built\n\ndf.query('yr_renovated != 0 and yr_renovated &lt; yr_built')\n\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n\n\n0 rows × 21 columns\n\n\n\n\nWith this command, we selected all observations where yr_renovated is different from zero and yr_renovated &lt; yr_built. Since there were no rows selected, there do not seem to be any errors in the dataset in this respect.\nAnother thing we can check is whether there are errors in the square footage variables. For example, we could check if the sum of sqft_above and sqft_basement is equal to sqft_living\n\ndf.query('sqft_above + sqft_basement != sqft_living')\n\n\n\n\n\n\n\n\n\nid\ndate\nprice\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\n...\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n\n\n0 rows × 21 columns\n\n\n\n\nThis indeed seems to be correct for all observations.\nWe haven’t looked at the square footage of the 15 nearest neighbors yet. Let’s check how it relates to price and the square footage of the house itself\n\nsns.pairplot(df[['price', 'sqft_living', 'sqft_lot', 'sqft_living15', 'sqft_lot15']], diag_kind='kde')\n\n\n\n\n\n\n\n\nThere seems to be a positive relationship between the square footage of the living area of the house and the square footage of the living area of the 15 nearest neighbors. There also seems to be a positive relationship with price. This likely just reflects the fact that neighborhoods tend to have houses of similar sizes and prices.\nFinally, let’s look at the distribution of the zip codes in the dataset\n\nplt.figure(figsize=(15, 5))\nsns.countplot(x='zipcode', data=df)\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nThis likely doesn’t tell us much, but it’s interesting to see that some zip codes are much more common than others. Finally, we can again look at the correlation between variables in our dataset\n\nf, ax = plt.subplots(figsize=(16, 12))\ncorr = df.drop(['id', 'date'], axis=1).corr()\ncmap = sns.diverging_palette(10, 255, as_cmap=True) # Create a color map\nmask = np.triu(np.ones_like(corr, dtype=bool)) # Create a mask to only show the lower triangle of the matrix\nsns.heatmap(corr, cmap=cmap, annot=True, vmax=1, center=0, mask=mask) # Create a heatmap of the correlation matrix (Note: vmax=1 makes sure that the color map goes up to 1 and center=0 are used to center the color map at 0)\nplt.show()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#implementation-of-house-price-prediction-models",
    "href": "applicationII.html#implementation-of-house-price-prediction-models",
    "title": "7  House Price Prediction",
    "section": "7.6 Implementation of House Price Prediction Models",
    "text": "7.6 Implementation of House Price Prediction Models\nWe have explored our dataset and are now ready to implement machine learning algorithms for house price prediction. Let’s start by importing the required libraries\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, LassoCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom joblib import dump, load\n\n\n7.6.1 Data Preprocessing\nThe dataset seems to be pretty clean already. Let’s check again the number of missing values\n\ndf.isnull().sum()\n\nid               0\ndate             0\nprice            0\nbedrooms         0\nbathrooms        0\nsqft_living      0\nsqft_lot         0\nfloors           0\nwaterfront       0\nview             0\ncondition        0\ngrade            0\nsqft_above       0\nsqft_basement    0\nyr_built         0\nyr_renovated     0\nzipcode          0\nlat              0\nlong             0\nsqft_living15    0\nsqft_lot15       0\ndtype: int64\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 21612 entries, 0 to 21612\nData columns (total 21 columns):\n #   Column         Non-Null Count  Dtype         \n---  ------         --------------  -----         \n 0   id             21612 non-null  int64         \n 1   date           21612 non-null  datetime64[ns]\n 2   price          21612 non-null  float64       \n 3   bedrooms       21612 non-null  int64         \n 4   bathrooms      21612 non-null  float64       \n 5   sqft_living    21612 non-null  int64         \n 6   sqft_lot       21612 non-null  int64         \n 7   floors         21612 non-null  float64       \n 8   waterfront     21612 non-null  int64         \n 9   view           21612 non-null  int64         \n 10  condition      21612 non-null  int64         \n 11  grade          21612 non-null  int64         \n 12  sqft_above     21612 non-null  int64         \n 13  sqft_basement  21612 non-null  int64         \n 14  yr_built       21612 non-null  int64         \n 15  yr_renovated   21612 non-null  int64         \n 16  zipcode        21612 non-null  int64         \n 17  lat            21612 non-null  float64       \n 18  long           21612 non-null  float64       \n 19  sqft_living15  21612 non-null  int64         \n 20  sqft_lot15     21612 non-null  int64         \ndtypes: datetime64[ns](1), float64(5), int64(15)\nmemory usage: 3.6 MB\n\n\nThere don’t seem to be any missing values. However, we could still check for duplicates\n\ndf.duplicated().sum()\n\n0\n\n\nThere also don’t seem to be any duplicates.\nThere are some variables such as id, zipcode, lat and long which likely don’t provide very useful information given the other variables in the dataset. We will drop these variables\n\ndf = df.drop(['id', 'zipcode', 'lat', 'long'], axis=1)\ndf.head().T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\ndate\n2014-10-13 00:00:00\n2014-12-09 00:00:00\n2015-02-25 00:00:00\n2014-12-09 00:00:00\n2015-02-18 00:00:00\n\n\nprice\n221900.0\n538000.0\n180000.0\n604000.0\n510000.0\n\n\nbedrooms\n3\n3\n2\n4\n3\n\n\nbathrooms\n1.0\n2.25\n1.0\n3.0\n2.0\n\n\nsqft_living\n1180\n2570\n770\n1960\n1680\n\n\nsqft_lot\n5650\n7242\n10000\n5000\n8080\n\n\nfloors\n1.0\n2.0\n1.0\n1.0\n1.0\n\n\nwaterfront\n0\n0\n0\n0\n0\n\n\nview\n0\n0\n0\n0\n0\n\n\ncondition\n3\n3\n3\n5\n3\n\n\ngrade\n7\n7\n6\n7\n8\n\n\nsqft_above\n1180\n2170\n770\n1050\n1680\n\n\nsqft_basement\n0\n400\n0\n910\n0\n\n\nyr_built\n1955\n1951\n1933\n1965\n1987\n\n\nyr_renovated\n0\n1991\n0\n0\n0\n\n\nsqft_living15\n1340\n1690\n2720\n1360\n1800\n\n\nsqft_lot15\n5650\n7639\n8062\n5000\n7503\n\n\n\n\n\n\n\n\n\nBinning & Encoding\nFurthermore, we will need to convert the date variable into something that can be used in a machine-learning model. We will extract the year and month from the date and drop the original date variable\n\ndf['year_sale'] = pd.DatetimeIndex(df['date']).year\ndf['month_sale'] = pd.DatetimeIndex(df['date']).month\n\nFurthermore, we can convert yr_built and yr_renovated into the age of the house and the number of years since the last renovation\n\ndf['age'] = df['year_sale'] - df['yr_built']\ndf['years_since_renovation'] = df['year_sale'] - np.maximum(df['yr_built'], df['yr_renovated'])\n\nIf the house has never been renovated, years_since_renovation will be equal to the age of the house. We can drop the original yr_built, yr_renovated, and date variables\n\ndf = df.drop(['yr_built', 'yr_renovated', 'date'], axis=1)\n\nLet’s check the summary statistics of the dataset again\n\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nprice\n21612.0\n540083.518786\n367135.061269\n75000.0\n321837.50\n450000.00\n645000.00\n7700000.0\n\n\nbedrooms\n21612.0\n3.369471\n0.907982\n0.0\n3.00\n3.00\n4.00\n11.0\n\n\nbathrooms\n21612.0\n2.114774\n0.770177\n0.0\n1.75\n2.25\n2.50\n8.0\n\n\nsqft_living\n21612.0\n2079.921016\n918.456818\n290.0\n1426.50\n1910.00\n2550.00\n13540.0\n\n\nsqft_lot\n21612.0\n15107.388951\n41421.423497\n520.0\n5040.00\n7619.00\n10688.25\n1651359.0\n\n\nfloors\n21612.0\n1.494332\n0.539991\n1.0\n1.00\n1.50\n2.00\n3.5\n\n\nwaterfront\n21612.0\n0.007542\n0.086519\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\nview\n21612.0\n0.234314\n0.766334\n0.0\n0.00\n0.00\n0.00\n4.0\n\n\ncondition\n21612.0\n3.409356\n0.650668\n1.0\n3.00\n3.00\n4.00\n5.0\n\n\ngrade\n21612.0\n7.656904\n1.175477\n1.0\n7.00\n7.00\n8.00\n13.0\n\n\nsqft_above\n21612.0\n1788.425319\n828.094487\n290.0\n1190.00\n1560.00\n2210.00\n9410.0\n\n\nsqft_basement\n21612.0\n291.495697\n442.580931\n0.0\n0.00\n0.00\n560.00\n4820.0\n\n\nsqft_living15\n21612.0\n1986.582871\n685.392610\n399.0\n1490.00\n1840.00\n2360.00\n6210.0\n\n\nsqft_lot15\n21612.0\n12768.828984\n27304.756179\n651.0\n5100.00\n7620.00\n10083.25\n871200.0\n\n\nyear_sale\n21612.0\n2014.322969\n0.467622\n2014.0\n2014.00\n2014.00\n2015.00\n2015.0\n\n\nmonth_sale\n21612.0\n6.574449\n3.115377\n1.0\n4.00\n6.00\n9.00\n12.0\n\n\nage\n21612.0\n43.316722\n29.375731\n-1.0\n18.00\n40.00\n63.00\n115.0\n\n\nyears_since_renovation\n21612.0\n40.935730\n28.813764\n-1.0\n15.00\n37.00\n60.00\n115.0\n\n\n\n\n\n\n\n\nFinally, we need to take care of the categorical variables in the dataset. We will use one-hot (aka ‘one-of-K’ or ‘dummy’) encoding for this purpose\n\n# Define for which variables to do the one-hot encoding\ncategorical_variables = ['view', 'condition', 'grade']\n\n# Initialize the encoder\nencoder = OneHotEncoder(sparse_output=False)\n\n# Apply the one-hot encoding to the desired columns\none_hot_encoded = encoder.fit_transform(df[categorical_variables])\n\n# Convert the results to a DataFrame\ndf_one_hot_encoded = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(['view', 'condition', 'grade']), index=df.index)\n\n# Concatenate the one-hot encoded columns with the original DataFrame\ndf_encoded = pd.concat([df, df_one_hot_encoded], axis=1)\n\n# Drop the old, unencoded columns from the old Dataframe\ndf_encoded = df_encoded.drop(categorical_variables, axis=1)\n\nYou can see that now we have many more dummy variables taking values zero or one in our dataset\n\ndf_encoded.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nprice\n21612.0\n540083.518786\n367135.061269\n75000.0\n321837.50\n450000.00\n645000.00\n7700000.0\n\n\nbedrooms\n21612.0\n3.369471\n0.907982\n0.0\n3.00\n3.00\n4.00\n11.0\n\n\nbathrooms\n21612.0\n2.114774\n0.770177\n0.0\n1.75\n2.25\n2.50\n8.0\n\n\nsqft_living\n21612.0\n2079.921016\n918.456818\n290.0\n1426.50\n1910.00\n2550.00\n13540.0\n\n\nsqft_lot\n21612.0\n15107.388951\n41421.423497\n520.0\n5040.00\n7619.00\n10688.25\n1651359.0\n\n\nfloors\n21612.0\n1.494332\n0.539991\n1.0\n1.00\n1.50\n2.00\n3.5\n\n\nwaterfront\n21612.0\n0.007542\n0.086519\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\nsqft_above\n21612.0\n1788.425319\n828.094487\n290.0\n1190.00\n1560.00\n2210.00\n9410.0\n\n\nsqft_basement\n21612.0\n291.495697\n442.580931\n0.0\n0.00\n0.00\n560.00\n4820.0\n\n\nsqft_living15\n21612.0\n1986.582871\n685.392610\n399.0\n1490.00\n1840.00\n2360.00\n6210.0\n\n\nsqft_lot15\n21612.0\n12768.828984\n27304.756179\n651.0\n5100.00\n7620.00\n10083.25\n871200.0\n\n\nyear_sale\n21612.0\n2014.322969\n0.467622\n2014.0\n2014.00\n2014.00\n2015.00\n2015.0\n\n\nmonth_sale\n21612.0\n6.574449\n3.115377\n1.0\n4.00\n6.00\n9.00\n12.0\n\n\nage\n21612.0\n43.316722\n29.375731\n-1.0\n18.00\n40.00\n63.00\n115.0\n\n\nyears_since_renovation\n21612.0\n40.935730\n28.813764\n-1.0\n15.00\n37.00\n60.00\n115.0\n\n\nview_0\n21612.0\n0.901721\n0.297698\n0.0\n1.00\n1.00\n1.00\n1.0\n\n\nview_1\n21612.0\n0.015362\n0.122990\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\nview_2\n21612.0\n0.044559\n0.206337\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\nview_3\n21612.0\n0.023598\n0.151797\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\nview_4\n21612.0\n0.014760\n0.120595\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ncondition_1\n21612.0\n0.001388\n0.037232\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ncondition_2\n21612.0\n0.007959\n0.088857\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ncondition_3\n21612.0\n0.649223\n0.477224\n0.0\n0.00\n1.00\n1.00\n1.0\n\n\ncondition_4\n21612.0\n0.262771\n0.440149\n0.0\n0.00\n0.00\n1.00\n1.0\n\n\ncondition_5\n21612.0\n0.078660\n0.269214\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_1\n21612.0\n0.000046\n0.006802\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_3\n21612.0\n0.000139\n0.011781\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_4\n21612.0\n0.001342\n0.036607\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_5\n21612.0\n0.011197\n0.105226\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_6\n21612.0\n0.094299\n0.292252\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_7\n21612.0\n0.415510\n0.492821\n0.0\n0.00\n0.00\n1.00\n1.0\n\n\ngrade_8\n21612.0\n0.280770\n0.449386\n0.0\n0.00\n0.00\n1.00\n1.0\n\n\ngrade_9\n21612.0\n0.120998\n0.326132\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_10\n21612.0\n0.052471\n0.222980\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_11\n21612.0\n0.018462\n0.134618\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_12\n21612.0\n0.004164\n0.064399\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\ngrade_13\n21612.0\n0.000602\n0.024519\n0.0\n0.00\n0.00\n0.00\n1.0\n\n\n\n\n\n\n\n\nGiven that these categorical variables are ordinal, this might have not been strictly necessary. However, is required if you have data that is not ordinal.\n\n\nSplitting the Data into Training and Test Sets\nBefore we can train a machine learning model, we need to split our dataset into a training set and a test set.\n\nX = df_encoded.drop('price', axis=1) # All variables except `SeriousDlqin2yrs`\ny = df_encoded[['price']] # Only SeriousDlqin2yrs\n\nWe will use 80% of the data for training and 20% for testing. Note that since our target variable is continuous, we don’t need to stratify the split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n\n\nScaling Features\nTo improve the performance of our machine learning model, we should scale the features. We will use the StandardScaler and MinMaxScalerclass from the sklearn.preprocessing module to scale the features. The StandardScaler scales each feature to have a mean of 0 and a standard deviation of 1. The MinMaxScaler scales each feature to a given range, usually 0 to 1.\n\ndef scale_features(scaler, df, col_names, only_transform=False):\n\n    # Extract the features we want to scale\n    features = df[col_names] \n\n    # Fit the scaler to the features and transform them\n    if only_transform:\n        features = scaler.transform(features.values)\n    else:\n        features = scaler.fit_transform(features.values)\n\n    # Replace the original features with the scaled features\n    df[col_names] = features\n\n\n# Define which features to scale with the StandardScaler and MinMaxScaler\nfor_standard_scaler = [\n    'bedrooms', \n    'bathrooms', \n    'sqft_living', \n    'sqft_lot', \n    'floors', \n    'sqft_above', \n    'sqft_basement', \n    'sqft_living15', \n    'sqft_lot15', \n    'age', \n    'years_since_renovation'\n]\n\nfor_min_max_scaler = [\n    'year_sale', \n    'month_sale'\n]\n\n# Apply the standard scaler (Note: we use the same mean and std for scaling the test set)\nstandard_scaler = StandardScaler() \nscale_features(standard_scaler, X_train, for_standard_scaler)\nscale_features(standard_scaler, X_test, for_standard_scaler, only_transform=True)\n\n# Apply the minmax scaler (Note: we use the same min and max for scaling the test set)\nminmax_scaler = MinMaxScaler()\nscale_features(minmax_scaler, X_train, for_min_max_scaler)\nscale_features(minmax_scaler, X_test, for_min_max_scaler, only_transform=True)\n\n# Apply standard scaler to the target variable\ntarget_scaler = StandardScaler()\ny_train = pd.DataFrame(target_scaler.fit_transform(y_train), columns=['price'])\ny_test = pd.DataFrame(target_scaler.transform(y_test), columns=['price']) \n\n\n\n\n7.6.2 Evaluation Criertia\nWe will evaluate our models based on the following criteria\n\nRoot Mean Squared Error (MSE): Square root of the mean of the squared differences between the predicted and the actual values\nMean Absolute Error (MAE): Mean of the absolute differences between the predicted and the actual values\nR-squared (R2): Proportion of the variance in the dependent variable that is predictable from the independent variables\n\nWe define a function that a function that will calculate these metrics for us\n\ndef evaluate_model(model, X_train, y_train, X_test, y_test, label='', print_results=True):\n\n    # Predict the target variable\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    # Transform the target variable back to the original scale \n    # (This makes it easier to interpret the RMSE and MAE)\n    y_train_inv = target_scaler.inverse_transform(y_train)\n    y_test_inv = target_scaler.inverse_transform(y_test)\n    y_pred_train_inv = target_scaler.inverse_transform(y_pred_train.reshape(-1, 1))\n    y_pred_test_inv = target_scaler.inverse_transform(y_pred_test.reshape(-1, 1))\n\n    # Calculate the evaluation metrics\n    rmse_train = mean_squared_error(y_train_inv, y_pred_train_inv, squared=False)\n    rmse_test = mean_squared_error(y_test_inv, y_pred_test_inv, squared=False)\n    mae_train = mean_absolute_error(y_train_inv, y_pred_train_inv)\n    mae_test = mean_absolute_error(y_test_inv, y_pred_test_inv)\n    r2_train = r2_score(y_train_inv, y_pred_train_inv)\n    r2_test = r2_score(y_test_inv, y_pred_test_inv)\n\n    # Print the evaluation metrics\n    if print_results:\n        print(f\"--------------------------------------------------------------\")\n        print(f\"Metrics: {label}\")\n        print(f\"--------------------------------------------------------------\")\n        print(f\"RMSE (Train): {rmse_train}\")\n        print(f\"MAE (Train): {mae_train}\")\n        print(f\"R2 (Train): {r2_train}\")\n        print(f\"--------------------------------------------------------------\")\n        print(f\"RMSE (Test): {rmse_test}\")\n        print(f\"MAE (Test): {mae_test}\")\n        print(f\"R2 (Test): {r2_test}\")\n        print(f\"--------------------------------------------------------------\")\n\n    return rmse_train, rmse_test, mae_train, mae_test, r2_train, r2_test\n\n\n\n7.6.3 Linear Regression\nWe will start by training a simple linear regression model using only a few basic features\n\nbasic_features = ['bedrooms', 'bathrooms', 'sqft_living']\nreg_lin_basic = LinearRegression().fit(X_train[basic_features], y_train)\n\nWe can evaluate the model using the function we defined earlier\n\nevaluate_model(reg_lin_basic, X_train[basic_features], y_train, X_test[basic_features], y_test, label = 'Linear Regression (Basic Features)');\n\n--------------------------------------------------------------\nMetrics: Linear Regression (Basic Features)\n--------------------------------------------------------------\nRMSE (Train): 253683.12629128053\nMAE (Train): 168994.9071419931\nR2 (Train): 0.5085183567620137\n--------------------------------------------------------------\nRMSE (Test): 271925.6970016718\nMAE (Test): 174452.3090166579\nR2 (Test): 0.5073277848405491\n--------------------------------------------------------------\n\n\nWe are not doing that badly with a RMSE of around $250000 if we take into account the minimum and maximum prices in the dataset\n\nprint(f'Min Price: {df[\"price\"].min()}, Max Price: {df[\"price\"].max()}')\n\nMin Price: 75000.0, Max Price: 7700000.0\n\n\nand the distribution of prices\n\nax = df['price'].plot.hist(bins=100)\nax.ticklabel_format(useOffset=False,style='plain')\nax.tick_params(axis='x', labelrotation=45)\nax.set_xlim(0,3000000)\n\n\n\n\n\n\n\n\nLet’s now try a linear regression but with all the features\n\nreg_lin = LinearRegression().fit(X_train, y_train)\n\nWe can evaluate the model using the function we defined earlier\n\nevaluate_model(reg_lin, X_train, y_train, X_test, y_test, label = 'Linear Regression (All Features)');\n\n--------------------------------------------------------------\nMetrics: Linear Regression (All Features)\n--------------------------------------------------------------\nRMSE (Train): 203680.13271422562\nMAE (Train): 133387.29115116654\nR2 (Train): 0.6831735158523966\n--------------------------------------------------------------\nRMSE (Test): 215930.28643225055\nMAE (Test): 137547.23127374452\nR2 (Test): 0.6893404625153258\n--------------------------------------------------------------\n\n\nThe performance of the model has improved. Since we have a large sample size but relatively few regressors it is unlikely to overfit. Note, however, that if we add more regressors, e.g., squared and cubed features, etc. we might run into trouble at a certain point. That’s why it’s important to use the train-test split to check that our model generalizes.\n\n\n7.6.4 LASSO Regression\nOne way to deal with overfitting in a linear regression is to use LASSO regression. LASSO regression is a type of linear regression that uses a penalty (or regularization) term to shrink the coefficients of the regressors towards zero. Essentially, LASSO selects a subset of features, which can help to prevent overfitting. We will use the Lasso class from the sklearn.linear_model module to train a LASSO regression model\n\nreg_lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n\nWe can evaluate the model using the function we defined earlier\n\nevaluate_model(reg_lasso, X_train, y_train, X_test, y_test, label = 'LASSO Regression');\n\n--------------------------------------------------------------\nMetrics: LASSO Regression\n--------------------------------------------------------------\nRMSE (Train): 254661.31873140397\nMAE (Train): 163764.59579244166\nR2 (Train): 0.5047207803287292\n--------------------------------------------------------------\nRMSE (Test): 273446.0239203981\nMAE (Test): 168730.33198849438\nR2 (Test): 0.5018033587261654\n--------------------------------------------------------------\n\n\nThis model is doing a bit worse than a standard linear regression. However, we just chose the value of the penalty term \\(\\alpha\\) arbitrarily. We can use cross-validation to find the best value of \\(\\alpha\\)\n\nreg_lasso_cv = LassoCV(cv=5, random_state=42).fit(X_train, y_train.values.ravel())\n\nThis command repeatedly runs 5-fold cross-validation for a LASSO regression using different values of \\(\\alpha\\). The \\(\\alpha\\) that minimizes the mean squared error is then stored in the alpha_ attribute of the model\n\nreg_lasso_cv.alpha_\n\n0.0007018253833076978\n\n\nThis \\(\\alpha\\) is much smaller than our initial value. Let’s see how well it does in terms of the RMSE\n\nevaluate_model(reg_lasso_cv, X_train, y_train, X_test, y_test, label = 'LASSO Regression (CV)');\n\n--------------------------------------------------------------\nMetrics: LASSO Regression (CV)\n--------------------------------------------------------------\nRMSE (Train): 204360.76389300276\nMAE (Train): 134141.73731503487\nR2 (Train): 0.6810525207180653\n--------------------------------------------------------------\nRMSE (Test): 217146.28161042708\nMAE (Test): 138085.31092628682\nR2 (Test): 0.6858316989155063\n--------------------------------------------------------------\n\n\nIt’s always a good idea to use cross-validation to find the best hyperparameters for your model. For more complicated models with several hyperparameter choices, one can use GridSearchCV or RandomizedSearchCV from sklearn to find the hyperparameters.\nWe can check which coefficients the LASSO regression has shrunk to zero because of the regularization term\n\nX_train.columns[np.abs(reg_lasso_cv.coef_) &lt; 1e-12]\n\nIndex(['sqft_above', 'view_1', 'view_3', 'condition_1', 'condition_3',\n       'grade_1', 'grade_3'],\n      dtype='object')\n\n\nCompare this to the linear regression where none of the coefficients were zero\n\nX_train.columns[(np.abs(reg_lin.coef_) &lt; 1e-12).reshape(-1)]\n\nIndex([], dtype='object')\n\n\n\n\n7.6.5 Decision Tree\nWe will now train a decision tree regressor on the data\n\nreg_tree = DecisionTreeRegressor(random_state=42).fit(X_train, y_train)\n\nWe can evaluate the model using the function we defined earlier\n\nevaluate_model(reg_tree, X_train, y_train, X_test, y_test, label = 'Decision Tree');\n\n--------------------------------------------------------------\nMetrics: Decision Tree\n--------------------------------------------------------------\nRMSE (Train): 0.0\nMAE (Train): 0.0\nR2 (Train): 1.0\n--------------------------------------------------------------\nRMSE (Test): 276927.1448808092\nMAE (Test): 163650.69442516772\nR2 (Test): 0.48903797314415076\n--------------------------------------------------------------\n\n\nThe decision tree perfectly fits the training data but does not generalize well to the test data. Why did this happen? We did not change any of the default hyperparameters of the decision tree which resulted in the decision tree overfitting, i.e., it learned the noise in the training data. We can try to reduce the depth of the tree to prevent overfitting\n\nreg_tree = DecisionTreeRegressor(max_depth=10, random_state=42).fit(X_train, y_train)\nevaluate_model(reg_tree, X_train, y_train, X_test, y_test, label = 'Decision Tree');\n\n--------------------------------------------------------------\nMetrics: Decision Tree\n--------------------------------------------------------------\nRMSE (Train): 151229.68262754745\nMAE (Train): 104245.46532488744\nR2 (Train): 0.8253380836313531\n--------------------------------------------------------------\nRMSE (Test): 242624.93510350658\nMAE (Test): 139455.8139319333\nR2 (Test): 0.6077811751936795\n--------------------------------------------------------------\n\n\nThis seems to have improved the performance of the model. However, we need a more rigorous way to find the best hyperparameters. One such way is to use grid search, which tries many different hyperparameter values. We, then, combine this with cross-validation to find the best hyperparameters for the decision tree. GridSearchCV from the sklearn package does exactly that\n\nparam_grid = {\n    'max_depth': [5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10, 15],\n    'min_samples_leaf': [1, 2, 5, 10]\n}\n\nreg_tree_cv = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5).fit(X_train, y_train)\n\nNote that param_grid is a dictionary where the keys are the hyperparameters of the decision tree and the values are lists of the values we want to try. The best hyperparameters are stored in the best_params_ attribute of the model\n\nreg_tree_cv.best_params_\n\n{'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 15}\n\n\nWe can then evaluate the model using the best hyperparameters\n\nevaluate_model(reg_tree_cv, X_train, y_train, X_test, y_test, label = 'Decision Tree (CV)');\n\n--------------------------------------------------------------\nMetrics: Decision Tree (CV)\n--------------------------------------------------------------\nRMSE (Train): 165756.35013566245\nMAE (Train): 111454.50933401058\nR2 (Train): 0.7901714932986897\n--------------------------------------------------------------\nRMSE (Test): 233527.64612876996\nMAE (Test): 137387.4802938534\nR2 (Test): 0.6366424628160059\n--------------------------------------------------------------\n\n\nNote that using reg_tree_cv as the model to be evaluated uses automatically the best estimator. Alternatively, we could also use best_estimator_ attribute in evaluate_model\n\nreg_tree_cv.best_estimator_\n\nDecisionTreeRegressor(max_depth=10, min_samples_leaf=5, min_samples_split=15,\n                      random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=10, min_samples_leaf=5, min_samples_split=15,\n                      random_state=42)\n\n\n\n\n7.6.6 Random Forest\nWe will now train a random forest regressor on the data\n\nreg_rf = RandomForestRegressor(random_state=42).fit(X_train, y_train)\n\nWe can evaluate the model using the function we defined earlier\n\nevaluate_model(reg_rf, X_train, y_train, X_test, y_test, label = 'Random Forest');\n\n--------------------------------------------------------------\nMetrics: Random Forest\n--------------------------------------------------------------\nRMSE (Train): 66638.2969151954\nMAE (Train): 42418.54030134768\nR2 (Train): 0.9660865542789411\n--------------------------------------------------------------\nRMSE (Test): 207350.8910584885\nMAE (Test): 120473.06515845477\nR2 (Test): 0.713536442057418\n--------------------------------------------------------------\n\n\nLet’s use grid search with cross-validation to find the best hyperparameters for the random forest\n\nparam_grid = {\n    'max_depth': [5, 10, 15, 20],\n    'n_estimators': [50, 100, 150, 200, 300],\n}\n\n#reg_rf_cv = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5).fit(X_train, y_train)\n#dump(reg_rf_cv, 'reg_rf_cv.joblib')\n\n\nreg_rf_cv = load('reg_rf_cv.joblib')\n\nWe are trying 20 different hyperparameter combinations and for each parameter combination we will have to estimate the model 5 times (5-fold cross-validation). This might take a while. The best hyperparameters are stored in the best_params_ attribute of the model\n\nreg_rf_cv.best_params_\n\n{'max_depth': 20, 'n_estimators': 300}\n\n\nWe can then evaluate the model using the best hyperparameters\n\nevaluate_model(reg_rf_cv, X_train, y_train, X_test, y_test, label = 'Random Forest (CV)');\n\n--------------------------------------------------------------\nMetrics: Random Forest (CV)\n--------------------------------------------------------------\nRMSE (Train): 72654.48965997726\nMAE (Train): 49716.92916875867\nR2 (Train): 0.959686634834322\n--------------------------------------------------------------\nRMSE (Test): 206073.35722748775\nMAE (Test): 120254.77646893183\nR2 (Test): 0.7170554960056299\n--------------------------------------------------------------\n\n\nThe tuned random forest model performs a bit better than the one with the default values. However, the improvement is not that big. This is likely because the default values of the random forest are already quite good. We could try to test more hyperparameters in the grid search. Note that we chose the highest value for both parameters. Thus, we could try even higher values. However, this would increase the computational time.\n\n\n7.6.7 XGBoost\nWe will now train an XGBoost regressor on the data\n\nreg_xgb = XGBRegressor(random_state=42).fit(X_train, y_train)\n\nWe can evaluate the model using the function we defined earlier\n\nevaluate_model(reg_xgb, X_train, y_train, X_test, y_test, label = 'XGBoost');\n\n--------------------------------------------------------------\nMetrics: XGBoost\n--------------------------------------------------------------\nRMSE (Train): 101571.54001161143\nMAE (Train): 76626.15270638122\nR2 (Train): 0.9212105237017153\n--------------------------------------------------------------\nRMSE (Test): 204360.86407090884\nMAE (Test): 120757.43200613001\nR2 (Test): 0.7217385587721041\n--------------------------------------------------------------\n\n\nLet’s use grid search with cross-validation to find the best hyperparameters for the XGBoost\n\nparam_grid = {\n    'max_depth': [5, 10, 15, 20],\n    'n_estimators': [50, 100, 150, 200, 300],\n}\n\n#reg_xgb_cv = GridSearchCV(XGBRegressor(random_state=42), param_grid, cv=5).fit(X_train, y_train)\n#dump(reg_xgb_cv, 'reg_xgb_cv.joblib')\n\n\nreg_xgb_cv = load('reg_xgb_cv.joblib')\n\nThe best hyperparameters are stored in the best_params_ attribute of the model\n\nreg_xgb_cv.best_params_\n\n{'max_depth': 5, 'n_estimators': 50}\n\n\nWe can then evaluate the model using the best hyperparameters\n\nevaluate_model(reg_xgb_cv, X_train, y_train, X_test, y_test, label = 'XGBoost (CV)');\n\n--------------------------------------------------------------\nMetrics: XGBoost (CV)\n--------------------------------------------------------------\nRMSE (Train): 134323.31888964228\nMAE (Train): 99419.23493894239\nR2 (Train): 0.862207059779255\n--------------------------------------------------------------\nRMSE (Test): 201839.3762899356\nMAE (Test): 123656.21579704488\nR2 (Test): 0.7285628038252234\n--------------------------------------------------------------\n\n\nAgain, the tuned XGBoost model performs a bit better than the one with the default values. However, the improvement is not that big.\n\n\n7.6.8 Neural Network\nFinally, let’s try to train a neural network on the data\n\n#reg_nn = MLPRegressor(random_state=42, verbose=True).fit(X_train, y_train)\n#dump(reg_nn, 'reg_nn.joblib') \n\n\nreg_nn = load('reg_nn.joblib')\n\nWe can evaluate the model using the function we defined earlier\n\nevaluate_model(reg_nn, X_train, y_train, X_test, y_test, label = 'Neural Network');\n\n--------------------------------------------------------------\nMetrics: Neural Network\n--------------------------------------------------------------\nRMSE (Train): 142063.16391660276\nMAE (Train): 101370.39765456515\nR2 (Train): 0.8458700261274621\n--------------------------------------------------------------\nRMSE (Test): 201217.79803902542\nMAE (Test): 125534.39997216879\nR2 (Test): 0.7302320486389542\n--------------------------------------------------------------\n\n\nWe can try to improve the performance of the neural network by tuning the hyperparameters. We will use grid search with cross-validation to find the best hyperparameters for the neural network\n\nparam_grid = {\n    'hidden_layer_sizes': [(100,), (100, 100), (200,), (200, 100)],\n    'alpha': [0.0001, 0.001, 0.01, 0.1],\n}\n\n#reg_nn_cv = GridSearchCV(MLPRegressor(random_state=42, verbose=True), param_grid, cv=5).fit(X_train, y_train)\n#dump(reg_nn_cv, 'reg_nn_cv.joblib')\n\n\nreg_nn_cv = load('reg_nn_cv.joblib')\n\nThe best hyperparameters are stored in the best_params_ attribute of the model\n\nreg_nn_cv.best_params_\n\n{'alpha': 0.1, 'hidden_layer_sizes': (100,)}\n\n\nWe can then evaluate the model using the best hyperparameters\n\nevaluate_model(reg_nn_cv, X_train, y_train, X_test, y_test, label = 'Neural Network');\n\n--------------------------------------------------------------\nMetrics: Neural Network\n--------------------------------------------------------------\nRMSE (Train): 155891.59268622578\nMAE (Train): 108538.19421278372\nR2 (Train): 0.814403608721182\n--------------------------------------------------------------\nRMSE (Test): 192879.83144507415\nMAE (Test): 122558.79964553488\nR2 (Test): 0.7521258686276469\n--------------------------------------------------------------\n\n\nThe tuned neural network model performs a bit better than the one with the default values.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#model-evaluation",
    "href": "applicationII.html#model-evaluation",
    "title": "7  House Price Prediction",
    "section": "7.7 Model Evaluation",
    "text": "7.7 Model Evaluation\nLet’s summarize the results of our models\n\nmodels = {\n    \"Linear Regression\" : reg_lin,\n    \"LASSO Regression\" : reg_lasso_cv,\n    \"Decision Tree\" : reg_tree_cv,\n    \"Random Forest\" : reg_rf_cv,\n    \"XGBoost\" : reg_xgb_cv,\n    \"Neural Network\" : reg_nn_cv\n}\n\nresults = pd.DataFrame(columns=['Model', 'RMSE Train', 'RMSE Test', 'MAE Train', 'MAE Test', 'R2 Train', 'R2 Test'])\n\nfor modelName in models:\n\n    # Evaluate the current model\n    rmse_train, rmse_test, mae_train, mae_test, r2_train, r2_test = evaluate_model(models[modelName], X_train, y_train, X_test, y_test, print_results=False)\n\n    # Store the results\n    res = {\n        'Model': modelName,\n        'RMSE Train': rmse_train, \n        'RMSE Test': rmse_test,\n        'MAE Train': mae_train,\n        'MAE Test': mae_test,\n        'R2 Train': r2_train,\n        'R2 Test': r2_test\n    }\n\n    df_tmp = pd.DataFrame(res, index=[0])\n\n    results = pd.concat([results, df_tmp], axis=0, ignore_index=True)\n\n# Sort the results by the RMSE of the test set\nresults = results.sort_values(by='RMSE Test').reset_index(drop=True)\n\nresults\n\n\n\n\n\n\n\n\n\nModel\nRMSE Train\nRMSE Test\nMAE Train\nMAE Test\nR2 Train\nR2 Test\n\n\n\n\n0\nNeural Network\n155891.592686\n192879.831445\n108538.194213\n122558.799646\n0.814404\n0.752126\n\n\n1\nXGBoost\n134323.318890\n201839.376290\n99419.234939\n123656.215797\n0.862207\n0.728563\n\n\n2\nRandom Forest\n72654.489660\n206073.357227\n49716.929169\n120254.776469\n0.959687\n0.717055\n\n\n3\nLinear Regression\n203680.132714\n215930.286432\n133387.291151\n137547.231274\n0.683174\n0.689340\n\n\n4\nLASSO Regression\n204360.763893\n217146.281610\n134141.737315\n138085.310926\n0.681053\n0.685832\n\n\n5\nDecision Tree\n165756.350136\n233527.646129\n111454.509334\n137387.480294\n0.790171\n0.636642",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "applicationII.html#conclusion",
    "href": "applicationII.html#conclusion",
    "title": "7  House Price Prediction",
    "section": "7.8 Conclusion",
    "text": "7.8 Conclusion\nIn this application, we have seen how to implement machine learning models for regression problems. We have used a dataset of house prices in King County, USA, to predict the price of a house based on a set of features. We have trained several models, including linear regression, LASSO regression, decision trees, random forests, XGBoost, and neural networks. We have used grid search with cross-validation to find the best hyperparameters for the models. We have evaluated the models based on the root mean squared error, mean absolute error, and R-squared. With a bit more careful hyperparameter tuning, we could likely improve the performance of the models even further and the ranking of the models might change.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>House Price Prediction</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alonso Robisco, Andrés, and José Manuel Carbó Martínez. 2022.\n“Measuring the model risk-adjusted\nperformance of machine learning algorithms in credit default\nprediction.” Financial Innovation 8 (1). https://doi.org/10.1186/s40854-022-00366-1.\n\n\nAruoba, S. Boragan, and Thomas Drechsel. 2022. “Identifying\nMonetary Policy Shocks: A Natural Language Approach.” CEPR\nDiscussion Paper DP17133. CEPR.\n\n\nBank for International Settlements. 2021. “Machine learning applications in central\nbanking.” IFC Bulletin 57. https://www.bis.org/ifc/publ/ifcb57.pdf.\n\n\nBishop, Christopher M. 2006. Pattern\nRecognition and Machine Learning. Edited by Michael Jordan,\nJon Kleinberg, and Bernhard Schölkopf. Information Science and\nStatistics. Springer Science+Business Media, LLC. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nFernández-Villaverde, Jesús, Samuel Hurtado, and Galo Nuño. 2023.\n“Financial Frictions and the Wealth\nDistribution.” Econometrica 91 (3): 869–901. https://doi.org/10.3982/ecta18180.\n\n\nFernández-Villaverde, Jesús, Joël Marbet, Galo Nuño, and Omar Rachedi.\n2024. “Inequality and the Zero Lower\nBound.” Working Paper 2407. Banco de España.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016.\nDeep Learning. MIT Press. http://www.deeplearningbook.org.\n\n\nGorodnichenko, Yuriy, Tho Pham, and Oleksandr Talavera. 2023.\n“The Voice of Monetary Policy.”\nAmerican Economic Review 113 (2): 548–84. https://doi.org/10.1257/aer.20220129.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning - Data Mining,\nInference, and Prediction. Second Edition. Springer.\n\n\nKaji, Tetsuya, Elena Manresa, and Guillaume Pouliot. 2023. “An Adversarial Approach to Structural\nEstimation.” Econometrica 91 (6): 2041–63. https://doi.org/10.3982/ecta18707.\n\n\nKase, Hanno, Leonardo Melosi, and Matthias Rottner. 2022. “Estimating Nonlinear Heterogeneous Agents Models with\nNeural Networks.” Federal Reserve Bank of Chicago. https://doi.org/10.21033/wp-2022-26.\n\n\nMaliar, Lilia, Serguei Maliar, and Pablo Winant. 2021. “Deep learning for solving dynamic economic\nmodels.” Journal of Monetary Economics 122\n(September): 76–101. https://doi.org/10.1016/j.jmoneco.2021.07.004.\n\n\nMcCulloch, Warren S., and Walter Pitts. 1943. “A logical calculus of the ideas immanent in nervous\nactivity.” The Bulletin of Mathematical\nBiophysics 5 (4): 115–33. https://doi.org/10.1007/bf02478259.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis:\nData Wrangling with pandas, NumPy, and Jupyter. Third\nEdition. O’Reilly Media. https://wesmckinney.com/book/.\n\n\nMicrosoft. 2024. “Deep learning vs. machine\nlearning in Azure Machine Learning.” Website. https://learn.microsoft.com/en-us/azure/machine-learning/concept-deep-learning-vs-machine-learning?view=azureml-api-2.\n\n\nMitchell, Tom. 1997. Machine Learning. McGraw\nHill. https://www.cs.cmu.edu/~tom/mlbook.html.\n\n\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic\nPerspective. Cambridge: MIT Press. https://probml.github.io/pml-book/book0.html.\n\n\n———. 2022. Probabilistic Machine Learning: An\nIntroduction. MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\n———. 2023. Probabilistic Machine Learning: Advanced\nTopics. MIT Press. https://probml.github.io/pml-book/book2.html.\n\n\nNielsen, Michael. 2019. Neural Networks and\nDeep Learning. http://neuralnetworksanddeeplearning.com.\n\n\nRosenblatt, F. 1958. “The perceptron: A\nprobabilistic model for information storage and organization in the\nbrain.” Psychological Review 65 (6): 386–408. https://doi.org/10.1037/h0042519.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement\nLearning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book-2nd.html.\n\n\nVarian, Hal R. 2014. “Big Data: New Tricks\nfor Econometrics.” Journal of Economic\nPerspectives 28 (2): 3–28. https://doi.org/10.1257/jep.28.2.3.",
    "crumbs": [
      "References"
    ]
  }
]